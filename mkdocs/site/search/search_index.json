{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#lance-modern-columnar-format-for-ml-workloads","title":"Lance: modern columnar format for ML workloads","text":"<p>Lance is a columnar format that is easy and fast to version, query and train on. It's designed to be used with images, videos, 3D point clouds, audio and of course tabular data. It supports any POSIX file systems, and cloud storage like AWS S3 and Google Cloud Storage. </p> <p>The key features of Lance include:</p> <ul> <li>High-performance random access: 100x faster than Parquet.</li> <li>Zero-copy schema evolution: add and drop columns without copying the entire dataset.</li> <li>Vector search: find nearest neighbors in under 1 millisecond and combine OLAP-queries with vector search.</li> <li>Ecosystem integrations: Apache-Arrow, DuckDB and more on the way.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>You can install Lance via pip:</p> <pre><code>pip install pylance\n</code></pre> <p>For the latest features and bug fixes, you can install the preview version:</p> <pre><code>pip install --pre --extra-index-url https://pypi.fury.io/lancedb/ pylance\n</code></pre> <p>Preview releases receive the same level of testing as regular releases.</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":""},{"location":"#introduction","title":"Introduction","text":"<ul> <li>Quickstart - Get started with Lance quickly</li> <li>Read and Write - Basic operations</li> <li>Schema Evolution - Evolving your data schema</li> </ul>"},{"location":"#advanced-usage","title":"Advanced Usage","text":"<ul> <li>Lance Format Spec - Technical specification</li> <li>Blob API - Working with large binary objects</li> <li>Tags - Tagging and metadata</li> <li>Object Store Configuration - Cloud storage setup</li> <li>Distributed Write - Distributed operations</li> <li>Performance Guide - Optimization tips</li> <li>Tokenizer - Text tokenization</li> <li>Extension Arrays - Custom array types</li> </ul>"},{"location":"#integrations","title":"Integrations","text":"<ul> <li>Huggingface - ML model integration</li> <li>Tensorflow - TensorFlow integration</li> <li>PyTorch - PyTorch integration</li> <li>Ray - Distributed computing</li> <li>Spark - Apache Spark integration</li> </ul>"},{"location":"#examples","title":"Examples","text":"<ul> <li>Examples Overview - All examples</li> <li>API References - Complete API documentation</li> <li>Contributing - How to contribute</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to get started? Check out our Quickstart guide or explore the examples to see Lance in action!</p>"},{"location":"arrays/","title":"Extension Arrays","text":"<p>Lance provides extensions for Arrow arrays and Pandas Series to represent data types for machine learning applications.</p>"},{"location":"arrays/#bfloat16","title":"BFloat16","text":"<p>BFloat16 is a 16-bit floating point number that is designed for machine learning use cases. Intuitively, it only has 2-3 digits of precision, but it has the same range as a 32-bit float: ~1e-38 to ~1e38. By comparison, a 16-bit float has a range of ~5.96e-8 to 65504.</p> <p>Lance provides an Arrow extension array (<code>lance.arrow.BFloat16Array</code>) and a Pandas extension array (<code>~lance._arrow.PandasBFloat16Type</code>) for BFloat16. These are compatible with the ml_dtypes bfloat16 NumPy extension array.</p> <p>If you are using Pandas, you can use the [lance.bfloat16]{.title-ref} dtype string to create the array:</p> <p>To create an Arrow array, use the <code>lance.arrow.bfloat16_array</code> function:</p> <pre><code>&gt;&gt;&gt; from lance.arrow import bfloat16_array\n\n&gt;&gt;&gt; bfloat16_array([1.1, 2.1, 3.4])\n&lt;lance.arrow.BFloat16Array object at 0x000000016feb94e0&gt;\n[\n  1.1015625,\n  2.09375,\n  3.40625\n]\n</code></pre> <p>Finally, if you have a pre-existing NumPy array, you can convert it into either:</p> <pre><code>\\&gt;\\&gt;\\&gt; import numpy as np \\&gt;\\&gt;\\&gt; from ml_dtypes import bfloat16 \\&gt;\\&gt;\\&gt;\nfrom lance.arrow import PandasBFloat16Array, BFloat16Array\n\n\\&gt;\\&gt;\\&gt; np_array = np.array(\\[1.1, 2.1, 3.4\\], dtype=bfloat16) \\&gt;\\&gt;\\&gt;\nPandasBFloat16Array.from_numpy(np_array) \\&lt;PandasBFloat16Array\\&gt;\n\\[1.1015625, 2.09375, 3.40625\\] Length: 3, dtype: lance.bfloat16 \\&gt;\\&gt;\\&gt;\nBFloat16Array.from_numpy(np_array) \\&lt;lance.arrow.BFloat16Array object at\n0x\\...\\&gt; \\[ 1.1015625, 2.09375, 3.40625 \\]\n</code></pre> <p>When reading, these can be converted back to to the NumPy bfloat16 dtype using each array class\\'s <code>to_numpy</code> method.</p>"},{"location":"arrays/#imageuri","title":"ImageURI","text":"<p><code>lance.arrow.ImageURIArray</code> is an array that stores the URI location of images in some other storage system. For example, <code>file:///path/to/image.png</code> for a local filesystem or <code>s3://bucket/path/image.jpeg</code> for an image on AWS S3. Use this array type when you want to lazily load images from an existing storage medium.</p> <p>It can be created by calling <code>lance.arrow.ImageURIArray.from_uris</code> with a list of URIs represented by either <code>pyarrow.StringArray</code> or an iterable that yields strings. Note that the URIs are not strongly validated and images are not read into memory automatically.</p> <p><code>lance.arrow.ImageURIArray.read_uris</code> will read images into memory and return them as a new <code>lance.arrow.EncodedImageArray</code> object.</p> <pre><code>from lance.arrow import ImageURIArray\n\nrelative_path = \"images/1.png\"\nuris = [os.path.join(os.path.dirname(__file__), relative_path)]\nImageURIArray.from_uris(uris).read_uris()\n</code></pre> <pre><code>\\&lt;lance.arrow.EncodedImageArray object at 0x\\...\\&gt;\n\\[b\\'x89PNGrnx1anx00x00x00rIHDRx00x00x00\\...\\'\\]\n</code></pre>"},{"location":"arrays/#encodedimage","title":"EncodedImage","text":"<p><code>lance.arrow.EncodedImageArray</code> is an array that stores jpeg and png images in their encoded and compressed representation as they would appear written on disk. Use this array when you want to manipulate images in their compressed format such as when you\\'re reading them from disk or embedding them into HTML.</p> <p>It can be created by calling <code>lance.arrow.ImageURIArray.read_uris</code> on an existing <code>lance.arrow.ImageURIArray</code>. This will read the referenced images into memory. It can also be created by calling <code>lance.arrow.ImageArray.from_array</code>{.interpreted-text role=\"func\"} and passing it an array of encoded images already read into <code>pyarrow.BinaryArray</code> or by calling <code>lance.arrow.ImageTensorArray.to_encoded</code>{.interpreted-text role=\"func\"}.</p> <p>A <code>lance.arrow.EncodedImageArray.to_tensor</code>{.interpreted-text role=\"func\"} method is provided to decode encoded images and return them as <code>lance.arrow.FixedShapeImageTensorArray</code>{.interpreted-text role=\"class\"}, from which they can be converted to numpy arrays or TensorFlow tensors. For decoding images, it will first attempt to use a decoder provided via the optional function parameter. If decoder is not provided it will attempt to use Pillow and tensorflow in that order. If neither library or custom decoder is available an exception will be raised.</p> <pre><code>from lance.arrow import ImageURIArray\n\nuris = [os.path.join(os.path.dirname(__file__), \"images/1.png\")]\nencoded_images = ImageURIArray.from_uris(uris).read_uris()\nprint(encoded_images.to_tensor())\n\ndef tensorflow_decoder(images):\n    import tensorflow as tf\n    import numpy as np\n\n    return np.stack(tf.io.decode_png(img.as_py(), channels=3) for img in images.storage)\n\nprint(encoded_images.to_tensor(tensorflow_decoder))\n</code></pre> <pre><code>\\&lt;lance.arrow.FixedShapeImageTensorArray object at 0x\\...\\&gt; \\[\\[42, 42,\n42, 255\\]\\] \\&lt;lance.arrow.FixedShapeImageTensorArray object at 0x\\...\\&gt;\n\\[\\[42, 42, 42, 255\\]\\]\n</code></pre>"},{"location":"arrays/#fixedshapeimagetensor","title":"FixedShapeImageTensor","text":"<p><code>lance.arrow.FixedShapeImageTensorArray</code> is an array that stores images as tensors where each individual pixel is represented as a numeric value. Typically images are stored as 3 dimensional tensors shaped (height, width, channels). In color images each pixel is represented by three values (channels) as per RGB color model. Images from this array can be read out as numpy arrays individually or stacked together into a single 4 dimensional numpy array shaped (batch_size, height, width, channels).</p> <p>It can be created by calling <code>lance.arrow.EncodedImageArray.to_tensor</code> on a previously existing <code>lance.arrow.EncodedImageArray</code>. This will decode encoded images and return them as a <code>lance.arrow.FixedShapeImageTensorArray</code>{.interpreted-text role=\"class\"}. It can also be created by calling <code>lance.arrow.ImageArray.from_array</code> and passing in a <code>pyarrow.FixedShapeTensorArray</code>{.interpreted-text role=\"class\"}.</p> <p>It can be encoded into to <code>lance.arrow.EncodedImageArray</code> by calling <code>lance.arrow.FixedShapeImageTensorArray.to_encoded</code>{.interpreted-text role=\"func\"} and passing custom encoder If encoder is not provided it will attempt to use tensorflow and Pillow in that order. Default encoders will encode to PNG. If neither library is available it will raise an exception.</p> <pre><code>image_uri = os.path.abspath(os.path.join(os.path.dirname(\\_\\_name\\_\\_),\n\\\"\\_static\\\", \\\"icon.png\\\"))\n</code></pre> <pre><code>\\&gt;\\&gt;\\&gt; from lance.arrow import ImageURIArray\n\n\\&gt;\\&gt;\\&gt; uris = \\[image_uri\\] \\&gt;\\&gt;\\&gt; tensor_images =\nImageURIArray.from_uris(uris).read_uris().to_tensor() \\&gt;\\&gt;\\&gt;\ntensor_images.to_encoded() \\&lt;lance.arrow.EncodedImageArray object at\n0x\\...\\&gt; \\[\\... b\\'x89PNGrnx1a\\...\\'\n</code></pre>"},{"location":"blob/","title":"Blob API","text":"<p>Lance supports storing large binary objects (blobs) alongside your data.</p>"},{"location":"blob/#overview","title":"Overview","text":"<p>Unlike other data formats, large multimodal data is a first-class citizen in the Lance columnar format. Lance provides a high-level API to store and retrieve large binary objects (blobs) in Lance datasets.</p> <p>Lance serves large binary data using <code>lance.BlobFile</code>{.interpreted-text role=\"py:class\"}, which is a file-like object that lazily reads large binary objects.</p> <p>To create a Lance dataset with large blob data, you can mark a large binary column as a blob column by adding the metadata <code>lance-encoding:blob</code> to <code>true</code>.</p> <pre><code>import pyarrow as pa\n\nschema = pa.schema(\n    [\n        pa.field(\"id\", pa.int64()),\n        pa.field(\"video\",\n            pa.large_binary(),\n            metadata={\"lance-encoding:blob\": \"true\"}\n        ),\n    ]\n)\n</code></pre> <p>To fetch blobs from a Lance dataset, you can use <code>lance.dataset.LanceDataset.take_blobs</code>{.interpreted-text role=\"py:meth\"}.</p> <p>For example, it\\'s easy to use [BlobFile]{.title-ref} to extract frames from a video file without loading the entire video into memory.</p> <pre><code># pip install av pylance\n\nimport av\nimport lance\n\nds = lance.dataset(\"./youtube.lance\")\nstart_time, end_time = 500, 1000\nblobs = ds.take_blobs([5], \"video\")\nwith av.open(blobs[0]) as container:\n    stream = container.streams.video[0]\n    stream.codec_context.skip_frame = \"NONKEY\"\n\n    start_time = start_time / stream.time_base\n    start_time = start_time.as_integer_ratio()[0]\n    end_time = end_time / stream.time_base\n    container.seek(start_time, stream=stream)\n\n    for frame in container.decode(stream):\n        if frame.time &gt; end_time:\n            break\n        display(frame.to_image())\n        clear_output(wait=True)\n</code></pre>"},{"location":"contributing/","title":"Guide for New Contributors","text":"<p>This is a guide for new contributors to the Lance project. Even if you have no previous experience with python, rust, and open source, you can still make an non-trivial impact by helping us improve documentation, examples, and more. For experienced developers, the issues you can work on run the gamut from warm-ups to serious challenges in python and rust.</p> <p>If you have any questions, please join our Discord for real-time support. Your feedback is always welcome!</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Join our Discord and say hi</li> <li>Setup your development environment</li> <li>Pick an issue to work on. See     https://github.com/lancedb/lance/contribute for good first issues.</li> <li>Have fun!</li> </ol>"},{"location":"contributing/#development-environment","title":"Development Environment","text":"<p>Currently Lance is implemented in Rust and comes with a Python wrapper. So you\\'ll want to make sure you setup both.</p> <ol> <li> <p>Install Rust: https://www.rust-lang.org/tools/install</p> </li> <li> <p>Install Python 3.9+: https://www.python.org/downloads/</p> </li> <li> <p>Install protoctol buffers:     https://grpc.io/docs/protoc-installation/ (make sure you have     version 3.20 or higher)</p> </li> <li> Install commit hooks: <p>a.  Install pre-commit: https://pre-commit.com/#install b.  Run [pre-commit install]{.title-ref} in the root of the repo</p> </li> </ol>"},{"location":"contributing/#sample-workflow","title":"Sample Workflow","text":"<ol> <li>Fork the repo</li> <li>Pick Github issue</li> <li>Create a branch for the issue</li> <li>Make your changes</li> <li>Create a pull request from your fork to lancedb/lance</li> <li>Get feedback and iterate</li> <li>Merge!</li> <li>Go back to step 2</li> </ol>"},{"location":"contributing/#python-development","title":"Python Development","text":"<p>See: https://github.com/lancedb/lance/blob/main/python/DEVELOPMENT.md</p>"},{"location":"contributing/#rust-development","title":"Rust Development","text":"<p>To format and lint Rust code:</p> <pre><code>cargo fmt --all\ncargo clippy --all-features --tests --benches\n</code></pre>"},{"location":"contributing/#repo-structure","title":"Repo Structure","text":""},{"location":"contributing/#core-format","title":"Core Format","text":"<p>The core format is implemented in Rust under the [rust]{.title-ref} directory. Once you\\'ve setup Rust you can build the core format with:</p> <pre><code>cargo build\n</code></pre> <p>This builds the debug build. For the optimized release build:</p> <pre><code>cargo build -r\n</code></pre> <p>To run the Rust unit tests:</p> <pre><code>cargo test\n</code></pre> <p>If you\\'re working on a performance related feature, benchmarks can be run via:</p> <pre><code>cargo bench\n</code></pre>"},{"location":"contributing/#python-bindings","title":"Python Bindings","text":"<p>The Python bindings for Lance uses a mix of Rust (pyo3) and Python. The Rust code that directly supports the Python bindings are under [python/src]{.title-ref} while the pure Python code lives under [python/python]{.title-ref}.</p> <p>To build the Python bindings, first install requirements:</p> <pre><code>pip install maturin\n</code></pre> <p>To make a dev install:</p> <pre><code>cd python\nmaturin develop\n</code></pre> <p>To use the local python bindings, it\\'s recommended to use venv or conda environment.</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>The documentation is built using Sphinx and lives under [docs]{.title-ref}. To build the docs, first install requirements:</p> <pre><code>pip install -r docs/requirements.txt\n</code></pre> <p>Then build the docs:</p> <pre><code>cd docs\nmake html\n</code></pre> <p>The docs will be built under [docs/build/html]{.title-ref}.</p>"},{"location":"contributing/#example-notebooks","title":"Example Notebooks","text":"<p>Example notebooks are under [examples]{.title-ref}. These are standalone notebooks you should be able to download and run.</p>"},{"location":"contributing/#benchmarks","title":"Benchmarks","text":"<p>Our Rust benchmarks are run multiple times a day and the history can be found here.</p> <p>Separately, we have vector index benchmarks that test against the sift1m dataset, as well as benchmarks for tpch. These live under [benchmarks]{.title-ref}.</p>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>See https://www.python.org/psf/conduct/ and https://www.rust-lang.org/policies/code-of-conduct for details.</p>"},{"location":"distributed_write/","title":"Distributed Write","text":"<p>Warning</p> <p>Lance provides out-of-the-box <code>Ray &lt;./integrations/ray&gt;</code> and Spark integrations.</p> <p>This page is intended for users who wish to perform distributed operations in a custom manner, i.e. using [slurm]{.title-ref} or [Kubernetes]{.title-ref} without the Lance integration.</p>"},{"location":"distributed_write/#overview","title":"Overview","text":"<p>The <code>Lance format &lt;format&gt;</code> is designed to support parallel writing across multiple distributed workers. A distributed write operation can be performed by two phases:</p> <ol> <li>Parallel Writes: Generate new     <code>~lance.LanceFragment</code> in     parallel across multiple workers.</li> <li>Commit: Collect all the     <code>~lance.FragmentMetadata</code> and commit     into a single dataset in a single     <code>~lance.LanceOperation</code>.</li> </ol> <p></p>"},{"location":"distributed_write/#write-new-data","title":"Write new data","text":"<p>Writing or appending new data is straightforward with <code>~lance.fragment.write_fragments</code>.</p> <pre><code>new_data\n\nimport json from lance.fragment import write_fragments\n\n\\# Run on each worker data_uri = \\\"./dist_write\\\" schema = pa.schema(\\[\n(\\\"a\\\", pa.int32()), (\\\"b\\\", pa.string()), \\])\n\n\\# Run on worker 1 data1 = { \\\"a\\\": \\[1, 2, 3\\], \\\"b\\\": \\[\\\"x\\\", \\\"y\\\",\n\\\"z\\\"\\], } fragments_1 = write_fragments(data1, data_uri, schema=schema)\nprint(\\\"Worker 1: \\\", fragments_1)\n\n\\# Run on worker 2 data2 = { \\\"a\\\": \\[4, 5, 6\\], \\\"b\\\": \\[\\\"u\\\", \\\"v\\\",\n\\\"w\\\"\\], } fragments_2 = write_fragments(data2, data_uri, schema=schema)\nprint(\\\"Worker 2: \\\", fragments_2)\n</code></pre> <pre><code>new_data\n\nWorker 1: \\[FragmentMetadata(id=0, files=\\...)\\] Worker 2:\n\\[FragmentMetadata(id=0, files=\\...)\\]\n</code></pre> <p>Now, use <code>lance.fragment.FragmentMetadata.to_json</code>{.interpreted-text role=\"meth\"} to serialize the fragment metadata, and collect all serialized metadata on a single worker to execute the final commit operation.</p> <pre><code>new_data\n\nimport json from lance import FragmentMetadata, LanceOperation\n\n\\# Serialize Fragments into JSON data fragments_json1 =\n\\[json.dumps(fragment.to_json()) for fragment in fragments_1\\]\nfragments_json2 = \\[json.dumps(fragment.to_json()) for fragment in\nfragments_2\\]\n\n\\# On one worker, collect all fragments all_fragments =\n\\[FragmentMetadata.from_json(f) for f in fragments_json1 +\nfragments_json2\\]\n\n\\# Commit the fragments into a single dataset \\# Use\nLanceOperation.Overwrite to overwrite the dataset or create new dataset.\nop = lance.LanceOperation.Overwrite(schema, all_fragments) read_version\n= 0 \\# Because it is empty at the time. lance.LanceDataset.commit(\ndata_uri, op, read_version=read_version, )\n\n\\# We can read the dataset using the Lance API: dataset =\nlance.dataset(data_uri) assert len(dataset.get_fragments()) == 2 assert\ndataset.version == 1 print(dataset.to_table().to_pandas())\n</code></pre> <pre><code>new_data\n\na b 0 1 x 1 2 y 2 3 z 3 4 u 4 5 v 5 6 w\n</code></pre>"},{"location":"distributed_write/#append-data","title":"Append data","text":"<p>Appending additional data follows a similar process. Use <code>lance.LanceOperation.Append</code> to commit the new fragments, ensuring that the <code>read_version</code> is set to the current dataset\\'s version.</p> <pre><code>ds = lance.dataset(data_uri)\nread_version = ds.version\n\nop = lance.LanceOperation.Append(schema, all_fragments)\nlance.LanceDataset.commit(\n    data_uri,\n    op,\n    read_version=read_version,\n)\n</code></pre>"},{"location":"distributed_write/#add-new-columns","title":"Add New Columns","text":"<p>Lance Format excels at operations such as adding columns. Thanks to its two-dimensional layout (see this blog post), adding new columns is highly efficient since it avoids copying the existing data files. Instead, the process simply creates new data files and links them to the existing dataset using metadata-only operations.</p> <pre><code>add_columns\n\nfrom pyarrow import RecordBatch import pyarrow.compute as pc\n\nfrom lance import LanceFragment, LanceOperation\n\ndataset = lance.dataset(\\\"./add_columns_example\\\") assert\nlen(dataset.get_fragments()) == 2 assert\ndataset.to_table().combine_chunks() == pa.Table.from_pydict({ \\\"name\\\":\n\\[\\\"alice\\\", \\\"bob\\\", \\\"charlie\\\", \\\"craig\\\", \\\"dave\\\", \\\"eve\\\"\\],\n\\\"age\\\": \\[25, 33, 44, 55, 66, 77\\], }, schema=schema)\n\ndef name_len(names: RecordBatch) -\\&gt; RecordBatch:\n\n:   \n\n    return RecordBatch.from_arrays(\n\n    :   \\[pc.utf8_length(names\\[\\\"name\\\"\\])\\], \\[\\\"name_len\\\"\\],\n\n    )\n\n\\# On Worker 1 frag1 = dataset.get_fragments()\\[0\\] new_fragment1,\nnew_schema = frag1.merge_columns(name_len, \\[\\\"name\\\"\\])\n\n\\# On Worker 2 frag2 = dataset.get_fragments()\\[1\\] new_fragment2, \\_ =\nfrag2.merge_columns(name_len, \\[\\\"name\\\"\\])\n\n\\# On Worker 3 - Commit all_fragments = \\[new_fragment1, new_fragment2\\]\nop = lance.LanceOperation.Merge(all_fragments, schema=new_schema)\nlance.LanceDataset.commit( \\\"./add_columns_example\\\", op,\nread_version=dataset.version, )\n\n\\# Verify dataset dataset = lance.dataset(\\\"./add_columns_example\\\")\nprint(dataset.to_table().to_pandas())\n</code></pre> <pre><code>add_columns\n\nname age name_len 0 alice 25 5 1 bob 33 3 2 charlie 44 7 3 craig 55 5 4\ndave 66 4 5 eve 77 3\n</code></pre>"},{"location":"format/","title":"Lance Formats","text":"<p>The Lance format is both a table format and a file format. Lance typically refers to tables as \\\"datasets\\\". A Lance dataset is designed to efficiently handle secondary indices, fast ingestion and modification of data, and a rich set of schema evolution features.</p>"},{"location":"format/#dataset-directory","title":"Dataset Directory","text":"<p>A [Lance Dataset]{.title-ref} is organized in a directory.</p> <pre><code>/path/to/dataset:\n    data/*.lance  -- Data directory\n    _versions/*.manifest -- Manifest file for each dataset version.\n    _indices/{UUID-*}/index.idx -- Secondary index, each index per directory.\n    _deletions/*.{arrow,bin} -- Deletion files, which contain ids of rows\n      that have been deleted.\n</code></pre> <p>A <code>Manifest</code> file includes the metadata to describe a version of the dataset.</p> <p>See the table.proto file for the complete Manifest definition.</p>"},{"location":"format/#fragments","title":"Fragments","text":"<p><code>DataFragment</code> represents a chunk of data in the dataset. Itself includes one or more <code>DataFile</code>, where each <code>DataFile</code> can contain several columns in the chunk of data. It also may include a <code>DeletionFile</code>, which is explained in a later section.</p> <p>See the table.proto file for the complete DataFragment and DataFile definitions.</p> <p>The overall structure of a fragment is shown below. One or more data files store the columns of a fragment. New columns can be added to a fragment by adding new data files. The deletion file (if present), stores the rows that have been deleted from the fragment.</p> <p></p> <p>Every row has a unique id, which is an u64 that is composed of two u32s: the fragment id and the local row id. The local row id is just the index of the row in the data files.</p>"},{"location":"format/#file-structure","title":"File Structure","text":"<p>Each <code>.lance</code> file is the container for the actual data.</p> <p></p> <p>At the tail of the file, [ColumnMetadata]{.title-ref} protobuf blocks are used to describe the encoding of the columns in the file.</p> <p>See the file2.proto file for the complete metadata definitions.</p> <p>A <code>Footer</code> describes the overall layout of the file. The entire file layout is described here:</p> <p>See the file2.proto file for the complete file layout definitions.</p>"},{"location":"format/#file-version","title":"File Version","text":"<p>The Lance file format has gone through a number of changes including a breaking change from version 1 to version 2. There are a number of APIs that allow the file version to be specified. Using a newer version of the file format will lead to better compression and/or performance. However, older software versions may not be able to read newer files.</p> <p>In addition, the latest version of the file format (next) is unstable and should not be used for production use cases. Breaking changes could be made to unstable encodings and that would mean that files written with these encodings are no longer readable by any newer versions of Lance. The <code>next</code> version should only be used for experimentation and benchmarking upcoming features.</p> <p>The following values are supported:</p> <p>Version        Minimal Lance  Maximum Lance  Description                  Version        Version        </p> <p>0.1            Any            Any            This is the initial Lance                                                format.</p> <p>2.0            0.16.0         Any            Rework of the Lance file                                                format that removed row                                                groups and introduced null                                                support for lists, fixed                                                size lists, and primitives</p> <p>2.1 (unstable) None           Any            Enhances integer and string                                                compression, adds support                                                for nulls in struct fields,                                                and improves random access                                                performance with nested                                                fields.</p> <p>legacy         N/A            N/A            Alias for 0.1</p> <p>stable         N/A            N/A            Alias for the latest stable                                                version (currently 2.0)</p> <p>next           N/A            N/A            Alias for the latest                                                unstable version (currently                                                2.1)</p> File Versions"},{"location":"format/#file-encodings","title":"File Encodings","text":"<p>Lance supports a variety of encodings for different data types. The encodings are chosen to give both random access and scan performance. Encodings are added over time and may be extended in the future. The manifest records a max format version which controls which encodings will be used. This allows for a gradual migration to a new data format so that old readers can still read new data while a migration is in progress.</p> <p>Encodings are divided into \\\"field encodings\\\" and \\\"array encodings\\\". Field encodings are consistent across an entire field of data, while array encodings are used for individual pages of data within a field. Array encodings can nest other array encodings (e.g. a dictionary encoding can bitpack the indices) however array encodings cannot nest field encodings. For this reason data types such as <code>Dictionary&lt;UInt8, List&lt;String&gt;&gt;</code> are not yet supported (since there is no dictionary field encoding)</p> <p>Encoding     Encoding   What it does                 Supported   When it is   Name         Type                                    Versions    applied</p> <p>Basic struct Field      Encodes non-nullable struct  &gt;= 2.0     Default encoding                encoding   data                                     for structs</p> <p>List         Field      Encodes lists (nullable or   &gt;= 2.0     Default encoding                encoding   non-nullable)                            for lists</p> <p>Basic        Field      Encodes primitive data types &gt;= 2.0     Default encoding   Primitive    encoding   using separate validity                  for primitive                           array                                    data types</p> <p>Value        Array      Encodes a single vector of   &gt;= 2.0     Fallback                encoding   fixed-width values                       encoding for                                                                    fixed-width                                                                    types</p> <p>Binary       Array      Encodes a single vector of   &gt;= 2.0     Fallback                encoding   variable-width data                      encoding for                                                                    variable-width                                                                    types</p> <p>Dictionary   Array      Encodes data using a         &gt;= 2.0     Used on string                encoding   dictionary array and an                  pages with fewer                           indices array which is                   than 100 unique                           useful for large data types              elements                           with few unique values                   </p> <p>Packed       Array      Encodes a struct with        &gt;= 2.0     Only used on   struct       encoding   fixed-width fields in a                  struct types if                           row-major format making                  the field                           random access more efficient             metadata                                                                    attribute                                                                    <code>\"packed\"</code> is                                                                    set to <code>\"true\"</code></p> <p>Fsst         Array      Compresses binary data by    &gt;= 2.1     Used on string                encoding   identifying common                       pages that are                           substrings (of 8 bytes or                not dictionary                           less) and encoding them as               encoded                           symbols                                  </p> <p>Bitpacking   Array      Encodes a single vector of   &gt;= 2.1     Used on integral                encoding   fixed-width values using                 types                           bitpacking which is useful                                        for integral types that do                                        not span the full range of                                        values                                   </p> Encodings Available"},{"location":"format/#feature-flags","title":"Feature Flags","text":"<p>As the file format and dataset evolve, new feature flags are added to the format. There are two separate fields for checking for feature flags, depending on whether you are trying to read or write the table. Readers should check the <code>reader_feature_flags</code> to see if there are any flag it is not aware of. Writers should check <code>writer_feature_flags</code>. If either sees a flag they don\\'t know, they should return an \\\"unsupported\\\" error on any read or write operation.</p>"},{"location":"format/#fields","title":"Fields","text":"<p>Fields represent the metadata for a column. This includes the name, data type, id, nullability, and encoding.</p> <p>Fields are listed in depth first order, and can be one of (1) parent (struct), (2) repeated (list/array), or (3) leaf (primitive). For example, the schema:</p> <pre><code>a: i32\nb: struct {\n    c: list&lt;i32&gt;\n    d: i32\n}\n</code></pre> <p>Would be represented as the following field list:</p> <p>name          id            type          parent_id     logical_type</p> <p><code>a</code>           1             LEAF          0             <code>\"int32\"</code></p> <p><code>b</code>           2             PARENT        0             <code>\"struct\"</code></p> <p><code>b.c</code>         3             REPEATED      2             <code>\"list\"</code></p> <p><code>b.c</code>         4             LEAF          3             <code>\"int32\"</code></p> <p><code>b.d</code>         5             LEAF          2             <code>\"int32\"</code></p>"},{"location":"format/#dataset-update-and-schema-evolution","title":"Dataset Update and Schema Evolution","text":"<p><code>Lance</code> supports fast dataset update and schema evolution via manipulating the <code>Manifest</code> metadata.</p> <p><code>Appending</code> is done by appending new <code>Fragment</code> to the dataset. While adding columns is done by adding new <code>DataFile</code> of the new columns to each <code>Fragment</code>. Finally, <code>Overwrite</code> a dataset can be done by resetting the <code>Fragment</code> list of the <code>Manifest</code>.</p> <p></p>"},{"location":"format/#deletion","title":"Deletion","text":"<p>Rows can be marked deleted by adding a deletion file next to the data in the <code>_deletions</code> folder. These files contain the indices of rows that have between deleted for some fragment. For a given version of the dataset, each fragment can have up to one deletion file. Fragments that have no deleted rows have no deletion file.</p> <p>Readers should filter out row ids contained in these deletion files during a scan or ANN search.</p> <p>Deletion files come in two flavors:</p> <ol> <li>Arrow files: which store a column with a flat vector of indices</li> <li>Roaring bitmaps: which store the indices as compressed bitmaps.</li> </ol> <p>Roaring Bitmaps are used for larger deletion sets, while Arrow files are used for small ones. This is because Roaring Bitmaps are known to be inefficient for small sets.</p> <p>The filenames of deletion files are structured like:</p> <pre><code>_deletions/{fragment_id}-{read_version}-{random_id}.{arrow|bin}\n</code></pre> <p>Where <code>fragment_id</code> is the fragment the file corresponds to, <code>read_version</code> is the version of the dataset that it was created off of (usually one less than the version it was committed to), and <code>random_id</code> is a random i64 used to avoid collisions. The suffix is determined by the file type (<code>.arrow</code> for Arrow file, <code>.bin</code> for roaring bitmap).</p> <p>See the table.proto file for the DeletionFile definition.</p> <p>Deletes can be materialized by re-writing data files with the deleted rows removed. However, this invalidates row indices and thus the ANN indices, which can be expensive to recompute.</p>"},{"location":"format/#committing-datasets","title":"Committing Datasets","text":"<p>A new version of a dataset is committed by writing a new manifest file to the <code>_versions</code> directory.</p> <p>To prevent concurrent writers from overwriting each other, the commit process must be atomic and consistent for all writers. If two writers try to commit using different mechanisms, they may overwrite each other\\'s changes. For any storage system that natively supports atomic rename-if-not-exists or put-if-not-exists, these operations should be used. This is true of local file systems and cloud object stores, with the notable except of AWS S3. For ones that lack this functionality, an external locking mechanism can be configured by the user.</p>"},{"location":"format/#manifest-naming-schemes","title":"Manifest Naming Schemes","text":"<p>Manifest files must use a consistent naming scheme. The names correspond to the versions. That way we can open the right version of the dataset without having to read all the manifests. It also makes it clear which file path is the next one to be written.</p> <p>There are two naming schemes that can be used:</p> <ol> <li>V1: <code>_versions/{version}.manifest</code>. This is the legacy naming     scheme.</li> <li>V2: <code>_versions/{u64::MAX - version:020}.manifest</code>. This is the new     naming scheme. The version is zero-padded (to 20 digits) and     subtracted from <code>u64::MAX</code>. This allows the versions to be sorted in     descending order, making it possible to find the latest manifest on     object storage using a single list call.</li> </ol> <p>It is an error for there to be a mixture of these two naming schemes.</p>"},{"location":"format/#conflict_resolution","title":"Conflict resolution","text":"<p>If two writers try to commit at the same time, one will succeed and the other will fail. The failed writer should attempt to retry the commit, but only if its changes are compatible with the changes made by the successful writer.</p> <p>The changes for a given commit are recorded as a transaction file, under the <code>_transactions</code> prefix in the dataset directory. The transaction file is a serialized <code>Transaction</code> protobuf message. See the <code>transaction.proto</code> file for its definition.</p> <p></p> <p>The commit process is as follows:</p> <ol> <li>The writer finishes writing all data files.</li> <li>The writer creates a transaction file in the <code>_transactions</code>     directory. This file describes the operations that were performed,     which is used for two purposes: (1) to detect conflicts, and (2)     to re-build the manifest during retries.</li> <li>Look for any new commits since the writer started writing. If     there are any, read their transaction files and check for     conflicts. If there are any conflicts, abort the commit.     Otherwise, continue.</li> <li>Build a manifest and attempt to commit it to the next version. If     the commit fails because another writer has already committed, go     back to step 3.</li> </ol> <p>When checking whether two transactions conflict, be conservative. If the transaction file is missing, assume it conflicts. If the transaction file has an unknown operation, assume it conflicts.</p>"},{"location":"format/#external-manifest-store","title":"External Manifest Store","text":"<p>If the backing object store does not support *-if-not-exists operations, an external manifest store can be used to allow concurrent writers. An external manifest store is a KV store that supports put-if-not-exists operation. The external manifest store supplements but does not replace the manifests in object storage. A reader unaware of the external manifest store could read a table that uses it, but it might be up to one version behind the true latest version of the table.</p> <p></p> <p>The commit process is as follows:</p> <ol> <li><code>PUT_OBJECT_STORE mydataset.lance/_versions/{version}.manifest-{uuid}</code>     stage a new manifest in object store under a unique path determined     by new uuid</li> <li><code>PUT_EXTERNAL_STORE base_uri, version, mydataset.lance/_versions/{version}.manifest-{uuid}</code>     commit the path of the staged manifest to the external store.</li> <li><code>COPY_OBJECT_STORE mydataset.lance/_versions/{version}.manifest-{uuid} mydataset.lance/_versions/{version}.manifest</code>     copy the staged manifest to the final path</li> <li><code>PUT_EXTERNAL_STORE base_uri, version, mydataset.lance/_versions/{version}.manifest</code>     update the external store to point to the final manifest</li> </ol> <p>Note that the commit is effectively complete after step 2. If the writer fails after step 2, a reader will be able to detect the external store and object store are out-of-sync, and will try to synchronize the two stores. If the reattempt at synchronization fails, the reader will refuse to load. This is to ensure that the dataset is always portable by copying the dataset directory without special tool.</p> <p></p> <p>The reader load process is as follows:</p> <ol> <li><code>GET_EXTERNAL_STORE base_uri, version, path</code> then, if path does not     end in a UUID return the path</li> <li><code>COPY_OBJECT_STORE mydataset.lance/_versions/{version}.manifest-{uuid} mydataset.lance/_versions/{version}.manifest</code>     reattempt synchronization</li> <li><code>PUT_EXTERNAL_STORE base_uri, version, mydataset.lance/_versions/{version}.manifest</code>     update the external store to point to the final manifest</li> <li><code>RETURN mydataset.lance/_versions/{version}.manifest</code> always return     the finalized path, return error if synchronization fails</li> </ol>"},{"location":"format/#statistics","title":"Statistics","text":"<p>Statistics are stored within Lance files. The statistics can be used to determine which pages can be skipped within a query. The null count, lower bound (min), and upper bound (max) are stored.</p> <p>Statistics themselves are stored in Lance\\'s columnar format, which allows for selectively reading only relevant stats columns.</p>"},{"location":"format/#statistic-values","title":"Statistic values","text":"<p>Three types of statistics are stored per column: null count, min value, max value. The min and max values are stored as their native data types in arrays.</p> <p>There are special behaviors for different data types to account for nulls:</p> <p>For integer-based data types (including signed and unsigned integers, dates, and timestamps), if the min and max are unknown (all values are null), then the minimum/maximum representable values should be used instead.</p> <p>For float data types, if the min and max are unknown, then use <code>-Inf</code> and <code>+Inf</code>, respectively. (<code>-Inf</code> and <code>+Inf</code> may also be used for min and max if those values are present in the arrays.) <code>NaN</code> values should be ignored for the purpose of min and max statistics. If the max value is zero (negative or positive), the max value should be recorded as <code>+0.0</code>. Likewise, if the min value is zero (positive or negative), it should be recorded as <code>-0.0</code>.</p> <p>For binary data types, if the min or max are unknown or unrepresentable, then use null value. Binary data type bounds can also be truncated. For example, an array containing just the value <code>\"abcd\"</code> could have a truncated min of <code>\"abc\"</code> and max of <code>\"abd\"</code>. If there is no truncated value greater than the maximum value, then instead use null for the maximum.</p> <p>Warning</p> <p>The <code>min</code> and <code>max</code> values are not guaranteed to be within the array; they are simply upper and lower bounds. Two common cases where they are not contained in the array is if the min or max original value was deleted and when binary data is truncated. Therefore, statistic should not be used to compute queries such as <code>SELECT max(col) FROM table</code>.</p>"},{"location":"format/#page-level-statistics-format","title":"Page-level statistics format","text":"<p>Page-level statistics are stored as arrays within the Lance file. Each array contains one page long and is <code>num_pages</code> long. The page offsets are stored in an array just like the data page table. The offset to the statistics page table is stored in the metadata.</p> <p>The schema for the statistics is:</p> <pre><code>&lt;field_id_1&gt;: struct\n    null_count: i64\n    min_value: &lt;field_1_data_type&gt;\n    max_value: &lt;field_1_data_type&gt;\n...\n&lt;field_id_N&gt;: struct\n    null_count: i64\n    min_value: &lt;field_N_data_type&gt;\n    max_value: &lt;field_N_data_type&gt;\n</code></pre> <p>Any number of fields may be missing, as statistics for some fields or of some kind may be skipped. In addition, readers should expect there may be extra fields that are not in this schema. These should be ignored. Future changes to the format may add additional fields, but these changes will be backwards compatible.</p> <p>However, writers should not write extra fields that aren\\'t described in this document. Until they are defined in the specification, there is no guarantee that readers will be able to safely interpret new forms of statistics.</p>"},{"location":"format/#feature-move-stable-row-ids","title":"Feature: Move-Stable Row IDs","text":"<p>The row ids features assigns a unique u64 id to each row in the table. This id is stable after being moved (such as during compaction), but is not necessarily stable after a row is updated. (A future feature may make them stable after updates.) To make access fast, a secondary index is created that maps row ids to their locations in the table. The respective parts of these indices are stored in the respective fragment\\'s metadata.</p> row id <p>A unique auto-incrementing u64 id assigned to each row in the table.</p> row address <p>The current location of a row in the table. This is a u64 that can be thought of as a pair of two u32 values: the fragment id and the local row offset. For example, if the row address is (42, 9), then the row is in the 42rd fragment and is the 10<sup>th</sup> row in that fragment.</p> row id sequence <p>The sequence of row ids in a fragment.</p> row id index <p>A secondary index that maps row ids to row addresses. This index is constructed by reading all the row id sequences.</p>"},{"location":"format/#assigning-row-ids","title":"Assigning row ids","text":"<p>Row ids are assigned in a monotonically increasing sequence. The next row id is stored in the manifest as the field <code>next_row_id</code>. This starts at zero. When making a commit, the writer uses that field to assign row ids to new fragments. If the commit fails, the writer will re-read the new <code>next_row_id</code>, update the new row ids, and then try again. This is similar to how the <code>max_fragment_id</code> is used to assign new fragment ids.</p> <p>When a row id updated, it is typically assigned a new row id rather than reusing the old one. This is because this feature doesn\\'t have a mechanism to update secondary indices that may reference the old values for the row id. By deleting the old row id and creating a new one, the secondary indices will avoid referencing stale data.</p>"},{"location":"format/#row-id-sequences","title":"Row ID sequences","text":"<p>The row id values for a fragment are stored in a <code>RowIdSequence</code> protobuf message. This is described in the protos/rowids.proto file. Row id sequences are just arrays of u64 values, which have representations optimized for the common case where they are sorted and possibly contiguous. For example, a new fragment will have a row id sequence that is just a simple range, so it is stored as a <code>start</code> and <code>end</code> value.</p> <p>These sequence messages are either stored inline in the fragment metadata, or are written to a separate file and referenced from the fragment metadata. This choice is typically made based on the size of the sequence. If the sequence is small, it is stored inline. If it is large, it is written to a separate file. By keeping the small sequences inline, we can avoid the overhead of additional IO operations.</p> <p>See the table.proto file for the complete row_id_sequence definition.</p>"},{"location":"format/#row-id-index","title":"Row ID index","text":"<p>To ensure fast access to rows by their row id, a secondary index is created that maps row ids to their locations in the table. This index is built when a table is loaded, based on the row id sequences in the fragments. For example, if fragment 42 has a row id sequence of <code>[0, 63, 10]</code>, then the index will have entries for <code>0 -&gt; (42, 0)</code>, <code>63 -&gt; (42, 1)</code>, <code>10 -&gt; (42, 2)</code>. The exact form of this index is left up to the implementation, but it should be optimized for fast lookups.</p>"},{"location":"object_store/","title":"Object Store Configuration","text":"<p>Lance supports object stores such as AWS S3 (and compatible stores), Azure Blob Store, and Google Cloud Storage. Which object store to use is determined by the URI scheme of the dataset path. For example, <code>s3://bucket/path</code> will use S3, <code>az://bucket/path</code> will use Azure, and <code>gs://bucket/path</code> will use GCS.</p> <p>Added in version</p> <p>0.10.7</p> <p>Passing options directly to storage options.</p> <p>These object stores take additional configuration objects. There are two ways to specify these configurations: by setting environment variables or by passing them to the <code>storage_options</code> parameter of <code>lance.dataset</code> and <code>lance.write_dataset</code>. So for example, to globally set a higher timeout, you would run in your shell:</p> <pre><code>export TIMEOUT=60s\n</code></pre> <p>If you only want to set the timeout for a single dataset, you can pass it as a storage option:</p> <pre><code>import lance\nds = lance.dataset(\"s3://path\", storage_options={\"timeout\": \"60s\"})\n</code></pre>"},{"location":"object_store/#general-configuration","title":"General Configuration","text":"<p>These options apply to all object stores.</p> <p>Key                            Description</p> <p><code>allow_http</code>                   Allow non-TLS, i.e. non-HTTPS connections.                                  Default, <code>False</code>.</p> <p><code>download_retry_count</code>         Number of times to retry a download. Default,                                  <code>3</code>. This limit is applied when the HTTP request                                  succeeds but the response is not fully                                  downloaded, typically due to a violation of                                  <code>request_timeout</code>.</p> <p><code>allow_invalid_certificates</code>   Skip certificate validation on https connections.                                  Default, <code>False</code>. Warning: This is insecure and                                  should only be used for testing.</p> <p><code>connect_timeout</code>              Timeout for only the connect phase of a Client.                                  Default, <code>5s</code>.</p> <p><code>request_timeout</code>              Timeout for the entire request, from connection                                  until the response body has finished. Default,                                  <code>30s</code>.</p> <p><code>user_agent</code>                   User agent string to use in requests.</p> <p><code>proxy_url</code>                    URL of a proxy server to use for requests.                                  Default, <code>None</code>.</p> <p><code>proxy_ca_certificate</code>         PEM-formatted CA certificate for proxy                                  connections</p> <p><code>proxy_excludes</code>               List of hosts that bypass proxy. This is a comma                                  separated list of domains and IP masks. Any                                  subdomain of the provided domain will be                                  bypassed. For example,                                  <code>example.com, 192.168.1.0/24</code> would bypass                                  <code>https://api.example.com</code>,                                  <code>https://www.example.com</code>, and any IP in the                                  range <code>192.168.1.0/24</code>.</p> <p><code>client_max_retries</code>           Number of times for a s3 client to retry the                                  request. Default, <code>10</code>.</p> <p><code>client_retry_timeout</code>         Timeout for a s3 client to retry the request in                                  seconds. Default, <code>180</code>.</p>"},{"location":"object_store/#s3-configuration","title":"S3 Configuration","text":"<p>S3 (and S3-compatible stores) have additional configuration options that configure authorization and S3-specific features (such as server-side encryption).</p> <p>AWS credentials can be set in the environment variables <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, and <code>AWS_SESSION_TOKEN</code>. Alternatively, they can be passed as parameters to the <code>storage_options</code> parameter:</p> <pre><code>import lance\nds = lance.dataset(\n    \"s3://bucket/path\",\n    storage_options={\n        \"access_key_id\": \"my-access-key\",\n        \"secret_access_key\": \"my-secret-key\",\n        \"session_token\": \"my-session-token\",\n    }\n)\n</code></pre> <p>If you are using AWS SSO, you can specify the <code>AWS_PROFILE</code> environment variable. It cannot be specified in the <code>storage_options</code> parameter.</p> <p>The following keys can be used as both environment variables or keys in the <code>storage_options</code> parameter:</p> <p>Key                                  Description</p> <p><code>aws_region</code> / <code>region</code>              The AWS region the bucket is in. This can be                                        automatically detected when using AWS S3, but                                        must be specified for S3-compatible stores.</p> <p><code>aws_access_key_id</code> /                The AWS access key ID to use.   <code>access_key_id</code> </p> <p><code>aws_secret_access_key</code> /            The AWS secret access key to use.   <code>secret_access_key</code> </p> <p><code>aws_session_token</code> /                The AWS session token to use.   <code>session_token</code> </p> <p><code>aws_endpoint</code> / <code>endpoint</code>          The endpoint to use for S3-compatible stores.</p> <p><code>aws_virtual_hosted_style_request</code> / Whether to use virtual hosted-style requests,   <code>virtual_hosted_style_request</code>       where bucket name is part of the endpoint. Meant                                        to be used with <code>aws_endpoint</code>. Default, <code>False</code>.</p> <p><code>aws_s3_express</code> / <code>s3_express</code>      Whether to use S3 Express One Zone endpoints.                                        Default, <code>False</code>. See more details below.</p> <p><code>aws_server_side_encryption</code>         The server-side encryption algorithm to use. Must                                        be one of <code>\"AES256\"</code>, <code>\"aws:kms\"</code>, or                                        <code>\"aws:kms:dsse\"</code>. Default, <code>None</code>.</p> <p><code>aws_sse_kms_key_id</code>                 The KMS key ID to use for server-side encryption.                                        If set, <code>aws_server_side_encryption</code> must be                                        <code>\"aws:kms\"</code> or <code>\"aws:kms:dsse\"</code>.</p> <p><code>aws_sse_bucket_key_enabled</code>         Whether to use bucket keys for server-side                                        encryption.</p>"},{"location":"object_store/#s3-compatible-stores","title":"S3-compatible stores","text":"<p>Lance can also connect to S3-compatible stores, such as MinIO. To do so, you must specify both region and endpoint:</p> <pre><code>import lance\nds = lance.dataset(\n    \"s3://bucket/path\",\n    storage_options={\n        \"region\": \"us-east-1\",\n        \"endpoint\": \"http://minio:9000\",\n    }\n)\n</code></pre> <p>This can also be done with the <code>AWS_ENDPOINT</code> and <code>AWS_DEFAULT_REGION</code> environment variables.</p>"},{"location":"object_store/#s3-express","title":"S3 Express","text":"<p>Added in version</p> <p>0.9.7</p> <p>Lance supports S3 Express One Zone endpoints, but requires additional configuration. Also, S3 Express endpoints only support connecting from an EC2 instance within the same region</p> <p>To configure Lance to use an S3 Express endpoint, you must set the storage option <code>s3_express</code>. The bucket name in your table URI should include the suffix.</p> <pre><code>import lance\nds = lance.dataset(\n    \"s3://my-bucket--use1-az4--x-s3/path/imagenet.lance\",\n    storage_options={\n        \"region\": \"us-east-1\",\n        \"s3_express\": \"true\",\n    }\n)\n</code></pre>"},{"location":"object_store/#committing-mechanisms-for-s3","title":"Committing mechanisms for S3","text":"<p>Deprecated</p> <p>S3 now supports atomic put-if-not-exists, so this feature is no longer necessary. It will be removed in a future version. You should migrate tables to use the new feature by removing the commit locks from all writers at the same time. Note that it is unsafe to mix writers with and without commit locks on the same dataset.</p> <p>Most supported storage systems (e.g. local file system, Google Cloud Storage, Azure Blob Store) natively support atomic commits, which prevent concurrent writers from corrupting the dataset. However, S3 does not support this natively. To work around this, you may provide a locking mechanism that Lance can use to lock the table while providing a write. To do so, you should implement a context manager that acquires and releases a lock and then pass that to the <code>commit_lock</code> parameter of <code>lance.write_dataset</code>.</p> <p>Note</p> <p>In order for the locking mechanism to work, all writers must use the same exact mechanism. Otherwise, Lance will not be able to detect conflicts.</p> <p>On entering, the context manager should acquire the lock on the table. The table version being committed is passed in as an argument, which may be used if the locking service wishes to keep track of the current version of the table, but this is not required. If the table is already locked by another transaction, it should wait until it is unlocked, since the other transaction may fail. Once unlocked, it should either lock the table or, if the lock keeps track of the current version of the table, return a <code>CommitConflictError</code> if the requested version has already been committed.</p> <p>To prevent poisoned locks, it\\'s recommended to set a timeout on the locks. That way, if a process crashes while holding the lock, the lock will be released eventually. The timeout should be no less than 30 seconds.</p> <pre><code>from contextlib import contextmanager\n\n@contextmanager\ndef commit_lock(version: int);\n    # Acquire the lock\n    my_lock.acquire()\n    try:\n      yield\n    except:\n      failed = True\n    finally:\n      my_lock.release()\n\nlance.write_dataset(data, \"s3://bucket/path/\", commit_lock=commit_lock)\n</code></pre> <p>When the context manager is exited, it will raise an exception if the commit failed. This might be because of a network error or if the version has already been written. Either way, the context manager should release the lock. Use a try/finally block to ensure that the lock is released.</p>"},{"location":"object_store/#concurrent-writer-on-s3-using-dynamodb","title":"Concurrent Writer on S3 using DynamoDB","text":"<p>Warning</p> <p>This feature is experimental at the moment</p> <p>Lance has native support for concurrent writers on S3 using DynamoDB instead of locking. User may pass in a DynamoDB table name alone with the S3 URI to their dataset to enable this feature.</p> <pre><code>import lance\n# s3+ddb:// URL scheme let's lance know that you want to\n# use DynamoDB for writing to S3 concurrently\nds = lance.dataset(\"s3+ddb://my-bucket/mydataset?ddbTableName=mytable\")\n</code></pre> <p>The DynamoDB table is expected to have a primary hash key of <code>base_uri</code> and a range key <code>version</code>. The key <code>base_uri</code> should be string type, and the key <code>version</code> should be number type.</p> <p>For details on how this feature works, please see <code>external-manifest-store</code>.</p>"},{"location":"object_store/#google-cloud-storage-configuration","title":"Google Cloud Storage Configuration","text":"<p>GCS credentials are configured by setting the <code>GOOGLE_SERVICE_ACCOUNT</code> environment variable to the path of a JSON file containing the service account credentials. Alternatively, you can pass the path to the JSON file in the <code>storage_options</code></p> <pre><code>import lance\nds = lance.dataset(\n    \"gs://my-bucket/my-dataset\",\n    storage_options={\n        \"service_account\": \"path/to/service-account.json\",\n    }\n)\n</code></pre> <p>Note</p> <p>By default, GCS uses HTTP/1 for communication, as opposed to HTTP/2. This improves maximum throughput significantly. However, if you wish to use HTTP/2 for some reason, you can set the environment variable <code>HTTP1_ONLY</code> to <code>false</code>.</p> <p>The following keys can be used as both environment variables or keys in the <code>storage_options</code> parameter:</p> <p>Key                                Description</p> <p><code>google_service_account</code> /         Path to the service account JSON file.   <code>service_account</code> </p> <p><code>google_service_account_key</code> /     The serialized service account key.   <code>service_account_key</code> </p> <p><code>google_application_credentials</code> / Path to the application credentials.   <code>application_credentials</code> </p>"},{"location":"object_store/#azure-blob-storage-configuration","title":"Azure Blob Storage Configuration","text":"<p>Azure Blob Storage credentials can be configured by setting the <code>AZURE_STORAGE_ACCOUNT_NAME</code> and <code>AZURE_STORAGE_ACCOUNT_KEY</code> environment variables. Alternatively, you can pass the account name and key in the <code>storage_options</code> parameter:</p> <pre><code>import lance\nds = lance.dataset(\n    \"az://my-container/my-dataset\",\n    storage_options={\n        \"account_name\": \"some-account\",\n        \"account_key\": \"some-key\",\n    }\n)\n</code></pre> <p>These keys can be used as both environment variables or keys in the <code>storage_options</code> parameter:</p> <p>Key                            Description</p> <p><code>azure_storage_account_name</code> / The name of the azure storage account.   <code>account_name</code> </p> <p><code>azure_storage_account_key</code> /  The serialized service account key.   <code>account_key</code> </p> <p><code>azure_client_id</code> /            Service principal client id for authorizing   <code>client_id</code>                    requests.</p> <p><code>azure_client_secret</code> /        Service principal client secret for authorizing   <code>client_secret</code>                requests.</p> <p><code>azure_tenant_id</code> /            Tenant id used in oauth flows.   <code>tenant_id</code> </p> <p><code>azure_storage_sas_key</code> /      Shared access signature. The signature is   <code>azure_storage_sas_token</code> /    expected to be percent-encoded, much like they   <code>sas_key</code> / <code>sas_token</code>        are provided in the azure storage explorer or                                  azure portal.</p> <p><code>azure_storage_token</code> /        Bearer token.   <code>bearer_token</code> / <code>token</code> </p> <p><code>azure_storage_use_emulator</code> / Use object store with azurite storage emulator.   <code>object_store_use_emulator</code> / <code>use_emulator</code> </p> <p><code>azure_endpoint</code> / <code>endpoint</code>  Override the endpoint used to communicate with                                  blob storage.</p> <p><code>azure_use_fabric_endpoint</code> /  Use object store with url scheme   <code>use_fabric_endpoint</code>          account.dfs.fabric.microsoft.com.</p> <p><code>azure_msi_endpoint</code> /         Endpoint to request a imds managed identity   <code>azure_identity_endpoint</code> /    token.   <code>identity_endpoint</code> /         <code>msi_endpoint</code> </p> <p><code>azure_object_id</code> /            Object id for use with managed identity   <code>object_id</code>                    authentication.</p> <p><code>azure_msi_resource_id</code> /      Msi resource id for use with managed identity   <code>msi_resource_id</code>              authentication.</p> <p><code>azure_federated_token_file</code> / File containing token for Azure AD workload   <code>federated_token_file</code>         identity federation.</p> <p><code>azure_use_azure_cli</code> /        Use azure cli for acquiring access token.   <code>use_azure_cli</code> </p> <p><code>azure_disable_tagging</code> /      Disables tagging objects. This can be desirable   <code>disable_tagging</code>              if not supported by the backing store.</p>"},{"location":"performance/","title":"Lance Performance Guide","text":"<p>This guide provides tips and tricks for optimizing the performance of your Lance applications.</p>"},{"location":"performance/#trace-events","title":"Trace Events","text":"<p>Lance uses tracing to log events. If you are running <code>pylance</code> then these events will be emitted to as log messages. For Rust connections you can use the <code>tracing</code> crate to capture these events.</p>"},{"location":"performance/#file-audit","title":"File Audit","text":"<p>File audit events are emitted when significant files are created or deleted.</p> <p>Event                 Parameter      Description</p> <p><code>lance::file_audit</code> <code>mode</code>         The mode of I/O operation (create, delete,                                        delete_unverified)</p> <p><code>lance::file_audit</code> <code>type</code>         The type of file affected (manifest, data                                        file, index file, deletion file)</p>"},{"location":"performance/#io-events","title":"I/O Events","text":"<p>I/O events are emitted when significant I/O operations are performed, particularly those related to indices. These events are NOT emitted when the index is loaded from the in-memory cache. Correct cache utilization is important for performance and these events are intended to help you debug cache usage.</p> <p>Event                Parameter      Description</p> <p><code>lance::io_events</code> <code>type</code>         The type of I/O operation                                       (open_scalar_index, open_vector_index,                                       load_vector_part, load_scalar_part)</p>"},{"location":"performance/#execution-events","title":"Execution Events","text":"<p>Execution events are emitted when an execution plan is run. These events are useful for debugging query performance.</p> <p>Event                Parameter             Description</p> <p><code>lance::execution</code> <code>type</code>                The type of execution event (plan_run is                                              the only type today)</p> <p><code>lance::execution</code> <code>output_rows</code>         The number of rows in the output of the                                              plan</p> <p><code>lance::execution</code> <code>iops</code>                The number of I/O operations performed by                                              the plan</p> <p><code>lance::execution</code> <code>bytes_read</code>          The number of bytes read by the plan</p> <p><code>lance::execution</code> <code>indices_loaded</code>      The number of indices loaded by the plan</p> <p><code>lance::execution</code> <code>parts_loaded</code>        The number of index partitions loaded by                                              the plan</p> <p><code>lance::execution</code> <code>index_comparisons</code>   The number of comparisons performed inside                                              the various indices</p>"},{"location":"performance/#threading-model","title":"Threading Model","text":"<p>Lance is designed to be thread-safe and performant. Lance APIs can be called concurrently unless explicitly stated otherwise. Users may create multiple tables and share tables between threads. Operations may run in parallel on the same table, but some operations may lead to conflicts. For details see <code>conflict_resolution</code>.</p> <p>Most Lance operations will use multiple threads to perform work in parallel. There are two thread pools in lance: the IO thread pool and the compute thread pool. The IO thread pool is used for reading and writing data from disk. The compute thread pool is used for performing computations on data. The number of threads in each pool can be configured by the user.</p> <p>The IO thread pool is used for reading and writing data from disk. The number of threads in the IO thread pool is determined by the object store that the operation is working with. Local object stores will use 8 threads by default. Cloud object stores will use 64 threads by default. This is a fairly conservative default and you may need 128 or 256 threads to saturate network bandwidth on some cloud providers. The <code>LANCE_IO_THREADS</code> environment variable can be used to override the number of IO threads. If you increase this variable you may also want to increase the <code>io_buffer_size</code>.</p> <p>The compute thread pool is used for performing computations on data. The number of threads in the compute thread pool is determined by the number of cores on the machine. The number of threads in the compute thread pool can be overridden by setting the <code>LANCE_CPU_THREADS</code> environment variable. This is commonly done when running multiple Lance processes on the same machine (e.g when working with tools like Ray). Keep in mind that decoding data is a compute intensive operation, even if a workload seems I/O bound (like scanning a table) it may still need quite a few compute threads to achieve peak performance.</p>"},{"location":"performance/#memory-requirements","title":"Memory Requirements","text":"<p>Lance is designed to be memory efficient. Operations should stream data from disk and not require loading the entire dataset into memory. However, there are a few components of Lance that can use a lot of memory.</p>"},{"location":"performance/#index-cache","title":"Index Cache","text":"<p>Lance uses an index cache to speed up queries. This caches vector and scalar indices in memory. The max size of this cache can be configured when creating a <code>LanceDataset</code> using the <code>index_cache_size</code> parameter. This cache is an LRU cached that is sized by \\\"number of entries\\\". The size of each entry and the number of entries needed depends on the index in question. For example, an IVF/PQ vector index contains 1 header entry and 1 entry for each partition. The size of each entry is determined by the number of vectors and the PQ parameters (e.g. number of subvectors). You can view the size of this cache by inspecting the result of <code>dataset.session().size_bytes()</code>.</p> <p>The index cache is not shared between tables. For best performance you should create a single table and share it across your application.</p>"},{"location":"performance/#scanning-data","title":"Scanning Data","text":"<p>Searches (e.g. vector search, full text search) do not use a lot of memory to hold data because they don\\'t typically return a lot of data. However, scanning data can use a lot of memory. Scanning is a streaming operation but we need enough memory to hold the data that we are scanning. The amount of memory needed is largely determined by the <code>io_buffer_size</code> and the <code>batch_size</code> variables.</p> <p>Each I/O thread should have enough memory to buffer an entire page of data. Pages today are typically between 8 and 32 MB. This means, as a rule of thumb, you should generally have about 32MB of memory per I/O thread. The default <code>io_buffer_size</code> is 2GB which is enough to buffer 64 pages of data. If you increase the number of I/O threads you should also increase the <code>io_buffer_size</code>.</p> <p>Scans will also decode data (and run any filtering or compute) in parallel on CPU threads. The amount of data decoded at any one time is determined by the <code>batch_size</code> and the size of your rows. Each CPU thread will need enough memory to hold one batch. Once batches are delivered to your application, they are no longer tracked by Lance and so if memory is a concern then you should also be careful not to accumulate memory in your own application (e.g. by running <code>to_table</code> or otherwise collecting all batches in memory.)</p> <p>The default <code>batch_size</code> is 8192 rows. When you are working with mostly scalar data you want to keep batches around 1MB and so the amount of memory needed by the compute threads is fairly small. However, when working with large data you may need to turn down the <code>batch_size</code> to keep memory usage under control. For example, when working with 1024-dimensional vector embeddings (e.g. 32-bit floats) then 8192 rows would be 32MB of data. If you spread that across 16 CPU threads then you would need 512MB of compute memory per scan. You might find working with 1024 rows per batch is more appropriate.</p> <p>In summary, scans could use up to <code>(2 * io_buffer_size) + (batch_size * num_compute_threads)</code> bytes of memory. Keep in mind that <code>io_buffer_size</code> is a soft limit (e.g. we cannot read less than one page at a time right now) and so it is not necessarily a bug if you see memory usage exceed this limit by a small margin.</p>"},{"location":"tags/","title":"Manage Tags","text":"<p>Lance, much like Git, employs the <code>LanceDataset.tags &lt;lance.LanceDataset.tags&gt;</code>{.interpreted-text role=\"py:attr\"} property to label specific versions within a dataset\\'s history.</p> <p><code>Tags &lt;lance.dataset.Tags&gt;</code> are particularly useful for tracking the evolution of datasets, especially in machine learning workflows where datasets are frequently updated. For example, you can <code>create &lt;lance.dataset.Tags.create&gt;</code>{.interpreted-text role=\"py:meth\"}, <code>update &lt;lance.dataset.Tags.update&gt;</code>{.interpreted-text role=\"meth\"}, and <code>delete &lt;lance.dataset.Tags.delete&gt;</code>{.interpreted-text role=\"meth\"} or <code>list &lt;lance.dataset.Tags.list&gt;</code>{.interpreted-text role=\"py:meth\"} tags.</p> <p>Note</p> <p>Creating or deleting tags does not generate new dataset versions. Tags exist as auxiliary metadata stored in a separate directory.</p> <p>Note</p> <p>Tagged versions are exempted from the <code>LanceDataset.cleanup_old_versions() &lt;lance.LanceDataset.cleanup_old_versions&gt;</code>{.interpreted-text role=\"py:meth\"} process.</p> <p>To remove a version that has been tagged, you must first <code>LanceDataset.tags.delete() &lt;lance.dataset.Tags.delete&gt;</code>{.interpreted-text role=\"py:meth\"} the associated tag.</p>"},{"location":"tokenizer/","title":"Tokenizers","text":"<p>Currently, Lance has built-in support for Jieba and Lindera. However, it doesn\\'t come with its own language models. If tokenization is needed, you can download language models by yourself. You can specify the location where the language models are stored by setting the environment variable LANCE_LANGUAGE_MODEL_HOME. If it\\'s not set, the default value is</p> <pre><code>${system data directory}/lance/language_models\n</code></pre> <p>It also supports configuring user dictionaries, which makes it convenient for users to expand their own dictionaries without retraining the language models.</p>"},{"location":"tokenizer/#language-models-of-jieba","title":"Language Models of Jieba","text":""},{"location":"tokenizer/#downloading-the-model","title":"Downloading the Model","text":"<pre><code>python -m lance.download jieba\n</code></pre> <p>The language model is stored by default in [${LANCE_LANGUAGE_MODEL_HOME}/jieba/default]{.title-ref}.</p>"},{"location":"tokenizer/#using-the-model","title":"Using the Model","text":"<pre><code>ds.create_scalar_index(\"text\", \"INVERTED\", base_tokenizer=\"jieba/default\")\n</code></pre>"},{"location":"tokenizer/#user-dictionaries","title":"User Dictionaries","text":"<p>Create a file named config.json in the root directory of the current model.</p> <pre><code>{\n    \"main\": \"dict.txt\",\n    \"users\": [\"path/to/user/dict.txt\"]\n}\n</code></pre> <ul> <li>The \\\"main\\\" field is optional. If not filled, the default is   \\\"dict.txt\\\".</li> <li>\\\"users\\\" is the path of the user dictionary. For the format of the   user dictionary, please refer to   https://github.com/messense/jieba-rs/blob/main/src/data/dict.txt.</li> </ul>"},{"location":"tokenizer/#language-models-of-lindera","title":"Language Models of Lindera","text":""},{"location":"tokenizer/#downloading-the-model_1","title":"Downloading the Model","text":"<pre><code>python -m lance.download lindera -l [ipadic|ko-dic|unidic]\n</code></pre> <p>Note that the language models of Lindera need to be compiled. Please install lindera-cli first. For detailed steps, please refer to https://github.com/lindera/lindera/tree/main/lindera-cli.</p> <p>The language model is stored by default in ${LANCE_LANGUAGE_MODEL_HOME}/lindera/[ipadic|ko-dic|unidic]</p>"},{"location":"tokenizer/#using-the-model_1","title":"Using the Model","text":"<pre><code>ds.create_scalar_index(\"text\", \"INVERTED\", base_tokenizer=\"lindera/ipadic\")\n</code></pre>"},{"location":"tokenizer/#user-dictionaries_1","title":"User Dictionaries","text":"<p>Create a file named config.json in the root directory of the current model.</p> <pre><code>{\n\"main\": \"main\",\n\"users\": \"path/to/user/dict.bin\",\n\"user_kind\": \"ipadic|ko-dic|unidic\"\n}\n</code></pre> <ul> <li>The \\\"main\\\" field is optional. If not filled, the default is the   \\\"main\\\" directory.</li> <li>\\\"user\\\" is the path of the user dictionary. The user dictionary can   be passed as a CSV file or as a binary file compiled by lindera-cli.</li> <li>The \\\"user_kind\\\" field can be left blank if the user dictionary is in   binary format. If it\\'s in CSV format, you need to specify the type of   the language model.</li> </ul>"},{"location":"tokenizer/#create-your-own-language-model","title":"Create your own language model","text":"<p>Put your language model into [LANCE_LANGUAGE_MODEL_HOME]{.title-ref}.</p>"},{"location":"api/api/","title":"APIs","text":"<ul> <li>Rust</li> <li>Python</li> </ul>"},{"location":"api/py_modules/","title":"Python API Reference","text":""},{"location":"api/py_modules/#lance_1","title":"lance","text":""},{"location":"api/py_modules/#lance","title":"<code>lance</code>","text":""},{"location":"api/py_modules/#lance.__version__","title":"<code>__version__ = '0.27.3'</code>  <code>module</code>","text":"<p>str(object='') -&gt; str str(bytes_or_buffer[, encoding[, errors]]) -&gt; str</p> <p>Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object.str() (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.</p>"},{"location":"api/py_modules/#lance.BlobColumn","title":"<code>BlobColumn</code>","text":"<p>A utility to wrap a Pyarrow binary column and iterate over the rows as file-like objects.</p> <p>This can be useful for working with medium-to-small binary objects that need to interface with APIs that expect file-like objects.  For very large binary objects (4-8MB or more per value) you might be better off creating a blob column and using meth:<code>lance.Dataset.take_blobs</code> to access the blob data.</p>"},{"location":"api/py_modules/#lance.BlobFile","title":"<code>BlobFile</code>","text":"<p>               Bases: <code>RawIOBase</code></p> <p>Represents a blob in a Lance dataset as a file-like object.</p>"},{"location":"api/py_modules/#lance.BlobFile.__init__","title":"<code>__init__(inner)</code>","text":"<p>Internal only:  To obtain a BlobFile use meth:<code>lance.dataset.Dataset.take_blobs</code>.</p>"},{"location":"api/py_modules/#lance.BlobFile.size","title":"<code>size()</code>","text":"<p>Returns the size of the blob in bytes.</p>"},{"location":"api/py_modules/#lance.DataStatistics","title":"<code>DataStatistics</code>  <code>dataclass</code>","text":"<p>Statistics about the data in the dataset</p>"},{"location":"api/py_modules/#lance.FieldStatistics","title":"<code>FieldStatistics</code>  <code>dataclass</code>","text":"<p>Statistics about a field in the dataset</p>"},{"location":"api/py_modules/#lance.FragmentMetadata","title":"<code>FragmentMetadata</code>  <code>dataclass</code>","text":"<p>Metadata for a fragment.</p>"},{"location":"api/py_modules/#lance.FragmentMetadata--attributes","title":"Attributes","text":"<p>id : int     The ID of the fragment. files : List[DataFile]     The data files of the fragment. Each data file must have the same number     of rows. Each file stores a different subset of the columns. physical_rows : int     The number of rows originally in this fragment. This is the number of rows     in the data files before deletions. deletion_file : Optional[DeletionFile]     The deletion file, if any. row_id_meta : Optional[RowIdMeta]     The row id metadata, if any.</p>"},{"location":"api/py_modules/#lance.FragmentMetadata.num_deletions","title":"<code>num_deletions</code>  <code>property</code>","text":"<p>The number of rows that have been deleted from this fragment.</p>"},{"location":"api/py_modules/#lance.FragmentMetadata.num_rows","title":"<code>num_rows</code>  <code>property</code>","text":"<p>The number of rows in this fragment after deletions.</p>"},{"location":"api/py_modules/#lance.FragmentMetadata.to_json","title":"<code>to_json()</code>","text":"<p>Get this as a simple JSON-serializable dictionary.</p>"},{"location":"api/py_modules/#lance.LanceDataset","title":"<code>LanceDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A Lance Dataset in Lance format where the data is stored at the given uri.</p>"},{"location":"api/py_modules/#lance.LanceDataset.data_storage_version","title":"<code>data_storage_version</code>  <code>property</code>","text":"<p>The version of the data storage format this dataset is using</p>"},{"location":"api/py_modules/#lance.LanceDataset.lance_schema","title":"<code>lance_schema</code>  <code>property</code>","text":"<p>The LanceSchema for this dataset</p>"},{"location":"api/py_modules/#lance.LanceDataset.latest_version","title":"<code>latest_version</code>  <code>property</code>","text":"<p>Returns the latest version of the dataset.</p>"},{"location":"api/py_modules/#lance.LanceDataset.max_field_id","title":"<code>max_field_id</code>  <code>property</code>","text":"<p>The max_field_id in manifest</p>"},{"location":"api/py_modules/#lance.LanceDataset.partition_expression","title":"<code>partition_expression</code>  <code>property</code>","text":"<p>Not implemented (just override pyarrow dataset to prevent segfault)</p>"},{"location":"api/py_modules/#lance.LanceDataset.schema","title":"<code>schema</code>  <code>property</code>","text":"<p>The pyarrow Schema for this dataset</p>"},{"location":"api/py_modules/#lance.LanceDataset.stats","title":"<code>stats</code>  <code>property</code>","text":"<p>Experimental API</p>"},{"location":"api/py_modules/#lance.LanceDataset.tags","title":"<code>tags</code>  <code>property</code>","text":"<p>Tag management for the dataset.</p> <p>Similar to Git, tags are a way to add metadata to a specific version of the dataset.</p> <p>.. warning::</p> <pre><code>Tagged versions are exempted from the :py:meth:`cleanup_old_versions()`\nprocess.\n\nTo remove a version that has been tagged, you must first\n:py:meth:`~Tags.delete` the associated tag.\n</code></pre>"},{"location":"api/py_modules/#lance.LanceDataset.tags--examples","title":"Examples","text":"<p>.. code-block:: python</p> <pre><code>ds = lance.open(\"dataset.lance\")\nds.tags.create(\"v2-prod-20250203\", 10)\n\ntags = ds.tags.list()\n</code></pre>"},{"location":"api/py_modules/#lance.LanceDataset.uri","title":"<code>uri</code>  <code>property</code>","text":"<p>The location of the data</p>"},{"location":"api/py_modules/#lance.LanceDataset.version","title":"<code>version</code>  <code>property</code>","text":"<p>Returns the currently checked out version of the dataset</p>"},{"location":"api/py_modules/#lance.LanceDataset.add_columns","title":"<code>add_columns(transforms, read_columns=None, reader_schema=None, batch_size=None)</code>","text":"<p>Add new columns with defined values.</p> <p>There are several ways to specify the new columns. First, you can provide SQL expressions for each new column. Second you can provide a UDF that takes a batch of existing data and returns a new batch with the new columns. These new columns will be appended to the dataset.</p> <p>You can also provide a RecordBatchReader which will read the new column values from some external source.  This is often useful when the new column values have already been staged to files (often by some distributed process)</p> <p>See the :func:<code>lance.add_columns_udf</code> decorator for more information on writing UDFs.</p>"},{"location":"api/py_modules/#lance.LanceDataset.add_columns--parameters","title":"Parameters","text":"<p>transforms : dict or AddColumnsUDF or ReaderLike     If this is a dictionary, then the keys are the names of the new     columns and the values are SQL expression strings. These strings can     reference existing columns in the dataset.     If this is a AddColumnsUDF, then it is a UDF that takes a batch of     existing data and returns a new batch with the new columns.     If this is :class:<code>pyarrow.Field</code> or :class:<code>pyarrow.Schema</code>, it adds     all NULL columns with the given schema, in a metadata-only operation. read_columns : list of str, optional     The names of the columns that the UDF will read. If None, then the     UDF will read all columns. This is only used when transforms is a     UDF. Otherwise, the read columns are inferred from the SQL expressions. reader_schema: pa.Schema, optional     Only valid if transforms is a <code>ReaderLike</code> object.  This will be used to     determine the schema of the reader. batch_size: int, optional     The number of rows to read at a time from the source dataset when applying     the transform.  This is ignored if the dataset is a v1 dataset.</p>"},{"location":"api/py_modules/#lance.LanceDataset.add_columns--examples","title":"Examples","text":"<p>import lance import pyarrow as pa table = pa.table({\"a\": [1, 2, 3]}) dataset = lance.write_dataset(table, \"my_dataset\") @lance.batch_udf() ... def double_a(batch): ...     df = batch.to_pandas() ...     return pd.DataFrame({'double_a': 2 * df['a']}) dataset.add_columns(double_a) dataset.to_table().to_pandas()    a  double_a 0  1         2 1  2         4 2  3         6 dataset.add_columns({\"triple_a\": \"a * 3\"}) dataset.to_table().to_pandas()    a  double_a  triple_a 0  1         2         3 1  2         4         6 2  3         6         9</p>"},{"location":"api/py_modules/#lance.LanceDataset.add_columns--see-also","title":"See Also","text":"<p>LanceDataset.merge :     Merge a pre-computed set of columns into the dataset.</p>"},{"location":"api/py_modules/#lance.LanceDataset.alter_columns","title":"<code>alter_columns(*alterations)</code>","text":"<p>Alter column name, data type, and nullability.</p> <p>Columns that are renamed can keep any indices that are on them. If a column has an IVF_PQ index, it can be kept if the column is casted to another type. However, other index types don't support casting at this time.</p> <p>Column types can be upcasted (such as int32 to int64) or downcasted (such as int64 to int32). However, downcasting will fail if there are any values that cannot be represented in the new type. In general, columns can be casted to same general type: integers to integers, floats to floats, and strings to strings. However, strings, binary, and list columns can be casted between their size variants. For example, string to large string, binary to large binary, and list to large list.</p> <p>Columns that are renamed can keep any indices that are on them. However, if the column is casted to a different type, its indices will be dropped.</p>"},{"location":"api/py_modules/#lance.LanceDataset.alter_columns--parameters","title":"Parameters","text":"<p>alterations : Iterable[Dict[str, Any]]     A sequence of dictionaries, each with the following keys:</p> <pre><code>- \"path\": str\n    The column path to alter. For a top-level column, this is the name.\n    For a nested column, this is the dot-separated path, e.g. \"a.b.c\".\n- \"name\": str, optional\n    The new name of the column. If not specified, the column name is\n    not changed.\n- \"nullable\": bool, optional\n    Whether the column should be nullable. If not specified, the column\n    nullability is not changed. Only non-nullable columns can be changed\n    to nullable. Currently, you cannot change a nullable column to\n    non-nullable.\n- \"data_type\": pyarrow.DataType, optional\n    The new data type to cast the column to. If not specified, the column\n    data type is not changed.\n</code></pre>"},{"location":"api/py_modules/#lance.LanceDataset.alter_columns--examples","title":"Examples","text":"<p>import lance import pyarrow as pa schema = pa.schema([pa.field('a', pa.int64()), ...                     pa.field('b', pa.string(), nullable=False)]) table = pa.table({\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"]}) dataset = lance.write_dataset(table, \"example\") dataset.alter_columns({\"path\": \"a\", \"name\": \"x\"}, ...                       {\"path\": \"b\", \"nullable\": True}) dataset.to_table().to_pandas()    x  b 0  1  a 1  2  b 2  3  c dataset.alter_columns({\"path\": \"x\", \"data_type\": pa.int32()}) dataset.schema x: int32 b: string</p>"},{"location":"api/py_modules/#lance.LanceDataset.checkout_version","title":"<code>checkout_version(version)</code>","text":"<p>Load the given version of the dataset.</p> <p>Unlike the :func:<code>dataset</code> constructor, this will re-use the current cache. This is a no-op if the dataset is already at the given version.</p>"},{"location":"api/py_modules/#lance.LanceDataset.checkout_version--parameters","title":"Parameters","text":"<p>version: int | str,     The version to check out. A version number (<code>int</code>) or a tag     (<code>str</code>) can be provided.</p>"},{"location":"api/py_modules/#lance.LanceDataset.checkout_version--returns","title":"Returns","text":"<p>LanceDataset</p>"},{"location":"api/py_modules/#lance.LanceDataset.cleanup_old_versions","title":"<code>cleanup_old_versions(older_than=None, *, delete_unverified=False, error_if_tagged_old_versions=True)</code>","text":"<p>Cleans up old versions of the dataset.</p> <p>Some dataset changes, such as overwriting, leave behind data that is not referenced by the latest dataset version.  The old data is left in place to allow the dataset to be restored back to an older version.</p> <p>This method will remove older versions and any data files they reference. Once this cleanup task has run you will not be able to checkout or restore these older versions.</p>"},{"location":"api/py_modules/#lance.LanceDataset.cleanup_old_versions--parameters","title":"Parameters","text":"timedelta, optional <p>Only versions older than this will be removed.  If not specified, this will default to two weeks.</p> bool, default False <p>Files leftover from a failed transaction may appear to be part of an in-progress operation (e.g. appending new data) and these files will not be deleted unless they are at least 7 days old.  If delete_unverified is True then these files will be deleted regardless of their age.</p> <p>This should only be set to True if you can guarantee that no other process is currently working on this dataset.  Otherwise the dataset could be put into a corrupted state.</p> bool, default True <p>Some versions may have tags associated with them. Tagged versions will not be cleaned up, regardless of how old they are. If this argument is set to <code>True</code> (the default), an exception will be raised if any tagged versions match the parameters. Otherwise, tagged versions will be ignored without any error and only untagged versions will be cleaned up.</p>"},{"location":"api/py_modules/#lance.LanceDataset.commit","title":"<code>commit(base_uri, operation, blobs_op=None, read_version=None, commit_lock=None, storage_options=None, enable_v2_manifest_paths=None, detached=False, max_retries=20)</code>  <code>staticmethod</code>","text":"<p>Create a new version of dataset</p> <p>This method is an advanced method which allows users to describe a change that has been made to the data files.  This method is not needed when using Lance to apply changes (e.g. when using class:<code>LanceDataset</code> or func:<code>write_dataset</code>.)</p> <p>It's current purpose is to allow for changes being made in a distributed environment where no single process is doing all of the work.  For example, a distributed bulk update or a distributed bulk modify operation.</p> <p>Once all of the changes have been made, this method can be called to make the changes visible by updating the dataset manifest.</p>"},{"location":"api/py_modules/#lance.LanceDataset.commit--warnings","title":"Warnings","text":"<p>This is an advanced API and doesn't provide the same level of validation as the other APIs. For example, it's the responsibility of the caller to ensure that the fragments are valid for the schema.</p>"},{"location":"api/py_modules/#lance.LanceDataset.commit--parameters","title":"Parameters","text":"<p>base_uri: str, Path, or LanceDataset     The base uri of the dataset, or the dataset object itself. Using     the dataset object can be more efficient because it can re-use the     file metadata cache. operation: BaseOperation     The operation to apply to the dataset.  This describes what changes     have been made. See available operations under :class:<code>LanceOperation</code>. read_version: int, optional     The version of the dataset that was used as the base for the changes.     This is not needed for overwrite or restore operations. commit_lock : CommitLock, optional     A custom commit lock.  Only needed if your object store does not support     atomic commits.  See the user guide for more details. storage_options : optional, dict     Extra options that make sense for a particular storage connection. This is     used to store connection parameters like credentials, endpoint, etc. enable_v2_manifest_paths : bool, optional     If True, and this is a new dataset, uses the new V2 manifest paths.     These paths provide more efficient opening of datasets with many     versions on object stores. This parameter has no effect if the dataset     already exists. To migrate an existing dataset, instead use the     :meth:<code>migrate_manifest_paths_v2</code> method. Default is False. WARNING:     turning this on will make the dataset unreadable for older versions     of Lance (prior to 0.17.0). detached : bool, optional     If True, then the commit will not be part of the dataset lineage.  It will     never show up as the latest dataset and the only way to check it out in the     future will be to specifically check it out by version.  The version will be     a random version that is only unique amongst detached commits.  The caller     should store this somewhere as there will be no other way to obtain it in     the future. max_retries : int     The maximum number of retries to perform when committing the dataset.</p>"},{"location":"api/py_modules/#lance.LanceDataset.commit--returns","title":"Returns","text":"<p>LanceDataset     A new version of Lance Dataset.</p>"},{"location":"api/py_modules/#lance.LanceDataset.commit--examples","title":"Examples","text":"<p>Creating a new dataset with the :class:<code>LanceOperation.Overwrite</code> operation:</p> <p>import lance import pyarrow as pa tab1 = pa.table({\"a\": [1, 2], \"b\": [\"a\", \"b\"]}) tab2 = pa.table({\"a\": [3, 4], \"b\": [\"c\", \"d\"]}) fragment1 = lance.fragment.LanceFragment.create(\"example\", tab1) fragment2 = lance.fragment.LanceFragment.create(\"example\", tab2) fragments = [fragment1, fragment2] operation = lance.LanceOperation.Overwrite(tab1.schema, fragments) dataset = lance.LanceDataset.commit(\"example\", operation) dataset.to_table().to_pandas()    a  b 0  1  a 1  2  b 2  3  c 3  4  d</p>"},{"location":"api/py_modules/#lance.LanceDataset.commit_batch","title":"<code>commit_batch(dest, transactions, commit_lock=None, storage_options=None, enable_v2_manifest_paths=None, detached=False, max_retries=20)</code>  <code>staticmethod</code>","text":"<p>Create a new version of dataset with multiple transactions.</p> <p>This method is an advanced method which allows users to describe a change that has been made to the data files.  This method is not needed when using Lance to apply changes (e.g. when using class:<code>LanceDataset</code> or func:<code>write_dataset</code>.)</p>"},{"location":"api/py_modules/#lance.LanceDataset.commit_batch--parameters","title":"Parameters","text":"<p>dest: str, Path, or LanceDataset     The base uri of the dataset, or the dataset object itself. Using     the dataset object can be more efficient because it can re-use the     file metadata cache. transactions: Iterable[Transaction]     The transactions to apply to the dataset. These will be merged into     a single transaction and applied to the dataset. Note: Only append     transactions are currently supported. Other transaction types will be     supported in the future. commit_lock : CommitLock, optional     A custom commit lock.  Only needed if your object store does not support     atomic commits.  See the user guide for more details. storage_options : optional, dict     Extra options that make sense for a particular storage connection. This is     used to store connection parameters like credentials, endpoint, etc. enable_v2_manifest_paths : bool, optional     If True, and this is a new dataset, uses the new V2 manifest paths.     These paths provide more efficient opening of datasets with many     versions on object stores. This parameter has no effect if the dataset     already exists. To migrate an existing dataset, instead use the     :meth:<code>migrate_manifest_paths_v2</code> method. Default is False. WARNING:     turning this on will make the dataset unreadable for older versions     of Lance (prior to 0.17.0). detached : bool, optional     If True, then the commit will not be part of the dataset lineage.  It will     never show up as the latest dataset and the only way to check it out in the     future will be to specifically check it out by version.  The version will be     a random version that is only unique amongst detached commits.  The caller     should store this somewhere as there will be no other way to obtain it in     the future. max_retries : int     The maximum number of retries to perform when committing the dataset.</p>"},{"location":"api/py_modules/#lance.LanceDataset.commit_batch--returns","title":"Returns","text":"<p>dict with keys:     dataset: LanceDataset         A new version of Lance Dataset.     merged: Transaction         The merged transaction that was applied to the dataset.</p>"},{"location":"api/py_modules/#lance.LanceDataset.count_rows","title":"<code>count_rows(filter=None, **kwargs)</code>","text":"<p>Count rows matching the scanner filter.</p>"},{"location":"api/py_modules/#lance.LanceDataset.count_rows--parameters","title":"Parameters","text":"<p>**kwargs : dict, optional     See py:method:<code>scanner</code> method for full parameter description.</p>"},{"location":"api/py_modules/#lance.LanceDataset.count_rows--returns","title":"Returns","text":"<p>count : int     The total number of rows in the dataset.</p>"},{"location":"api/py_modules/#lance.LanceDataset.create_index","title":"<code>create_index(column, index_type, name=None, metric='L2', replace=False, num_partitions=None, ivf_centroids=None, pq_codebook=None, num_sub_vectors=None, accelerator=None, index_cache_size=None, shuffle_partition_batches=None, shuffle_partition_concurrency=None, ivf_centroids_file=None, precomputed_partition_dataset=None, storage_options=None, filter_nan=True, one_pass_ivfpq=False, **kwargs)</code>","text":"<p>Create index on column.</p> <p>Experimental API</p>"},{"location":"api/py_modules/#lance.LanceDataset.create_index--parameters","title":"Parameters","text":"<p>column : str     The column to be indexed. index_type : str     The type of the index.     <code>\"IVF_PQ, IVF_HNSW_PQ and IVF_HNSW_SQ\"</code> are supported now. name : str, optional     The index name. If not provided, it will be generated from the     column name. metric : str     The distance metric type, i.e., \"L2\" (alias to \"euclidean\"), \"cosine\"     or \"dot\" (dot product). Default is \"L2\". replace : bool     Replace the existing index if it exists. num_partitions : int, optional     The number of partitions of IVF (Inverted File Index). ivf_centroids : optional     It can be either class:<code>np.ndarray</code>,     class:<code>pyarrow.FixedSizeListArray</code> or     class:<code>pyarrow.FixedShapeTensorArray</code>.     A <code>num_partitions x dimension</code> array of existing K-mean centroids     for IVF clustering. If not provided, a new KMeans model will be trained. pq_codebook : optional,     It can be class:<code>np.ndarray</code>, class:<code>pyarrow.FixedSizeListArray</code>,     or class:<code>pyarrow.FixedShapeTensorArray</code>.     A <code>num_sub_vectors x (2 ^ nbits * dimensions // num_sub_vectors)</code>     array of K-mean centroids for PQ codebook.</p> <pre><code>Note: ``nbits`` is always 8 for now.\nIf not provided, a new PQ model will be trained.\n</code></pre> <p>num_sub_vectors : int, optional     The number of sub-vectors for PQ (Product Quantization). accelerator : str or <code>torch.Device</code>, optional     If set, use an accelerator to speed up the training process.     Accepted accelerator: \"cuda\" (Nvidia GPU) and \"mps\" (Apple Silicon GPU).     If not set, use the CPU. index_cache_size : int, optional     The size of the index cache in number of entries. Default value is 256. shuffle_partition_batches : int, optional     The number of batches, using the row group size of the dataset, to include     in each shuffle partition. Default value is 10240.</p> <pre><code>Assuming the row group size is 1024, each shuffle partition will hold\n10240 * 1024 = 10,485,760 rows. By making this value smaller, this shuffle\nwill consume less memory but will take longer to complete, and vice versa.\n</code></pre> <p>shuffle_partition_concurrency : int, optional     The number of shuffle partitions to process concurrently. Default value is 2</p> <pre><code>By making this value smaller, this shuffle will consume less memory but will\ntake longer to complete, and vice versa.\n</code></pre> <p>storage_options : optional, dict     Extra options that make sense for a particular storage connection. This is     used to store connection parameters like credentials, endpoint, etc. filter_nan: bool     Defaults to True. False is UNSAFE, and will cause a crash if any null/nan     values are present (and otherwise will not). Disables the null filter used     for nullable columns. Obtains a small speed boost. one_pass_ivfpq: bool     Defaults to False. If enabled, index type must be \"IVF_PQ\". Reduces disk IO. kwargs :     Parameters passed to the index building process.</p> <p>The SQ (Scalar Quantization) is available for only <code>IVF_HNSW_SQ</code> index type, this quantization method is used to reduce the memory usage of the index, it maps the float vectors to integer vectors, each integer is of <code>num_bits</code>, now only 8 bits are supported.</p> <p>If <code>index_type</code> is \"IVF_*\", then the following parameters are required:     num_partitions</p> <p>If <code>index_type</code> is with \"PQ\", then the following parameters are required:     num_sub_vectors</p> <p>Optional parameters for <code>IVF_PQ</code>:</p> <pre><code>- ivf_centroids\n    Existing K-mean centroids for IVF clustering.\n- num_bits\n    The number of bits for PQ (Product Quantization). Default is 8.\n    Only 4, 8 are supported.\n</code></pre> <p>Optional parameters for <code>IVF_HNSW_*</code>:     max_level         Int, the maximum number of levels in the graph.     m         Int, the number of edges per node in the graph.     ef_construction         Int, the number of nodes to examine during the construction.</p>"},{"location":"api/py_modules/#lance.LanceDataset.create_index--examples","title":"Examples","text":"<p>.. code-block:: python</p> <pre><code>import lance\n\ndataset = lance.dataset(\"/tmp/sift.lance\")\ndataset.create_index(\n    \"vector\",\n    \"IVF_PQ\",\n    num_partitions=256,\n    num_sub_vectors=16\n)\n</code></pre> <p>.. code-block:: python</p> <pre><code>import lance\n\ndataset = lance.dataset(\"/tmp/sift.lance\")\ndataset.create_index(\n    \"vector\",\n    \"IVF_HNSW_SQ\",\n    num_partitions=256,\n)\n</code></pre> <p>Experimental Accelerator (GPU) support:</p> <ul> <li>accelerate: use GPU to train IVF partitions.     Only supports CUDA (Nvidia) or MPS (Apple) currently.     Requires PyTorch being installed.</li> </ul> <p>.. code-block:: python</p> <pre><code>import lance\n\ndataset = lance.dataset(\"/tmp/sift.lance\")\ndataset.create_index(\n    \"vector\",\n    \"IVF_PQ\",\n    num_partitions=256,\n    num_sub_vectors=16,\n    accelerator=\"cuda\"\n)\n</code></pre>"},{"location":"api/py_modules/#lance.LanceDataset.create_index--references","title":"References","text":"<ul> <li><code>Faiss Index &lt;https://github.com/facebookresearch/faiss/wiki/Faiss-indexes&gt;</code>_</li> <li>IVF introduced in <code>Video Google: a text retrieval approach to object matching   in videos &lt;https://ieeexplore.ieee.org/abstract/document/1238663&gt;</code>_</li> <li><code>Product quantization for nearest neighbor search   &lt;https://hal.inria.fr/inria-00514462v2/document&gt;</code>_</li> </ul>"},{"location":"api/py_modules/#lance.LanceDataset.create_scalar_index","title":"<code>create_scalar_index(column, index_type, name=None, *, replace=True, **kwargs)</code>","text":"<p>Create a scalar index on a column.</p> <p>Scalar indices, like vector indices, can be used to speed up scans.  A scalar index can speed up scans that contain filter expressions on the indexed column. For example, the following scan will be faster if the column <code>my_col</code> has a scalar index:</p> <p>.. code-block:: python</p> <pre><code>import lance\n\ndataset = lance.dataset(\"/tmp/images.lance\")\nmy_table = dataset.scanner(filter=\"my_col != 7\").to_table()\n</code></pre> <p>Vector search with pre-filers can also benefit from scalar indices. For example,</p> <p>.. code-block:: python</p> <pre><code>import lance\n\ndataset = lance.dataset(\"/tmp/images.lance\")\nmy_table = dataset.scanner(\n    nearest=dict(\n       column=\"vector\",\n       q=[1, 2, 3, 4],\n       k=10,\n    )\n    filter=\"my_col != 7\",\n    prefilter=True\n)\n</code></pre> <p>There are 5 types of scalar indices available today.</p> <ul> <li><code>BTREE</code>. The most common type is <code>BTREE</code>. This index is inspired   by the btree data structure although only the first few layers of the btree   are cached in memory.  It will   perform well on columns with a large number of unique values and few rows per   value.</li> <li><code>BITMAP</code>. This index stores a bitmap for each unique value in the column.   This index is useful for columns with a small number of unique values and   many rows per value.</li> <li><code>LABEL_LIST</code>. A special index that is used to index list   columns whose values have small cardinality.  For example, a column that   contains lists of tags (e.g. <code>[\"tag1\", \"tag2\", \"tag3\"]</code>) can be indexed   with a <code>LABEL_LIST</code> index.  This index can only speedup queries with   <code>array_has_any</code> or <code>array_has_all</code> filters.</li> <li><code>NGRAM</code>. A special index that is used to index string columns.  This index   creates a bitmap for each ngram in the string.  By default we use trigrams.   This index can currently speed up queries using the <code>contains</code> function   in filters.</li> <li><code>FTS/INVERTED</code>. It is used to index document columns. This index   can conduct full-text searches. For example, a column that contains any word   of query string \"hello world\". The results will be ranked by BM25.</li> </ul> <p>Note that the <code>LANCE_BYPASS_SPILLING</code> environment variable can be used to bypass spilling to disk. Setting this to true can avoid memory exhaustion issues (see https://github.com/apache/datafusion/issues/10073 for more info).</p> <p>Experimental API</p>"},{"location":"api/py_modules/#lance.LanceDataset.create_scalar_index--parameters","title":"Parameters","text":"<p>column : str     The column to be indexed.  Must be a boolean, integer, float,     or string column. index_type : str     The type of the index.  One of <code>\"BTREE\"</code>, <code>\"BITMAP\"</code>,     <code>\"LABEL_LIST\"</code>, <code>\"NGRAM\"</code>, <code>\"FTS\"</code> or <code>\"INVERTED\"</code>. name : str, optional     The index name. If not provided, it will be generated from the     column name. replace : bool, default True     Replace the existing index if it exists.</p> bool, default True <p>This is for the <code>INVERTED</code> index. If True, the index will store the positions of the words in the document, so that you can conduct phrase query. This will significantly increase the index size. It won't impact the performance of non-phrase queries even if it is set to True.</p> <p>base_tokenizer: str, default \"simple\"     This is for the <code>INVERTED</code> index. The base tokenizer to use. The value     can be:     * \"simple\": splits tokens on whitespace and punctuation.     * \"whitespace\": splits tokens on whitespace.     * \"raw\": no tokenization. language: str, default \"English\"     This is for the <code>INVERTED</code> index. The language for stemming     and stop words. This is only used when <code>stem</code> or <code>remove_stop_words</code> is true max_token_length: Optional[int], default 40     This is for the <code>INVERTED</code> index. The maximum token length.     Any token longer than this will be removed. lower_case: bool, default True     This is for the <code>INVERTED</code> index. If True, the index will convert all     text to lowercase. stem: bool, default False     This is for the <code>INVERTED</code> index. If True, the index will stem the     tokens. remove_stop_words: bool, default False     This is for the <code>INVERTED</code> index. If True, the index will remove     stop words. ascii_folding: bool, default False     This is for the <code>INVERTED</code> index. If True, the index will convert     non-ascii characters to ascii characters if possible.     This would remove accents like \"\u00e9\" -&gt; \"e\".</p>"},{"location":"api/py_modules/#lance.LanceDataset.create_scalar_index--examples","title":"Examples","text":"<p>.. code-block:: python</p> <pre><code>import lance\n\ndataset = lance.dataset(\"/tmp/images.lance\")\ndataset.create_index(\n    \"category\",\n    \"BTREE\",\n)\n</code></pre> <p>Scalar indices can only speed up scans for basic filters using equality, comparison, range (e.g. <code>my_col BETWEEN 0 AND 100</code>), and set membership (e.g. <code>my_col IN (0, 1, 2)</code>)</p> <p>Scalar indices can be used if the filter contains multiple indexed columns and the filter criteria are AND'd or OR'd together (e.g. <code>my_col &lt; 0 AND other_col&gt; 100</code>)</p> <p>Scalar indices may be used if the filter contains non-indexed columns but, depending on the structure of the filter, they may not be usable.  For example, if the column <code>not_indexed</code> does not have a scalar index then the filter <code>my_col = 0 OR not_indexed = 1</code> will not be able to use any scalar index on <code>my_col</code>.</p> <p>To determine if a scan is making use of a scalar index you can use <code>explain_plan</code> to look at the query plan that lance has created.  Queries that use scalar indices will either have a <code>ScalarIndexQuery</code> relation or a <code>MaterializeIndex</code> operator.</p>"},{"location":"api/py_modules/#lance.LanceDataset.delete","title":"<code>delete(predicate)</code>","text":"<p>Delete rows from the dataset.</p> <p>This marks rows as deleted, but does not physically remove them from the files. This keeps the existing indexes still valid.</p>"},{"location":"api/py_modules/#lance.LanceDataset.delete--parameters","title":"Parameters","text":"<p>predicate : str or pa.compute.Expression     The predicate to use to select rows to delete. May either be a SQL     string or a pyarrow Expression.</p>"},{"location":"api/py_modules/#lance.LanceDataset.delete--examples","title":"Examples","text":"<p>import lance import pyarrow as pa table = pa.table({\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"]}) dataset = lance.write_dataset(table, \"example\") dataset.delete(\"a = 1 or b in ('a', 'b')\") dataset.to_table() pyarrow.Table a: int64 b: string</p> <p>a: [[3]] b: [[\"c\"]]</p>"},{"location":"api/py_modules/#lance.LanceDataset.drop_columns","title":"<code>drop_columns(columns)</code>","text":"<p>Drop one or more columns from the dataset</p> <p>This is a metadata-only operation and does not remove the data from the underlying storage. In order to remove the data, you must subsequently call <code>compact_files</code> to rewrite the data without the removed columns and then call <code>cleanup_old_versions</code> to remove the old files.</p>"},{"location":"api/py_modules/#lance.LanceDataset.drop_columns--parameters","title":"Parameters","text":"<p>columns : list of str     The names of the columns to drop. These can be nested column references     (e.g. \"a.b.c\") or top-level column names (e.g. \"a\").</p>"},{"location":"api/py_modules/#lance.LanceDataset.drop_columns--examples","title":"Examples","text":"<p>import lance import pyarrow as pa table = pa.table({\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"]}) dataset = lance.write_dataset(table, \"example\") dataset.drop_columns([\"a\"]) dataset.to_table().to_pandas()    b 0  a 1  b 2  c</p>"},{"location":"api/py_modules/#lance.LanceDataset.drop_index","title":"<code>drop_index(name)</code>","text":"<p>Drops an index from the dataset</p> <p>Note: Indices are dropped by \"index name\".  This is not the same as the field name. If you did not specify a name when you created the index then a name was generated for you.  You can use the <code>list_indices</code> method to get the names of the indices.</p>"},{"location":"api/py_modules/#lance.LanceDataset.get_fragment","title":"<code>get_fragment(fragment_id)</code>","text":"<p>Get the fragment with fragment id.</p>"},{"location":"api/py_modules/#lance.LanceDataset.get_fragments","title":"<code>get_fragments(filter=None)</code>","text":"<p>Get all fragments from the dataset.</p> <p>Note: filter is not supported yet.</p>"},{"location":"api/py_modules/#lance.LanceDataset.head","title":"<code>head(num_rows, **kwargs)</code>","text":"<p>Load the first N rows of the dataset.</p>"},{"location":"api/py_modules/#lance.LanceDataset.head--parameters","title":"Parameters","text":"<p>num_rows : int     The number of rows to load. **kwargs : dict, optional     See scanner() method for full parameter description.</p>"},{"location":"api/py_modules/#lance.LanceDataset.head--returns","title":"Returns","text":"<p>table : Table</p>"},{"location":"api/py_modules/#lance.LanceDataset.insert","title":"<code>insert(data, *, mode='append', **kwargs)</code>","text":"<p>Insert data into the dataset.</p>"},{"location":"api/py_modules/#lance.LanceDataset.insert--parameters","title":"Parameters","text":"<p>data_obj: Reader-like     The data to be written. Acceptable types are:     - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader     - Huggingface dataset mode: str, default 'append'     The mode to use when writing the data. Options are:         create - create a new dataset (raises if uri already exists).         overwrite - create a new snapshot version         append - create a new version that is the concat of the input the         latest version (raises if uri does not exist) **kwargs : dict, optional     Additional keyword arguments to pass to :func:<code>write_dataset</code>.</p>"},{"location":"api/py_modules/#lance.LanceDataset.join","title":"<code>join(right_dataset, keys, right_keys=None, join_type='left outer', left_suffix=None, right_suffix=None, coalesce_keys=True, use_threads=True)</code>","text":"<p>Not implemented (just override pyarrow dataset to prevent segfault)</p>"},{"location":"api/py_modules/#lance.LanceDataset.merge","title":"<code>merge(data_obj, left_on, right_on=None, schema=None)</code>","text":"<p>Merge another dataset into this one.</p> <p>Performs a left join, where the dataset is the left side and data_obj is the right side. Rows existing in the dataset but not on the left will be filled with null values, unless Lance doesn't support null values for some types, in which case an error will be raised.</p>"},{"location":"api/py_modules/#lance.LanceDataset.merge--parameters","title":"Parameters","text":"<p>data_obj: Reader-like     The data to be merged. Acceptable types are:     - Pandas DataFrame, Pyarrow Table, Dataset, Scanner,     Iterator[RecordBatch], or RecordBatchReader left_on: str     The name of the column in the dataset to join on. right_on: str or None     The name of the column in data_obj to join on. If None, defaults to     left_on.</p>"},{"location":"api/py_modules/#lance.LanceDataset.merge--examples","title":"Examples","text":"<p>import lance import pyarrow as pa df = pa.table({'x': [1, 2, 3], 'y': ['a', 'b', 'c']}) dataset = lance.write_dataset(df, \"dataset\") dataset.to_table().to_pandas()    x  y 0  1  a 1  2  b 2  3  c new_df = pa.table({'x': [1, 2, 3], 'z': ['d', 'e', 'f']}) dataset.merge(new_df, 'x') dataset.to_table().to_pandas()    x  y  z 0  1  a  d 1  2  b  e 2  3  c  f</p>"},{"location":"api/py_modules/#lance.LanceDataset.merge--see-also","title":"See Also","text":"<p>LanceDataset.add_columns :     Add new columns by computing batch-by-batch.</p>"},{"location":"api/py_modules/#lance.LanceDataset.merge_insert","title":"<code>merge_insert(on)</code>","text":"<p>Returns a builder that can be used to create a \"merge insert\" operation</p> <p>This operation can add rows, update rows, and remove rows in a single transaction. It is a very generic tool that can be used to create behaviors like \"insert if not exists\", \"update or insert (i.e. upsert)\", or even replace a portion of existing data with new data (e.g. replace all data where month=\"january\")</p> <p>The merge insert operation works by combining new data from a source table with existing data in a target table by using a join.  There are three categories of records.</p> <p>\"Matched\" records are records that exist in both the source table and the target table. \"Not matched\" records exist only in the source table (e.g. these are new data). \"Not matched by source\" records exist only in the target table (this is old data).</p> <p>The builder returned by this method can be used to customize what should happen for each category of data.</p> <p>Please note that the data will be reordered as part of this operation.  This is because updated rows will be deleted from the dataset and then reinserted at the end with the new values.  The order of the newly inserted rows may fluctuate randomly because a hash-join operation is used internally.</p>"},{"location":"api/py_modules/#lance.LanceDataset.merge_insert--parameters","title":"Parameters","text":"Union[str, Iterable[str]] <p>A column (or columns) to join on.  This is how records from the source table and target table are matched.  Typically this is some kind of key or id column.</p>"},{"location":"api/py_modules/#lance.LanceDataset.merge_insert--examples","title":"Examples","text":"<p>Use <code>when_matched_update_all()</code> and <code>when_not_matched_insert_all()</code> to perform an \"upsert\" operation.  This will update rows that already exist in the dataset and insert rows that do not exist.</p> <p>import lance import pyarrow as pa table = pa.table({\"a\": [2, 1, 3], \"b\": [\"a\", \"b\", \"c\"]}) dataset = lance.write_dataset(table, \"example\") new_table = pa.table({\"a\": [2, 3, 4], \"b\": [\"x\", \"y\", \"z\"]})</p> <p>Use <code>when_not_matched_insert_all()</code> to perform an \"insert if not exists\" operation.  This will only insert rows that do not already exist in the dataset.</p> <p>import lance import pyarrow as pa table = pa.table({\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"]}) dataset = lance.write_dataset(table, \"example2\") new_table = pa.table({\"a\": [2, 3, 4], \"b\": [\"x\", \"y\", \"z\"]})</p> <p>You are not required to provide all the columns. If you only want to update a subset of columns, you can omit columns you don't want to update. Omitted columns will keep their existing values if they are updated, or will be null if they are inserted.</p> <p>import lance import pyarrow as pa table = pa.table({\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"], \\ ...                   \"c\": [\"x\", \"y\", \"z\"]}) dataset = lance.write_dataset(table, \"example3\") new_table = pa.table({\"a\": [2, 3, 4], \"b\": [\"x\", \"y\", \"z\"]})</p>"},{"location":"api/py_modules/#lance.LanceDataset.merge_insert--perform-a-upsert-operation","title":"Perform a \"upsert\" operation","text":"<p>dataset.merge_insert(\"a\")     \\ ...             .when_matched_update_all()     \\ ...             .when_not_matched_insert_all() \\ ...             .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0} dataset.to_table().sort_by(\"a\").to_pandas()    a  b 0  1  b 1  2  x 2  3  y 3  4  z</p>"},{"location":"api/py_modules/#lance.LanceDataset.merge_insert--perform-an-insert-if-not-exists-operation","title":"Perform an \"insert if not exists\" operation","text":"<p>dataset.merge_insert(\"a\")     \\ ...             .when_not_matched_insert_all() \\ ...             .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 0, 'num_deleted_rows': 0} dataset.to_table().sort_by(\"a\").to_pandas()    a  b 0  1  a 1  2  b 2  3  c 3  4  z</p>"},{"location":"api/py_modules/#lance.LanceDataset.merge_insert--perform-an-upsert-operation-only-updating-column-a","title":"Perform an \"upsert\" operation, only updating column \"a\"","text":"<p>dataset.merge_insert(\"a\")     \\ ...             .when_matched_update_all()     \\ ...             .when_not_matched_insert_all() \\ ...             .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0} dataset.to_table().sort_by(\"a\").to_pandas()    a  b     c 0  1  a     x 1  2  x     y 2  3  y     z 3  4  z  None</p>"},{"location":"api/py_modules/#lance.LanceDataset.migrate_manifest_paths_v2","title":"<code>migrate_manifest_paths_v2()</code>","text":"<p>Migrate the manifest paths to the new format.</p> <p>This will update the manifest to use the new v2 format for paths.</p> <p>This function is idempotent, and can be run multiple times without changing the state of the object store.</p> <p>DANGER: this should not be run while other concurrent operations are happening. And it should also run until completion before resuming other operations.</p>"},{"location":"api/py_modules/#lance.LanceDataset.prewarm_index","title":"<code>prewarm_index(name)</code>","text":"<p>Prewarm an index</p> <p>This will load the entire index into memory.  This can help avoid cold start issues with index queries.  If the index does not fit in the index cache, then this will result in wasted I/O.</p>"},{"location":"api/py_modules/#lance.LanceDataset.prewarm_index--parameters","title":"Parameters","text":"<p>name: str     The name of the index to prewarm.</p>"},{"location":"api/py_modules/#lance.LanceDataset.replace_field_metadata","title":"<code>replace_field_metadata(field_name, new_metadata)</code>","text":"<p>Replace the metadata of a field in the schema</p>"},{"location":"api/py_modules/#lance.LanceDataset.replace_field_metadata--parameters","title":"Parameters","text":"<p>field_name: str     The name of the field to replace the metadata for new_metadata: dict     The new metadata to set</p>"},{"location":"api/py_modules/#lance.LanceDataset.replace_schema","title":"<code>replace_schema(schema)</code>","text":"<p>Not implemented (just override pyarrow dataset to prevent segfault)</p> <p>See method:<code>replace_schema_metadata</code> or method:<code>replace_field_metadata</code></p>"},{"location":"api/py_modules/#lance.LanceDataset.replace_schema_metadata","title":"<code>replace_schema_metadata(new_metadata)</code>","text":"<p>Replace the schema metadata of the dataset</p>"},{"location":"api/py_modules/#lance.LanceDataset.replace_schema_metadata--parameters","title":"Parameters","text":"<p>new_metadata: dict     The new metadata to set</p>"},{"location":"api/py_modules/#lance.LanceDataset.restore","title":"<code>restore()</code>","text":"<p>Restore the currently checked out version as the latest version of the dataset.</p> <p>This creates a new commit.</p>"},{"location":"api/py_modules/#lance.LanceDataset.sample","title":"<code>sample(num_rows, columns=None, randomize_order=True, **kwargs)</code>","text":"<p>Select a random sample of data</p>"},{"location":"api/py_modules/#lance.LanceDataset.sample--parameters","title":"Parameters","text":"<p>num_rows: int     number of rows to retrieve columns: list of str, or dict of str to str default None     List of column names to be fetched.     Or a dictionary of column names to SQL expressions.     All columns are fetched if None or unspecified. **kwargs : dict, optional     see scanner() method for full parameter description.</p>"},{"location":"api/py_modules/#lance.LanceDataset.sample--returns","title":"Returns","text":"<p>table : Table</p>"},{"location":"api/py_modules/#lance.LanceDataset.scanner","title":"<code>scanner(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, fragments=None, full_text_query=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, fast_search=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, include_deleted_rows=None, scan_stats_callback=None, strict_batch_size=None)</code>","text":"<p>Return a Scanner that can support various pushdowns.</p>"},{"location":"api/py_modules/#lance.LanceDataset.scanner--parameters","title":"Parameters","text":"<p>columns: list of str, or dict of str to str default None     List of column names to be fetched.     Or a dictionary of column names to SQL expressions.     All columns are fetched if None or unspecified. filter: pa.compute.Expression or str     Expression or str that is a valid SQL where clause. See     <code>Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;</code>_     for valid SQL expressions. limit: int, default None     Fetch up to this many rows. All rows if None or unspecified. offset: int, default None     Fetch starting with this row. 0 if None or unspecified. nearest: dict, default None     Get the rows corresponding to the K most similar vectors. Example:</p> <pre><code>.. code-block:: python\n\n    {\n        \"column\": &lt;embedding col name&gt;,\n        \"q\": &lt;query vector as pa.Float32Array&gt;,\n        \"k\": 10,\n        \"nprobes\": 1,\n        \"refine_factor\": 1\n    }\n</code></pre> int, default None <p>The target size of batches returned.  In some cases batches can be up to twice this size (but never larger than this).  In some cases batches can be smaller than this size.</p> <p>io_buffer_size: int, default None     The size of the IO buffer.  See <code>ScannerBuilder.io_buffer_size</code>     for more information. batch_readahead: int, optional     The number of batches to read ahead. fragment_readahead: int, optional     The number of fragments to read ahead. scan_in_order: bool, default True     Whether to read the fragments and batches in order. If false,     throughput may be higher, but batches will be returned out of order     and memory use might increase. fragments: iterable of LanceFragment, default None     If specified, only scan these fragments. If scan_in_order is True, then     the fragments will be scanned in the order given. prefilter: bool, default False     If True then the filter will be applied before the vector query is run.     This will generate more correct results but it may be a more costly     query.  It's generally good when the filter is highly selective.</p> <pre><code>If False then the filter will be applied after the vector query is run.\nThis will perform well but the results may have fewer than the requested\nnumber of rows (or be empty) if the rows closest to the query do not\nmatch the filter.  It's generally good when the filter is not very\nselective.\n</code></pre> <p>use_scalar_index: bool, default True     Lance will automatically use scalar indices to optimize a query.  In some     corner cases this can make query performance worse and this parameter can     be used to disable scalar indices in these cases. late_materialization: bool or List[str], default None     Allows custom control over late materialization.  Late materialization     fetches non-query columns using a take operation after the filter.  This     is useful when there are few results or columns are very large.</p> <pre><code>Early materialization can be better when there are many results or the\ncolumns are very narrow.\n\nIf True, then all columns are late materialized.\nIf False, then all columns are early materialized.\nIf a list of strings, then only the columns in the list are\nlate materialized.\n\nThe default uses a heuristic that assumes filters will select about 0.1%\nof the rows.  If your filter is more selective (e.g. find by id) you may\nwant to set this to True.  If your filter is not very selective (e.g.\nmatches 20% of the rows) you may want to set this to False.\n</code></pre> <p>full_text_query: str or dict, optional     query string to search for, the results will be ranked by BM25.     e.g. \"hello world\", would match documents containing \"hello\" or \"world\".     or a dictionary with the following keys:</p> <pre><code>- columns: list[str]\n    The columns to search,\n    currently only supports a single column in the columns list.\n- query: str\n    The query string to search for.\n</code></pre> <p>fast_search:  bool, default False     If True, then the search will only be performed on the indexed data, which     yields faster search time. scan_stats_callback: Callable[[ScanStatistics], None], default None     A callback function that will be called with the scan statistics after the     scan is complete.  Errors raised by the callback will be logged but not     re-raised. include_deleted_rows: bool, default False     If True, then rows that have been deleted, but are still present in the     fragment, will be returned.  These rows will have the _rowid column set     to null.  All other columns will reflect the value stored on disk and may     not be null.</p> <pre><code>Note: if this is a search operation, or a take operation (including scalar\nindexed scans) then deleted rows cannot be returned.\n</code></pre> <p>.. note::</p> <pre><code>For now, if BOTH filter and nearest is specified, then:\n\n1. nearest is executed first.\n2. The results are filtered afterwards.\n</code></pre> <p>For debugging ANN results, you can choose to not use the index even if present by specifying <code>use_index=False</code>. For example, the following will always return exact KNN results:</p> <p>.. code-block:: python</p> <pre><code>dataset.to_table(nearest={\n    \"column\": \"vector\",\n    \"k\": 10,\n    \"q\": &lt;query vector&gt;,\n    \"use_index\": False\n}\n</code></pre>"},{"location":"api/py_modules/#lance.LanceDataset.session","title":"<code>session()</code>","text":"<p>Return the dataset session, which holds the dataset's state.</p>"},{"location":"api/py_modules/#lance.LanceDataset.take","title":"<code>take(indices, columns=None)</code>","text":"<p>Select rows of data by index.</p>"},{"location":"api/py_modules/#lance.LanceDataset.take--parameters","title":"Parameters","text":"<p>indices : Array or array-like     indices of rows to select in the dataset. columns: list of str, or dict of str to str default None     List of column names to be fetched.     Or a dictionary of column names to SQL expressions.     All columns are fetched if None or unspecified.</p>"},{"location":"api/py_modules/#lance.LanceDataset.take--returns","title":"Returns","text":"<p>table : pyarrow.Table</p>"},{"location":"api/py_modules/#lance.LanceDataset.take_blobs","title":"<code>take_blobs(blob_column, ids=None, addresses=None, indices=None)</code>","text":"<p>Select blobs by row IDs.</p> <p>Instead of loading large binary blob data into memory before processing it, this API allows you to open binary blob data as a regular Python file-like object. For more details, see class:<code>lance.BlobFile</code>.</p> <p>Exactly one of ids, addresses, or indices must be specified. Parameters</p> <p>blob_column : str     The name of the blob column to select. ids : Integer Array or array-like     row IDs to select in the dataset. addresses: Integer Array or array-like     The (unstable) row addresses to select in the dataset. indices : Integer Array or array-like     The offset / indices of the row in the dataset.</p>"},{"location":"api/py_modules/#lance.LanceDataset.take_blobs--returns","title":"Returns","text":"<p>blob_files : List[BlobFile]</p>"},{"location":"api/py_modules/#lance.LanceDataset.to_batches","title":"<code>to_batches(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, full_text_query=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, strict_batch_size=None, **kwargs)</code>","text":"<p>Read the dataset as materialized record batches.</p>"},{"location":"api/py_modules/#lance.LanceDataset.to_batches--parameters","title":"Parameters","text":"<p>**kwargs : dict, optional     Arguments for meth:<code>~LanceDataset.scanner</code>.</p>"},{"location":"api/py_modules/#lance.LanceDataset.to_batches--returns","title":"Returns","text":"<p>record_batches : Iterator of class:<code>~pyarrow.RecordBatch</code></p>"},{"location":"api/py_modules/#lance.LanceDataset.to_table","title":"<code>to_table(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, fast_search=None, full_text_query=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, include_deleted_rows=None)</code>","text":"<p>Read the data into memory as a class:<code>pyarrow.Table</code></p>"},{"location":"api/py_modules/#lance.LanceDataset.to_table--parameters","title":"Parameters","text":"<p>columns: list of str, or dict of str to str default None     List of column names to be fetched.     Or a dictionary of column names to SQL expressions.     All columns are fetched if None or unspecified. filter : pa.compute.Expression or str     Expression or str that is a valid SQL where clause. See     <code>Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;</code>_     for valid SQL expressions. limit: int, default None     Fetch up to this many rows. All rows if None or unspecified. offset: int, default None     Fetch starting with this row. 0 if None or unspecified. nearest: dict, default None     Get the rows corresponding to the K most similar vectors. Example:</p> <pre><code>.. code-block:: python\n\n    {\n        \"column\": &lt;embedding col name&gt;,\n        \"q\": &lt;query vector as pa.Float32Array&gt;,\n        \"k\": 10,\n        \"metric\": \"cosine\",\n        \"nprobes\": 1,\n        \"refine_factor\": 1\n    }\n</code></pre> int, optional <p>The number of rows to read at a time.</p> <p>io_buffer_size: int, default None     The size of the IO buffer.  See <code>ScannerBuilder.io_buffer_size</code>     for more information. batch_readahead: int, optional     The number of batches to read ahead. fragment_readahead: int, optional     The number of fragments to read ahead. scan_in_order: bool, optional, default True     Whether to read the fragments and batches in order. If false,     throughput may be higher, but batches will be returned out of order     and memory use might increase. prefilter: bool, optional, default False     Run filter before the vector search. late_materialization: bool or List[str], default None     Allows custom control over late materialization.  See     <code>ScannerBuilder.late_materialization</code> for more information. use_scalar_index: bool, default True     Allows custom control over scalar index usage.  See     <code>ScannerBuilder.use_scalar_index</code> for more information. with_row_id: bool, optional, default False     Return row ID. with_row_address: bool, optional, default False     Return row address use_stats: bool, optional, default True     Use stats pushdown during filters. fast_search: bool, optional, default False full_text_query: str or dict, optional     query string to search for, the results will be ranked by BM25.     e.g. \"hello world\", would match documents contains \"hello\" or \"world\".     or a dictionary with the following keys:</p> <pre><code>- columns: list[str]\n    The columns to search,\n    currently only supports a single column in the columns list.\n- query: str\n    The query string to search for.\n</code></pre> <p>include_deleted_rows: bool, optional, default False     If True, then rows that have been deleted, but are still present in the     fragment, will be returned.  These rows will have the _rowid column set     to null.  All other columns will reflect the value stored on disk and may     not be null.</p> <pre><code>Note: if this is a search operation, or a take operation (including scalar\nindexed scans) then deleted rows cannot be returned.\n</code></pre>"},{"location":"api/py_modules/#lance.LanceDataset.to_table--notes","title":"Notes","text":"<p>If BOTH filter and nearest is specified, then:</p> <ol> <li>nearest is executed first.</li> <li>The results are filtered afterward, unless pre-filter sets to True.</li> </ol>"},{"location":"api/py_modules/#lance.LanceDataset.update","title":"<code>update(updates, where=None)</code>","text":"<p>Update column values for rows matching where.</p>"},{"location":"api/py_modules/#lance.LanceDataset.update--parameters","title":"Parameters","text":"<p>updates : dict of str to str     A mapping of column names to a SQL expression. where : str, optional     A SQL predicate indicating which rows should be updated.</p>"},{"location":"api/py_modules/#lance.LanceDataset.update--returns","title":"Returns","text":"<p>updates : dict     A dictionary containing the number of rows updated.</p>"},{"location":"api/py_modules/#lance.LanceDataset.update--examples","title":"Examples","text":"<p>import lance import pyarrow as pa table = pa.table({\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"]}) dataset = lance.write_dataset(table, \"example\") update_stats = dataset.update(dict(a = 'a + 2'), where=\"b != 'a'\") update_stats[\"num_updated_rows\"] = 2 dataset.to_table().to_pandas()    a  b 0  1  a 1  4  b 2  5  c</p>"},{"location":"api/py_modules/#lance.LanceDataset.validate","title":"<code>validate()</code>","text":"<p>Validate the dataset.</p> <p>This checks the integrity of the dataset and will raise an exception if the dataset is corrupted.</p>"},{"location":"api/py_modules/#lance.LanceDataset.versions","title":"<code>versions()</code>","text":"<p>Return all versions in this dataset.</p>"},{"location":"api/py_modules/#lance.LanceFragment","title":"<code>LanceFragment</code>","text":"<p>               Bases: <code>Fragment</code></p>"},{"location":"api/py_modules/#lance.LanceFragment.metadata","title":"<code>metadata</code>  <code>property</code>","text":"<p>Return the metadata of this fragment.</p>"},{"location":"api/py_modules/#lance.LanceFragment.metadata--returns","title":"Returns","text":"<p>FragmentMetadata</p>"},{"location":"api/py_modules/#lance.LanceFragment.num_deletions","title":"<code>num_deletions</code>  <code>property</code>","text":"<p>Return the number of deleted rows in this fragment.</p>"},{"location":"api/py_modules/#lance.LanceFragment.physical_rows","title":"<code>physical_rows</code>  <code>property</code>","text":"<p>Return the number of rows originally in this fragment.</p> <p>To get the number of rows after deletions, use :meth:<code>count_rows</code> instead.</p>"},{"location":"api/py_modules/#lance.LanceFragment.schema","title":"<code>schema</code>  <code>property</code>","text":"<p>Return the schema of this fragment.</p>"},{"location":"api/py_modules/#lance.LanceFragment.create","title":"<code>create(dataset_uri, data, fragment_id=None, schema=None, max_rows_per_group=1024, progress=None, mode='append', *, data_storage_version=None, use_legacy_format=None, storage_options=None)</code>  <code>staticmethod</code>","text":"<p>Create a :class:<code>FragmentMetadata</code> from the given data.</p> <p>This can be used if the dataset is not yet created.</p> <p>.. warning::</p> <pre><code>Internal API. This method is not intended to be used by end users.\n</code></pre>"},{"location":"api/py_modules/#lance.LanceFragment.create--parameters","title":"Parameters","text":"<p>dataset_uri: str     The URI of the dataset. fragment_id: int     The ID of the fragment. data: pa.Table or pa.RecordBatchReader     The data to be written to the fragment. schema: pa.Schema, optional     The schema of the data. If not specified, the schema will be inferred     from the data. max_rows_per_group: int, default 1024     The maximum number of rows per group in the data file. progress: FragmentWriteProgress, optional     Experimental API. Progress tracking for writing the fragment. Pass     a custom class that defines hooks to be called when each fragment is     starting to write and finishing writing. mode: str, default \"append\"     The write mode. If \"append\" is specified, the data will be checked     against the existing dataset's schema. Otherwise, pass \"create\" or     \"overwrite\" to assign new field ids to the schema. data_storage_version: optional, str, default None     The version of the data storage format to use. Newer versions are more     efficient but require newer versions of lance to read.  The default (None)     will use the latest stable version.  See the user guide for more details. use_legacy_format: bool, default None     Deprecated parameter.  Use data_storage_version instead. storage_options : optional, dict     Extra options that make sense for a particular storage connection. This is     used to store connection parameters like credentials, endpoint, etc.</p>"},{"location":"api/py_modules/#lance.LanceFragment.create--see-also","title":"See Also","text":"<p>lance.dataset.LanceOperation.Overwrite :     The operation used to create a new dataset or overwrite one using     fragments created with this API. See the doc page for an example of     using this API. lance.dataset.LanceOperation.Append :     The operation used to append fragments created with this API to an     existing dataset. See the doc page for an example of using this API.</p>"},{"location":"api/py_modules/#lance.LanceFragment.create--returns","title":"Returns","text":"<p>FragmentMetadata</p>"},{"location":"api/py_modules/#lance.LanceFragment.create_from_file","title":"<code>create_from_file(filename, dataset, fragment_id)</code>  <code>staticmethod</code>","text":"<p>Create a fragment from the given datafile uri.</p> <p>This can be used if the datafile is loss from dataset.</p> <p>.. warning::</p> <pre><code>Internal API. This method is not intended to be used by end users.\n</code></pre>"},{"location":"api/py_modules/#lance.LanceFragment.create_from_file--parameters","title":"Parameters","text":"<p>filename: str     The filename of the datafile. dataset: LanceDataset     The dataset that the fragment belongs to. fragment_id: int     The ID of the fragment.</p>"},{"location":"api/py_modules/#lance.LanceFragment.data_files","title":"<code>data_files()</code>","text":"<p>Return the data files of this fragment.</p>"},{"location":"api/py_modules/#lance.LanceFragment.delete","title":"<code>delete(predicate)</code>","text":"<p>Delete rows from this Fragment.</p> <p>This will add or update the deletion file of this fragment. It does not modify or delete the data files of this fragment. If no rows are left after the deletion, this method will return None.</p> <p>.. warning::</p> <pre><code>Internal API. This method is not intended to be used by end users.\n</code></pre>"},{"location":"api/py_modules/#lance.LanceFragment.delete--parameters","title":"Parameters","text":"<p>predicate: str     A SQL predicate that specifies the rows to delete.</p>"},{"location":"api/py_modules/#lance.LanceFragment.delete--returns","title":"Returns","text":"<p>FragmentMetadata or None     A new fragment containing the new deletion file, or None if no rows left.</p>"},{"location":"api/py_modules/#lance.LanceFragment.delete--examples","title":"Examples","text":"<p>import lance import pyarrow as pa tab = pa.table({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}) dataset = lance.write_dataset(tab, \"dataset\") frag = dataset.get_fragment(0) frag.delete(\"a &gt; 1\") FragmentMetadata(id=0, files=[DataFile(path='...', fields=[0, 1], ...), ...) frag.delete(\"a &gt; 0\") is None True</p>"},{"location":"api/py_modules/#lance.LanceFragment.delete--see-also","title":"See Also","text":"<p>lance.dataset.LanceOperation.Delete :     The operation used to commit these changes to a dataset. See the     doc page for an example of using this API.</p>"},{"location":"api/py_modules/#lance.LanceFragment.deletion_file","title":"<code>deletion_file()</code>","text":"<p>Return the deletion file, if any</p>"},{"location":"api/py_modules/#lance.LanceFragment.merge","title":"<code>merge(data_obj, left_on, right_on=None, schema=None)</code>","text":"<p>Merge another dataset into this fragment.</p> <p>Performs a left join, where the fragment is the left side and data_obj is the right side. Rows existing in the dataset but not on the left will be filled with null values, unless Lance doesn't support null values for some types, in which case an error will be raised.</p>"},{"location":"api/py_modules/#lance.LanceFragment.merge--parameters","title":"Parameters","text":"<p>data_obj: Reader-like     The data to be merged. Acceptable types are:     - Pandas DataFrame, Pyarrow Table, Dataset, Scanner,     Iterator[RecordBatch], or RecordBatchReader left_on: str     The name of the column in the dataset to join on. right_on: str or None     The name of the column in data_obj to join on. If None, defaults to     left_on.</p>"},{"location":"api/py_modules/#lance.LanceFragment.merge--examples","title":"Examples","text":"<p>import lance import pyarrow as pa df = pa.table({'x': [1, 2, 3], 'y': ['a', 'b', 'c']}) dataset = lance.write_dataset(df, \"dataset\") dataset.to_table().to_pandas()    x  y 0  1  a 1  2  b 2  3  c fragments = dataset.get_fragments() new_df = pa.table({'x': [1, 2, 3], 'z': ['d', 'e', 'f']}) merged = [] schema = None for f in fragments: ...     f, schema = f.merge(new_df, 'x') ...     merged.append(f) merge = lance.LanceOperation.Merge(merged, schema) dataset = lance.LanceDataset.commit(\"dataset\", merge, read_version=1) dataset.to_table().to_pandas()    x  y  z 0  1  a  d 1  2  b  e 2  3  c  f</p>"},{"location":"api/py_modules/#lance.LanceFragment.merge--see-also","title":"See Also","text":"<p>LanceDataset.merge_columns :     Add columns to this Fragment.</p>"},{"location":"api/py_modules/#lance.LanceFragment.merge--returns","title":"Returns","text":"<p>Tuple[FragmentMetadata, LanceSchema]     A new fragment with the merged column(s) and the final schema.</p>"},{"location":"api/py_modules/#lance.LanceFragment.merge_columns","title":"<code>merge_columns(value_func, columns=None, batch_size=None, reader_schema=None)</code>","text":"<p>Add columns to this Fragment.</p> <p>.. warning::</p> <pre><code>Internal API. This method is not intended to be used by end users.\n</code></pre> <p>The parameters and their interpretation are the same as in the :meth:<code>lance.dataset.LanceDataset.add_columns</code> operation.</p> <p>The only difference is that, instead of modifying the dataset, a new fragment is created.  The new schema of the fragment is returned as well. These can be used in a later operation to commit the changes to the dataset.</p>"},{"location":"api/py_modules/#lance.LanceFragment.merge_columns--see-also","title":"See Also","text":"<p>lance.dataset.LanceOperation.Merge :     The operation used to commit these changes to the dataset. See the     doc page for an example of using this API.</p>"},{"location":"api/py_modules/#lance.LanceFragment.merge_columns--returns","title":"Returns","text":"<p>Tuple[FragmentMetadata, LanceSchema]     A new fragment with the added column(s) and the final schema.</p>"},{"location":"api/py_modules/#lance.LanceFragment.scanner","title":"<code>scanner(*, columns=None, batch_size=None, filter=None, limit=None, offset=None, with_row_id=False, with_row_address=False, batch_readahead=16)</code>","text":"<p>See Dataset::scanner for details</p>"},{"location":"api/py_modules/#lance.LanceOperation","title":"<code>LanceOperation</code>","text":""},{"location":"api/py_modules/#lance.LanceOperation.Append","title":"<code>Append</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>Append new rows to the dataset.</p>"},{"location":"api/py_modules/#lance.LanceOperation.Append--attributes","title":"Attributes","text":"<p>fragments: list[FragmentMetadata]     The fragments that contain the new rows.</p>"},{"location":"api/py_modules/#lance.LanceOperation.Append--warning","title":"Warning","text":"<p>This is an advanced API for distributed operations. To append to a dataset on a single machine, use :func:<code>lance.write_dataset</code>.</p>"},{"location":"api/py_modules/#lance.LanceOperation.Append--examples","title":"Examples","text":"<p>To append new rows to a dataset, first use :meth:<code>lance.fragment.LanceFragment.create</code> to create fragments. Then collect the fragment metadata into a list and pass it to this class. Finally, pass the operation to the :meth:<code>LanceDataset.commit</code> method to create the new dataset.</p> <p>import lance import pyarrow as pa tab1 = pa.table({\"a\": [1, 2], \"b\": [\"a\", \"b\"]}) dataset = lance.write_dataset(tab1, \"example\") tab2 = pa.table({\"a\": [3, 4], \"b\": [\"c\", \"d\"]}) fragment = lance.fragment.LanceFragment.create(\"example\", tab2) operation = lance.LanceOperation.Append([fragment]) dataset = lance.LanceDataset.commit(\"example\", operation, ...                                     read_version=dataset.version) dataset.to_table().to_pandas()    a  b 0  1  a 1  2  b 2  3  c 3  4  d</p>"},{"location":"api/py_modules/#lance.LanceOperation.BaseOperation","title":"<code>BaseOperation</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for operations that can be applied to a dataset.</p> <p>See available operations under :class:<code>LanceOperation</code>.</p>"},{"location":"api/py_modules/#lance.LanceOperation.CreateIndex","title":"<code>CreateIndex</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>Operation that creates an index on the dataset.</p>"},{"location":"api/py_modules/#lance.LanceOperation.DataReplacement","title":"<code>DataReplacement</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>Operation that replaces existing datafiles in the dataset.</p>"},{"location":"api/py_modules/#lance.LanceOperation.DataReplacementGroup","title":"<code>DataReplacementGroup</code>  <code>dataclass</code>","text":"<p>Group of data replacements</p>"},{"location":"api/py_modules/#lance.LanceOperation.Delete","title":"<code>Delete</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>Remove fragments or rows from the dataset.</p>"},{"location":"api/py_modules/#lance.LanceOperation.Delete--attributes","title":"Attributes","text":"<p>updated_fragments: list[FragmentMetadata]     The fragments that have been updated with new deletion vectors. deleted_fragment_ids: list[int]     The ids of the fragments that have been deleted entirely. These are     the fragments where :meth:<code>LanceFragment.delete()</code> returned None. predicate: str     The original SQL predicate used to select the rows to delete.</p>"},{"location":"api/py_modules/#lance.LanceOperation.Delete--warning","title":"Warning","text":"<p>This is an advanced API for distributed operations. To delete rows from dataset on a single machine, use :meth:<code>lance.LanceDataset.delete</code>.</p>"},{"location":"api/py_modules/#lance.LanceOperation.Delete--examples","title":"Examples","text":"<p>To delete rows from a dataset, call :meth:<code>lance.fragment.LanceFragment.delete</code> on each of the fragments. If that returns a new fragment, add that to the <code>updated_fragments</code> list. If it returns None, that means the whole fragment was deleted, so add the fragment id to the <code>deleted_fragment_ids</code>. Finally, pass the operation to the :meth:<code>LanceDataset.commit</code> method to complete the deletion operation.</p> <p>import lance import pyarrow as pa table = pa.table({\"a\": [1, 2], \"b\": [\"a\", \"b\"]}) dataset = lance.write_dataset(table, \"example\") table = pa.table({\"a\": [3, 4], \"b\": [\"c\", \"d\"]}) dataset = lance.write_dataset(table, \"example\", mode=\"append\") dataset.to_table().to_pandas()    a  b 0  1  a 1  2  b 2  3  c 3  4  d predicate = \"a &gt;= 2\" updated_fragments = [] deleted_fragment_ids = [] for fragment in dataset.get_fragments(): ...     new_fragment = fragment.delete(predicate) ...     if new_fragment is not None: ...         updated_fragments.append(new_fragment) ...     else: ...         deleted_fragment_ids.append(fragment.fragment_id) operation = lance.LanceOperation.Delete(updated_fragments, ...                                         deleted_fragment_ids, ...                                         predicate) dataset = lance.LanceDataset.commit(\"example\", operation, ...                                     read_version=dataset.version) dataset.to_table().to_pandas()    a  b 0  1  a</p>"},{"location":"api/py_modules/#lance.LanceOperation.Merge","title":"<code>Merge</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>Operation that adds columns. Unlike Overwrite, this should not change the structure of the fragments, allowing existing indices to be kept.</p>"},{"location":"api/py_modules/#lance.LanceOperation.Merge--attributes","title":"Attributes","text":"<p>fragments: iterable of FragmentMetadata     The fragments that make up the new dataset. schema: LanceSchema or pyarrow.Schema     The schema of the new dataset. Passing a LanceSchema is preferred,     and passing a pyarrow.Schema is deprecated.</p>"},{"location":"api/py_modules/#lance.LanceOperation.Merge--warning","title":"Warning","text":"<p>This is an advanced API for distributed operations. To overwrite or create new dataset on a single machine, use :func:<code>lance.write_dataset</code>.</p>"},{"location":"api/py_modules/#lance.LanceOperation.Merge--examples","title":"Examples","text":"<p>To add new columns to a dataset, first define a method that will create the new columns based on the existing columns. Then use :meth:<code>lance.fragment.LanceFragment.add_columns</code></p> <p>import lance import pyarrow as pa import pyarrow.compute as pc table = pa.table({\"a\": [1, 2, 3, 4], \"b\": [\"a\", \"b\", \"c\", \"d\"]}) dataset = lance.write_dataset(table, \"example\") dataset.to_table().to_pandas()    a  b 0  1  a 1  2  b 2  3  c 3  4  d def double_a(batch: pa.RecordBatch) -&gt; pa.RecordBatch: ...     doubled = pc.multiply(batch[\"a\"], 2) ...     return pa.record_batch([doubled], [\"a_doubled\"]) fragments = [] for fragment in dataset.get_fragments(): ...     new_fragment, new_schema = fragment.merge_columns(double_a, ...                                                       columns=['a']) ...     fragments.append(new_fragment) operation = lance.LanceOperation.Merge(fragments, new_schema) dataset = lance.LanceDataset.commit(\"example\", operation, ...                                     read_version=dataset.version) dataset.to_table().to_pandas()    a  b  a_doubled 0  1  a          2 1  2  b          4 2  3  c          6 3  4  d          8</p>"},{"location":"api/py_modules/#lance.LanceOperation.Overwrite","title":"<code>Overwrite</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>Overwrite or create a new dataset.</p>"},{"location":"api/py_modules/#lance.LanceOperation.Overwrite--attributes","title":"Attributes","text":"<p>new_schema: pyarrow.Schema     The schema of the new dataset. fragments: list[FragmentMetadata]     The fragments that make up the new dataset.</p>"},{"location":"api/py_modules/#lance.LanceOperation.Overwrite--warning","title":"Warning","text":"<p>This is an advanced API for distributed operations. To overwrite or create new dataset on a single machine, use :func:<code>lance.write_dataset</code>.</p>"},{"location":"api/py_modules/#lance.LanceOperation.Overwrite--examples","title":"Examples","text":"<p>To create or overwrite a dataset, first use :meth:<code>lance.fragment.LanceFragment.create</code> to create fragments. Then collect the fragment metadata into a list and pass it along with the schema to this class. Finally, pass the operation to the :meth:<code>LanceDataset.commit</code> method to create the new dataset.</p> <p>import lance import pyarrow as pa tab1 = pa.table({\"a\": [1, 2], \"b\": [\"a\", \"b\"]}) tab2 = pa.table({\"a\": [3, 4], \"b\": [\"c\", \"d\"]}) fragment1 = lance.fragment.LanceFragment.create(\"example\", tab1) fragment2 = lance.fragment.LanceFragment.create(\"example\", tab2) fragments = [fragment1, fragment2] operation = lance.LanceOperation.Overwrite(tab1.schema, fragments) dataset = lance.LanceDataset.commit(\"example\", operation) dataset.to_table().to_pandas()    a  b 0  1  a 1  2  b 2  3  c 3  4  d</p>"},{"location":"api/py_modules/#lance.LanceOperation.Project","title":"<code>Project</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>Operation that project columns. Use this operator for drop column or rename/swap column.</p>"},{"location":"api/py_modules/#lance.LanceOperation.Project--attributes","title":"Attributes","text":"<p>schema: LanceSchema     The lance schema of the new dataset.</p>"},{"location":"api/py_modules/#lance.LanceOperation.Project--examples","title":"Examples","text":"<p>Use the projece operator to swap column:</p> <p>import lance import pyarrow as pa import pyarrow.compute as pc from lance.schema import LanceSchema table = pa.table({\"a\": [1, 2], \"b\": [\"a\", \"b\"], \"b1\": [\"c\", \"d\"]}) dataset = lance.write_dataset(table, \"example\") dataset.to_table().to_pandas()    a  b b1 0  1  a  c 1  2  b  d</p>"},{"location":"api/py_modules/#lance.LanceOperation.Project--rename-column-b-into-b0-and-rename-b1-into-b","title":"rename column <code>b</code> into <code>b0</code> and rename b1 into <code>b</code>","text":"<p>table = pa.table({\"a\": [3, 4], \"b0\": [\"a\", \"b\"], \"b\": [\"c\", \"d\"]}) lance_schema = LanceSchema.from_pyarrow(table.schema) operation = lance.LanceOperation.Project(lance_schema) dataset = lance.LanceDataset.commit(\"example\", operation, read_version=1) dataset.to_table().to_pandas()    a b0  b 0  1  a  c 1  2  b  d</p>"},{"location":"api/py_modules/#lance.LanceOperation.Restore","title":"<code>Restore</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>Operation that restores a previous version of the dataset.</p>"},{"location":"api/py_modules/#lance.LanceOperation.Rewrite","title":"<code>Rewrite</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>Operation that rewrites one or more files and indices into one or more files and indices.</p>"},{"location":"api/py_modules/#lance.LanceOperation.Rewrite--attributes","title":"Attributes","text":"<p>groups: list[RewriteGroup]     Groups of files that have been rewritten. rewritten_indices: list[RewrittenIndex]     Indices that have been rewritten.</p>"},{"location":"api/py_modules/#lance.LanceOperation.Rewrite--warning","title":"Warning","text":"<p>This is an advanced API not intended for general use.</p>"},{"location":"api/py_modules/#lance.LanceOperation.RewriteGroup","title":"<code>RewriteGroup</code>  <code>dataclass</code>","text":"<p>Collection of rewritten files</p>"},{"location":"api/py_modules/#lance.LanceOperation.RewrittenIndex","title":"<code>RewrittenIndex</code>  <code>dataclass</code>","text":"<p>An index that has been rewritten</p>"},{"location":"api/py_modules/#lance.LanceOperation.Update","title":"<code>Update</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>Operation that updates rows in the dataset.</p>"},{"location":"api/py_modules/#lance.LanceOperation.Update--attributes","title":"Attributes","text":"<p>removed_fragment_ids: list[int]     The ids of the fragments that have been removed entirely. updated_fragments: list[FragmentMetadata]     The fragments that have been updated with new deletion vectors. new_fragments: list[FragmentMetadata]     The fragments that contain the new rows.</p>"},{"location":"api/py_modules/#lance.LanceScanner","title":"<code>LanceScanner</code>","text":"<p>               Bases: <code>Scanner</code></p>"},{"location":"api/py_modules/#lance.LanceScanner.dataset_schema","title":"<code>dataset_schema</code>  <code>property</code>","text":"<p>The schema with which batches will be read from fragments.</p>"},{"location":"api/py_modules/#lance.LanceScanner.analyze_plan","title":"<code>analyze_plan()</code>","text":"<p>Execute the plan for this scanner and display with runtime metrics.</p>"},{"location":"api/py_modules/#lance.LanceScanner.analyze_plan--parameters","title":"Parameters","text":"<p>verbose : bool, default False     Use a verbose output format.</p>"},{"location":"api/py_modules/#lance.LanceScanner.analyze_plan--returns","title":"Returns","text":"<p>plan : str</p>"},{"location":"api/py_modules/#lance.LanceScanner.count_rows","title":"<code>count_rows()</code>","text":"<p>Count rows matching the scanner filter.</p>"},{"location":"api/py_modules/#lance.LanceScanner.count_rows--returns","title":"Returns","text":"<p>count : int</p>"},{"location":"api/py_modules/#lance.LanceScanner.explain_plan","title":"<code>explain_plan(verbose=False)</code>","text":"<p>Return the execution plan for this scanner.</p>"},{"location":"api/py_modules/#lance.LanceScanner.explain_plan--parameters","title":"Parameters","text":"<p>verbose : bool, default False     Use a verbose output format.</p>"},{"location":"api/py_modules/#lance.LanceScanner.explain_plan--returns","title":"Returns","text":"<p>plan : str</p>"},{"location":"api/py_modules/#lance.LanceScanner.from_batches","title":"<code>from_batches(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Not implemented</p>"},{"location":"api/py_modules/#lance.LanceScanner.from_dataset","title":"<code>from_dataset(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Not implemented</p>"},{"location":"api/py_modules/#lance.LanceScanner.from_fragment","title":"<code>from_fragment(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Not implemented</p>"},{"location":"api/py_modules/#lance.LanceScanner.head","title":"<code>head(num_rows)</code>","text":"<p>Load the first N rows of the dataset.</p>"},{"location":"api/py_modules/#lance.LanceScanner.head--parameters","title":"Parameters","text":"<p>num_rows : int     The number of rows to load.</p>"},{"location":"api/py_modules/#lance.LanceScanner.head--returns","title":"Returns","text":"<p>Table</p>"},{"location":"api/py_modules/#lance.LanceScanner.scan_batches","title":"<code>scan_batches()</code>","text":"<p>Consume a Scanner in record batches with corresponding fragments.</p>"},{"location":"api/py_modules/#lance.LanceScanner.scan_batches--returns","title":"Returns","text":"<p>record_batches : iterator of TaggedRecordBatch</p>"},{"location":"api/py_modules/#lance.LanceScanner.take","title":"<code>take(indices)</code>","text":"<p>Not implemented</p>"},{"location":"api/py_modules/#lance.LanceScanner.to_table","title":"<code>to_table()</code>","text":"<p>Read the data into memory and return a pyarrow Table.</p>"},{"location":"api/py_modules/#lance.MergeInsertBuilder","title":"<code>MergeInsertBuilder</code>","text":"<p>               Bases: <code>_MergeInsertBuilder</code></p>"},{"location":"api/py_modules/#lance.MergeInsertBuilder.conflict_retries","title":"<code>conflict_retries(max_retries)</code>","text":"<p>Set number of times to retry the operation if there is contention.</p> <p>If this is set &gt; 0, then the operation will keep a copy of the input data either in memory or on disk (depending on the size of the data) and will retry the operation if there is contention.</p> <p>Default is 10.</p>"},{"location":"api/py_modules/#lance.MergeInsertBuilder.execute","title":"<code>execute(data_obj, *, schema=None)</code>","text":"<p>Executes the merge insert operation</p> <p>This function updates the original dataset and returns a dictionary with information about merge statistics - i.e. the number of inserted, updated, and deleted rows.</p>"},{"location":"api/py_modules/#lance.MergeInsertBuilder.execute--parameters","title":"Parameters","text":"ReaderLike <p>The new data to use as the source table for the operation.  This parameter can be any source of data (e.g. table / dataset) that :func:<code>~lance.write_dataset</code> accepts.</p> <p>schema: Optional[pa.Schema]     The schema of the data.  This only needs to be supplied whenever the data     source is some kind of generator.</p>"},{"location":"api/py_modules/#lance.MergeInsertBuilder.execute_uncommitted","title":"<code>execute_uncommitted(data_obj, *, schema=None)</code>","text":"<p>Executes the merge insert operation without committing</p> <p>This function updates the original dataset and returns a dictionary with information about merge statistics - i.e. the number of inserted, updated, and deleted rows.</p>"},{"location":"api/py_modules/#lance.MergeInsertBuilder.execute_uncommitted--parameters","title":"Parameters","text":"ReaderLike <p>The new data to use as the source table for the operation.  This parameter can be any source of data (e.g. table / dataset) that :func:<code>~lance.write_dataset</code> accepts.</p> <p>schema: Optional[pa.Schema]     The schema of the data.  This only needs to be supplied whenever the data     source is some kind of generator.</p>"},{"location":"api/py_modules/#lance.MergeInsertBuilder.retry_timeout","title":"<code>retry_timeout(timeout)</code>","text":"<p>Set the timeout used to limit retries.</p> <p>This is the maximum time to spend on the operation before giving up. At least one attempt will be made, regardless of how long it takes to complete. Subsequent attempts will be cancelled once this timeout is reached. If the timeout has been reached during the first attempt, the operation will be cancelled immediately before making a second attempt.</p> <p>The default is 30 seconds.</p>"},{"location":"api/py_modules/#lance.MergeInsertBuilder.when_matched_update_all","title":"<code>when_matched_update_all(condition=None)</code>","text":"<p>Configure the operation to update matched rows</p> <p>After this method is called, when the merge insert operation executes, any rows that match both the source table and the target table will be updated.  The rows from the target table will be removed and the rows from the source table will be added.</p> <p>An optional condition may be specified.  This should be an SQL filter and, if present, then only matched rows that also satisfy this filter will be updated.  The SQL filter should use the prefix <code>target.</code> to refer to columns in the target table and the prefix <code>source.</code> to refer to columns in the source table.  For example, <code>source.last_update &lt; target.last_update</code>.</p> <p>If a condition is specified and rows do not satisfy the condition then these rows will not be updated.  Failure to satisfy the filter does not cause a \"matched\" row to become a \"not matched\" row.</p>"},{"location":"api/py_modules/#lance.MergeInsertBuilder.when_not_matched_by_source_delete","title":"<code>when_not_matched_by_source_delete(expr=None)</code>","text":"<p>Configure the operation to delete source rows that do not match</p> <p>After this method is called, when the merge insert operation executes, any rows that exist only in the target table will be deleted.  An optional filter can be specified to limit the scope of the delete operation.  If given (as an SQL filter) then only rows which match the filter will be deleted.</p>"},{"location":"api/py_modules/#lance.MergeInsertBuilder.when_not_matched_insert_all","title":"<code>when_not_matched_insert_all()</code>","text":"<p>Configure the operation to insert not matched rows</p> <p>After this method is called, when the merge insert operation executes, any rows that exist only in the source table will be inserted into the target table.</p>"},{"location":"api/py_modules/#lance.batch_udf","title":"<code>batch_udf(output_schema=None, checkpoint_file=None)</code>","text":"<p>Create a user defined function (UDF) that adds columns to a dataset.</p> <p>This function is used to add columns to a dataset. It takes a function that takes a single argument, a RecordBatch, and returns a RecordBatch. The function is called once for each batch in the dataset. The function should not modify the input batch, but instead create a new batch with the new columns added.</p>"},{"location":"api/py_modules/#lance.batch_udf--parameters","title":"Parameters","text":"<p>output_schema : Schema, optional     The schema of the output RecordBatch. This is used to validate the     output of the function. If not provided, the schema of the first output     RecordBatch will be used. checkpoint_file : str or Path, optional     If specified, this file will be used as a cache for unsaved results of     this UDF. If the process fails, and you call add_columns again with this     same file, it will resume from the last saved state. This is useful for     long running processes that may fail and need to be resumed. This file     may get very large. It will hold up to an entire data files' worth of     results on disk, which can be multiple gigabytes of data.</p>"},{"location":"api/py_modules/#lance.batch_udf--returns","title":"Returns","text":"<p>AddColumnsUDF</p>"},{"location":"api/py_modules/#lance.json_to_schema","title":"<code>json_to_schema(schema_json)</code>","text":"<p>Converts a JSON string to a PyArrow schema.</p>"},{"location":"api/py_modules/#lance.json_to_schema--parameters","title":"Parameters","text":"<p>schema_json: Dict[str, Any]     The JSON payload to convert to a PyArrow Schema.</p>"},{"location":"api/py_modules/#lance.schema_to_json","title":"<code>schema_to_json(schema)</code>","text":"<p>Converts a pyarrow schema to a JSON string.</p>"},{"location":"api/py_modules/#lance.schema_to_json--parameters","title":"Parameters","text":""},{"location":"api/py_modules/#lance.write_dataset","title":"<code>write_dataset(data_obj, uri, schema=None, mode='create', *, max_rows_per_file=1024 * 1024, max_rows_per_group=1024, max_bytes_per_file=90 * 1024 * 1024 * 1024, commit_lock=None, progress=None, storage_options=None, data_storage_version=None, use_legacy_format=None, enable_v2_manifest_paths=False, enable_move_stable_row_ids=False)</code>","text":"<p>Write a given data_obj to the given uri</p>"},{"location":"api/py_modules/#lance.write_dataset--parameters","title":"Parameters","text":"<p>data_obj: Reader-like     The data to be written. Acceptable types are:     - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader     - Huggingface dataset uri: str, Path, or LanceDataset     Where to write the dataset to (directory). If a LanceDataset is passed,     the session will be reused. schema: Schema, optional     If specified and the input is a pandas DataFrame, use this schema     instead of the default pandas to arrow table conversion. mode: str     create - create a new dataset (raises if uri already exists).     overwrite - create a new snapshot version     append - create a new version that is the concat of the input the     latest version (raises if uri does not exist) max_rows_per_file: int, default 1024 * 1024     The max number of rows to write before starting a new file max_rows_per_group: int, default 1024     The max number of rows before starting a new group (in the same file) max_bytes_per_file: int, default 90 * 1024 * 1024 * 1024     The max number of bytes to write before starting a new file. This is a     soft limit. This limit is checked after each group is written, which     means larger groups may cause this to be overshot meaningfully. This     defaults to 90 GB, since we have a hard limit of 100 GB per file on     object stores. commit_lock : CommitLock, optional     A custom commit lock.  Only needed if your object store does not support     atomic commits.  See the user guide for more details. progress: FragmentWriteProgress, optional     Experimental API. Progress tracking for writing the fragment. Pass     a custom class that defines hooks to be called when each fragment is     starting to write and finishing writing. storage_options : optional, dict     Extra options that make sense for a particular storage connection. This is     used to store connection parameters like credentials, endpoint, etc. data_storage_version: optional, str, default None     The version of the data storage format to use. Newer versions are more     efficient but require newer versions of lance to read.  The default (None)     will use the latest stable version.  See the user guide for more details. use_legacy_format : optional, bool, default None     Deprecated method for setting the data storage version. Use the     <code>data_storage_version</code> parameter instead. enable_v2_manifest_paths : bool, optional     If True, and this is a new dataset, uses the new V2 manifest paths.     These paths provide more efficient opening of datasets with many     versions on object stores. This parameter has no effect if the dataset     already exists. To migrate an existing dataset, instead use the     :meth:<code>LanceDataset.migrate_manifest_paths_v2</code> method. Default is False. enable_move_stable_row_ids : bool, optional     Experimental parameter: if set to true, the writer will use move-stable row ids.     These row ids are stable after compaction operations, but not after updates.     This makes compaction more efficient, since with stable row ids no     secondary indices need to be updated to point to new row ids.</p>"},{"location":"api/py_modules/#lancedataset","title":"lance.dataset","text":""},{"location":"api/py_modules/#lance.dataset","title":"<code>lance.dataset</code>","text":""},{"location":"api/py_modules/#lance.dataset.DataStatistics","title":"<code>DataStatistics</code>  <code>dataclass</code>","text":"<p>Statistics about the data in the dataset</p>"},{"location":"api/py_modules/#lance.dataset.DatasetOptimizer","title":"<code>DatasetOptimizer</code>","text":""},{"location":"api/py_modules/#lance.dataset.DatasetOptimizer.compact_files","title":"<code>compact_files(*, target_rows_per_fragment=1024 * 1024, max_rows_per_group=1024, max_bytes_per_file=None, materialize_deletions=True, materialize_deletions_threshold=0.1, num_threads=None, batch_size=None)</code>","text":"<p>Compacts small files in the dataset, reducing total number of files.</p> This does a few things <ul> <li>Removes deleted rows from fragments</li> <li>Removes dropped columns from fragments</li> <li>Merges small fragments into larger ones</li> </ul> <p>This method preserves the insertion order of the dataset. This may mean it leaves small fragments in the dataset if they are not adjacent to other fragments that need compaction. For example, if you have fragments with row counts 5 million, 100, and 5 million, the middle fragment will not be compacted because the fragments it is adjacent to do not need compaction.</p>"},{"location":"api/py_modules/#lance.dataset.DatasetOptimizer.compact_files--parameters","title":"Parameters","text":"<p>target_rows_per_fragment: int, default 1024*1024     The target number of rows per fragment. This is the number of rows     that will be in each fragment after compaction. max_rows_per_group: int, default 1024     Max number of rows per group. This does not affect which fragments     need compaction, but does affect how they are re-written if selected.</p> <pre><code>This setting only affects datasets using the legacy storage format.\nThe newer format does not require row groups.\n</code></pre> <p>max_bytes_per_file: Optional[int], default None     Max number of bytes in a single file.  This does not affect which     fragments need compaction, but does affect how they are re-written if     selected.  If this value is too small you may end up with fragments     that are smaller than <code>target_rows_per_fragment</code>.</p> <pre><code>The default will use the default from ``write_dataset``.\n</code></pre> <p>materialize_deletions: bool, default True     Whether to compact fragments with soft deleted rows so they are no     longer present in the file. materialize_deletions_threshold: float, default 0.1     The fraction of original rows that are soft deleted in a fragment     before the fragment is a candidate for compaction. num_threads: int, optional     The number of threads to use when performing compaction. If not     specified, defaults to the number of cores on the machine. batch_size: int, optional     The batch size to use when scanning input fragments.  You may want     to reduce this if you are running out of memory during compaction.</p> <pre><code>The default will use the same default from ``scanner``.\n</code></pre>"},{"location":"api/py_modules/#lance.dataset.DatasetOptimizer.compact_files--returns","title":"Returns","text":"<p>CompactionMetrics     Metrics about the compaction process</p>"},{"location":"api/py_modules/#lance.dataset.DatasetOptimizer.compact_files--see-also","title":"See Also","text":"<p>lance.optimize.Compaction</p>"},{"location":"api/py_modules/#lance.dataset.DatasetOptimizer.optimize_indices","title":"<code>optimize_indices(**kwargs)</code>","text":"<p>Optimizes index performance.</p> <p>As new data arrives it is not added to existing indexes automatically. When searching we need to perform an indexed search of the old data plus an expensive unindexed search on the new data.  As the amount of new unindexed data grows this can have an impact on search latency. This function will add the new data to existing indexes, restoring the performance.  This function does not retrain the index, it only assigns the new data to existing partitions.  This means an update is much quicker than retraining the entire index but may have less accuracy (especially if the new data exhibits new patterns, concepts, or trends)</p>"},{"location":"api/py_modules/#lance.dataset.DatasetOptimizer.optimize_indices--parameters","title":"Parameters","text":"<p>num_indices_to_merge: int, default 1     The number of indices to merge.     If set to 0, new delta index will be created. index_names: List[str], default None     The names of the indices to optimize.     If None, all indices will be optimized. retrain: bool, default False     Whether to retrain the whole index.     If true, the index will be retrained based on the current data,     <code>num_indices_to_merge</code> will be ignored,     and all indices will be merged into one.</p> <pre><code>This is useful when the data distribution has changed significantly,\nand we want to retrain the index to improve the search quality.\nThis would be faster than re-create the index from scratch.\n</code></pre>"},{"location":"api/py_modules/#lance.dataset.FieldStatistics","title":"<code>FieldStatistics</code>  <code>dataclass</code>","text":"<p>Statistics about a field in the dataset</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset","title":"<code>LanceDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A Lance Dataset in Lance format where the data is stored at the given uri.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.data_storage_version","title":"<code>data_storage_version</code>  <code>property</code>","text":"<p>The version of the data storage format this dataset is using</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.lance_schema","title":"<code>lance_schema</code>  <code>property</code>","text":"<p>The LanceSchema for this dataset</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.latest_version","title":"<code>latest_version</code>  <code>property</code>","text":"<p>Returns the latest version of the dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.max_field_id","title":"<code>max_field_id</code>  <code>property</code>","text":"<p>The max_field_id in manifest</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.partition_expression","title":"<code>partition_expression</code>  <code>property</code>","text":"<p>Not implemented (just override pyarrow dataset to prevent segfault)</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.schema","title":"<code>schema</code>  <code>property</code>","text":"<p>The pyarrow Schema for this dataset</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.stats","title":"<code>stats</code>  <code>property</code>","text":"<p>Experimental API</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.tags","title":"<code>tags</code>  <code>property</code>","text":"<p>Tag management for the dataset.</p> <p>Similar to Git, tags are a way to add metadata to a specific version of the dataset.</p> <p>.. warning::</p> <pre><code>Tagged versions are exempted from the :py:meth:`cleanup_old_versions()`\nprocess.\n\nTo remove a version that has been tagged, you must first\n:py:meth:`~Tags.delete` the associated tag.\n</code></pre>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.tags--examples","title":"Examples","text":"<p>.. code-block:: python</p> <pre><code>ds = lance.open(\"dataset.lance\")\nds.tags.create(\"v2-prod-20250203\", 10)\n\ntags = ds.tags.list()\n</code></pre>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.uri","title":"<code>uri</code>  <code>property</code>","text":"<p>The location of the data</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.version","title":"<code>version</code>  <code>property</code>","text":"<p>Returns the currently checked out version of the dataset</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.add_columns","title":"<code>add_columns(transforms, read_columns=None, reader_schema=None, batch_size=None)</code>","text":"<p>Add new columns with defined values.</p> <p>There are several ways to specify the new columns. First, you can provide SQL expressions for each new column. Second you can provide a UDF that takes a batch of existing data and returns a new batch with the new columns. These new columns will be appended to the dataset.</p> <p>You can also provide a RecordBatchReader which will read the new column values from some external source.  This is often useful when the new column values have already been staged to files (often by some distributed process)</p> <p>See the :func:<code>lance.add_columns_udf</code> decorator for more information on writing UDFs.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.add_columns--parameters","title":"Parameters","text":"<p>transforms : dict or AddColumnsUDF or ReaderLike     If this is a dictionary, then the keys are the names of the new     columns and the values are SQL expression strings. These strings can     reference existing columns in the dataset.     If this is a AddColumnsUDF, then it is a UDF that takes a batch of     existing data and returns a new batch with the new columns.     If this is :class:<code>pyarrow.Field</code> or :class:<code>pyarrow.Schema</code>, it adds     all NULL columns with the given schema, in a metadata-only operation. read_columns : list of str, optional     The names of the columns that the UDF will read. If None, then the     UDF will read all columns. This is only used when transforms is a     UDF. Otherwise, the read columns are inferred from the SQL expressions. reader_schema: pa.Schema, optional     Only valid if transforms is a <code>ReaderLike</code> object.  This will be used to     determine the schema of the reader. batch_size: int, optional     The number of rows to read at a time from the source dataset when applying     the transform.  This is ignored if the dataset is a v1 dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.add_columns--examples","title":"Examples","text":"<p>import lance import pyarrow as pa table = pa.table({\"a\": [1, 2, 3]}) dataset = lance.write_dataset(table, \"my_dataset\") @lance.batch_udf() ... def double_a(batch): ...     df = batch.to_pandas() ...     return pd.DataFrame({'double_a': 2 * df['a']}) dataset.add_columns(double_a) dataset.to_table().to_pandas()    a  double_a 0  1         2 1  2         4 2  3         6 dataset.add_columns({\"triple_a\": \"a * 3\"}) dataset.to_table().to_pandas()    a  double_a  triple_a 0  1         2         3 1  2         4         6 2  3         6         9</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.add_columns--see-also","title":"See Also","text":"<p>LanceDataset.merge :     Merge a pre-computed set of columns into the dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.alter_columns","title":"<code>alter_columns(*alterations)</code>","text":"<p>Alter column name, data type, and nullability.</p> <p>Columns that are renamed can keep any indices that are on them. If a column has an IVF_PQ index, it can be kept if the column is casted to another type. However, other index types don't support casting at this time.</p> <p>Column types can be upcasted (such as int32 to int64) or downcasted (such as int64 to int32). However, downcasting will fail if there are any values that cannot be represented in the new type. In general, columns can be casted to same general type: integers to integers, floats to floats, and strings to strings. However, strings, binary, and list columns can be casted between their size variants. For example, string to large string, binary to large binary, and list to large list.</p> <p>Columns that are renamed can keep any indices that are on them. However, if the column is casted to a different type, its indices will be dropped.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.alter_columns--parameters","title":"Parameters","text":"<p>alterations : Iterable[Dict[str, Any]]     A sequence of dictionaries, each with the following keys:</p> <pre><code>- \"path\": str\n    The column path to alter. For a top-level column, this is the name.\n    For a nested column, this is the dot-separated path, e.g. \"a.b.c\".\n- \"name\": str, optional\n    The new name of the column. If not specified, the column name is\n    not changed.\n- \"nullable\": bool, optional\n    Whether the column should be nullable. If not specified, the column\n    nullability is not changed. Only non-nullable columns can be changed\n    to nullable. Currently, you cannot change a nullable column to\n    non-nullable.\n- \"data_type\": pyarrow.DataType, optional\n    The new data type to cast the column to. If not specified, the column\n    data type is not changed.\n</code></pre>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.alter_columns--examples","title":"Examples","text":"<p>import lance import pyarrow as pa schema = pa.schema([pa.field('a', pa.int64()), ...                     pa.field('b', pa.string(), nullable=False)]) table = pa.table({\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"]}) dataset = lance.write_dataset(table, \"example\") dataset.alter_columns({\"path\": \"a\", \"name\": \"x\"}, ...                       {\"path\": \"b\", \"nullable\": True}) dataset.to_table().to_pandas()    x  b 0  1  a 1  2  b 2  3  c dataset.alter_columns({\"path\": \"x\", \"data_type\": pa.int32()}) dataset.schema x: int32 b: string</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.checkout_version","title":"<code>checkout_version(version)</code>","text":"<p>Load the given version of the dataset.</p> <p>Unlike the :func:<code>dataset</code> constructor, this will re-use the current cache. This is a no-op if the dataset is already at the given version.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.checkout_version--parameters","title":"Parameters","text":"<p>version: int | str,     The version to check out. A version number (<code>int</code>) or a tag     (<code>str</code>) can be provided.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.checkout_version--returns","title":"Returns","text":"<p>LanceDataset</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.cleanup_old_versions","title":"<code>cleanup_old_versions(older_than=None, *, delete_unverified=False, error_if_tagged_old_versions=True)</code>","text":"<p>Cleans up old versions of the dataset.</p> <p>Some dataset changes, such as overwriting, leave behind data that is not referenced by the latest dataset version.  The old data is left in place to allow the dataset to be restored back to an older version.</p> <p>This method will remove older versions and any data files they reference. Once this cleanup task has run you will not be able to checkout or restore these older versions.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.cleanup_old_versions--parameters","title":"Parameters","text":"timedelta, optional <p>Only versions older than this will be removed.  If not specified, this will default to two weeks.</p> bool, default False <p>Files leftover from a failed transaction may appear to be part of an in-progress operation (e.g. appending new data) and these files will not be deleted unless they are at least 7 days old.  If delete_unverified is True then these files will be deleted regardless of their age.</p> <p>This should only be set to True if you can guarantee that no other process is currently working on this dataset.  Otherwise the dataset could be put into a corrupted state.</p> bool, default True <p>Some versions may have tags associated with them. Tagged versions will not be cleaned up, regardless of how old they are. If this argument is set to <code>True</code> (the default), an exception will be raised if any tagged versions match the parameters. Otherwise, tagged versions will be ignored without any error and only untagged versions will be cleaned up.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.commit","title":"<code>commit(base_uri, operation, blobs_op=None, read_version=None, commit_lock=None, storage_options=None, enable_v2_manifest_paths=None, detached=False, max_retries=20)</code>  <code>staticmethod</code>","text":"<p>Create a new version of dataset</p> <p>This method is an advanced method which allows users to describe a change that has been made to the data files.  This method is not needed when using Lance to apply changes (e.g. when using class:<code>LanceDataset</code> or func:<code>write_dataset</code>.)</p> <p>It's current purpose is to allow for changes being made in a distributed environment where no single process is doing all of the work.  For example, a distributed bulk update or a distributed bulk modify operation.</p> <p>Once all of the changes have been made, this method can be called to make the changes visible by updating the dataset manifest.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.commit--warnings","title":"Warnings","text":"<p>This is an advanced API and doesn't provide the same level of validation as the other APIs. For example, it's the responsibility of the caller to ensure that the fragments are valid for the schema.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.commit--parameters","title":"Parameters","text":"<p>base_uri: str, Path, or LanceDataset     The base uri of the dataset, or the dataset object itself. Using     the dataset object can be more efficient because it can re-use the     file metadata cache. operation: BaseOperation     The operation to apply to the dataset.  This describes what changes     have been made. See available operations under :class:<code>LanceOperation</code>. read_version: int, optional     The version of the dataset that was used as the base for the changes.     This is not needed for overwrite or restore operations. commit_lock : CommitLock, optional     A custom commit lock.  Only needed if your object store does not support     atomic commits.  See the user guide for more details. storage_options : optional, dict     Extra options that make sense for a particular storage connection. This is     used to store connection parameters like credentials, endpoint, etc. enable_v2_manifest_paths : bool, optional     If True, and this is a new dataset, uses the new V2 manifest paths.     These paths provide more efficient opening of datasets with many     versions on object stores. This parameter has no effect if the dataset     already exists. To migrate an existing dataset, instead use the     :meth:<code>migrate_manifest_paths_v2</code> method. Default is False. WARNING:     turning this on will make the dataset unreadable for older versions     of Lance (prior to 0.17.0). detached : bool, optional     If True, then the commit will not be part of the dataset lineage.  It will     never show up as the latest dataset and the only way to check it out in the     future will be to specifically check it out by version.  The version will be     a random version that is only unique amongst detached commits.  The caller     should store this somewhere as there will be no other way to obtain it in     the future. max_retries : int     The maximum number of retries to perform when committing the dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.commit--returns","title":"Returns","text":"<p>LanceDataset     A new version of Lance Dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.commit--examples","title":"Examples","text":"<p>Creating a new dataset with the :class:<code>LanceOperation.Overwrite</code> operation:</p> <p>import lance import pyarrow as pa tab1 = pa.table({\"a\": [1, 2], \"b\": [\"a\", \"b\"]}) tab2 = pa.table({\"a\": [3, 4], \"b\": [\"c\", \"d\"]}) fragment1 = lance.fragment.LanceFragment.create(\"example\", tab1) fragment2 = lance.fragment.LanceFragment.create(\"example\", tab2) fragments = [fragment1, fragment2] operation = lance.LanceOperation.Overwrite(tab1.schema, fragments) dataset = lance.LanceDataset.commit(\"example\", operation) dataset.to_table().to_pandas()    a  b 0  1  a 1  2  b 2  3  c 3  4  d</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.commit_batch","title":"<code>commit_batch(dest, transactions, commit_lock=None, storage_options=None, enable_v2_manifest_paths=None, detached=False, max_retries=20)</code>  <code>staticmethod</code>","text":"<p>Create a new version of dataset with multiple transactions.</p> <p>This method is an advanced method which allows users to describe a change that has been made to the data files.  This method is not needed when using Lance to apply changes (e.g. when using class:<code>LanceDataset</code> or func:<code>write_dataset</code>.)</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.commit_batch--parameters","title":"Parameters","text":"<p>dest: str, Path, or LanceDataset     The base uri of the dataset, or the dataset object itself. Using     the dataset object can be more efficient because it can re-use the     file metadata cache. transactions: Iterable[Transaction]     The transactions to apply to the dataset. These will be merged into     a single transaction and applied to the dataset. Note: Only append     transactions are currently supported. Other transaction types will be     supported in the future. commit_lock : CommitLock, optional     A custom commit lock.  Only needed if your object store does not support     atomic commits.  See the user guide for more details. storage_options : optional, dict     Extra options that make sense for a particular storage connection. This is     used to store connection parameters like credentials, endpoint, etc. enable_v2_manifest_paths : bool, optional     If True, and this is a new dataset, uses the new V2 manifest paths.     These paths provide more efficient opening of datasets with many     versions on object stores. This parameter has no effect if the dataset     already exists. To migrate an existing dataset, instead use the     :meth:<code>migrate_manifest_paths_v2</code> method. Default is False. WARNING:     turning this on will make the dataset unreadable for older versions     of Lance (prior to 0.17.0). detached : bool, optional     If True, then the commit will not be part of the dataset lineage.  It will     never show up as the latest dataset and the only way to check it out in the     future will be to specifically check it out by version.  The version will be     a random version that is only unique amongst detached commits.  The caller     should store this somewhere as there will be no other way to obtain it in     the future. max_retries : int     The maximum number of retries to perform when committing the dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.commit_batch--returns","title":"Returns","text":"<p>dict with keys:     dataset: LanceDataset         A new version of Lance Dataset.     merged: Transaction         The merged transaction that was applied to the dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.count_rows","title":"<code>count_rows(filter=None, **kwargs)</code>","text":"<p>Count rows matching the scanner filter.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.count_rows--parameters","title":"Parameters","text":"<p>**kwargs : dict, optional     See py:method:<code>scanner</code> method for full parameter description.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.count_rows--returns","title":"Returns","text":"<p>count : int     The total number of rows in the dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.create_index","title":"<code>create_index(column, index_type, name=None, metric='L2', replace=False, num_partitions=None, ivf_centroids=None, pq_codebook=None, num_sub_vectors=None, accelerator=None, index_cache_size=None, shuffle_partition_batches=None, shuffle_partition_concurrency=None, ivf_centroids_file=None, precomputed_partition_dataset=None, storage_options=None, filter_nan=True, one_pass_ivfpq=False, **kwargs)</code>","text":"<p>Create index on column.</p> <p>Experimental API</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.create_index--parameters","title":"Parameters","text":"<p>column : str     The column to be indexed. index_type : str     The type of the index.     <code>\"IVF_PQ, IVF_HNSW_PQ and IVF_HNSW_SQ\"</code> are supported now. name : str, optional     The index name. If not provided, it will be generated from the     column name. metric : str     The distance metric type, i.e., \"L2\" (alias to \"euclidean\"), \"cosine\"     or \"dot\" (dot product). Default is \"L2\". replace : bool     Replace the existing index if it exists. num_partitions : int, optional     The number of partitions of IVF (Inverted File Index). ivf_centroids : optional     It can be either class:<code>np.ndarray</code>,     class:<code>pyarrow.FixedSizeListArray</code> or     class:<code>pyarrow.FixedShapeTensorArray</code>.     A <code>num_partitions x dimension</code> array of existing K-mean centroids     for IVF clustering. If not provided, a new KMeans model will be trained. pq_codebook : optional,     It can be class:<code>np.ndarray</code>, class:<code>pyarrow.FixedSizeListArray</code>,     or class:<code>pyarrow.FixedShapeTensorArray</code>.     A <code>num_sub_vectors x (2 ^ nbits * dimensions // num_sub_vectors)</code>     array of K-mean centroids for PQ codebook.</p> <pre><code>Note: ``nbits`` is always 8 for now.\nIf not provided, a new PQ model will be trained.\n</code></pre> <p>num_sub_vectors : int, optional     The number of sub-vectors for PQ (Product Quantization). accelerator : str or <code>torch.Device</code>, optional     If set, use an accelerator to speed up the training process.     Accepted accelerator: \"cuda\" (Nvidia GPU) and \"mps\" (Apple Silicon GPU).     If not set, use the CPU. index_cache_size : int, optional     The size of the index cache in number of entries. Default value is 256. shuffle_partition_batches : int, optional     The number of batches, using the row group size of the dataset, to include     in each shuffle partition. Default value is 10240.</p> <pre><code>Assuming the row group size is 1024, each shuffle partition will hold\n10240 * 1024 = 10,485,760 rows. By making this value smaller, this shuffle\nwill consume less memory but will take longer to complete, and vice versa.\n</code></pre> <p>shuffle_partition_concurrency : int, optional     The number of shuffle partitions to process concurrently. Default value is 2</p> <pre><code>By making this value smaller, this shuffle will consume less memory but will\ntake longer to complete, and vice versa.\n</code></pre> <p>storage_options : optional, dict     Extra options that make sense for a particular storage connection. This is     used to store connection parameters like credentials, endpoint, etc. filter_nan: bool     Defaults to True. False is UNSAFE, and will cause a crash if any null/nan     values are present (and otherwise will not). Disables the null filter used     for nullable columns. Obtains a small speed boost. one_pass_ivfpq: bool     Defaults to False. If enabled, index type must be \"IVF_PQ\". Reduces disk IO. kwargs :     Parameters passed to the index building process.</p> <p>The SQ (Scalar Quantization) is available for only <code>IVF_HNSW_SQ</code> index type, this quantization method is used to reduce the memory usage of the index, it maps the float vectors to integer vectors, each integer is of <code>num_bits</code>, now only 8 bits are supported.</p> <p>If <code>index_type</code> is \"IVF_*\", then the following parameters are required:     num_partitions</p> <p>If <code>index_type</code> is with \"PQ\", then the following parameters are required:     num_sub_vectors</p> <p>Optional parameters for <code>IVF_PQ</code>:</p> <pre><code>- ivf_centroids\n    Existing K-mean centroids for IVF clustering.\n- num_bits\n    The number of bits for PQ (Product Quantization). Default is 8.\n    Only 4, 8 are supported.\n</code></pre> <p>Optional parameters for <code>IVF_HNSW_*</code>:     max_level         Int, the maximum number of levels in the graph.     m         Int, the number of edges per node in the graph.     ef_construction         Int, the number of nodes to examine during the construction.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.create_index--examples","title":"Examples","text":"<p>.. code-block:: python</p> <pre><code>import lance\n\ndataset = lance.dataset(\"/tmp/sift.lance\")\ndataset.create_index(\n    \"vector\",\n    \"IVF_PQ\",\n    num_partitions=256,\n    num_sub_vectors=16\n)\n</code></pre> <p>.. code-block:: python</p> <pre><code>import lance\n\ndataset = lance.dataset(\"/tmp/sift.lance\")\ndataset.create_index(\n    \"vector\",\n    \"IVF_HNSW_SQ\",\n    num_partitions=256,\n)\n</code></pre> <p>Experimental Accelerator (GPU) support:</p> <ul> <li>accelerate: use GPU to train IVF partitions.     Only supports CUDA (Nvidia) or MPS (Apple) currently.     Requires PyTorch being installed.</li> </ul> <p>.. code-block:: python</p> <pre><code>import lance\n\ndataset = lance.dataset(\"/tmp/sift.lance\")\ndataset.create_index(\n    \"vector\",\n    \"IVF_PQ\",\n    num_partitions=256,\n    num_sub_vectors=16,\n    accelerator=\"cuda\"\n)\n</code></pre>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.create_index--references","title":"References","text":"<ul> <li><code>Faiss Index &lt;https://github.com/facebookresearch/faiss/wiki/Faiss-indexes&gt;</code>_</li> <li>IVF introduced in <code>Video Google: a text retrieval approach to object matching   in videos &lt;https://ieeexplore.ieee.org/abstract/document/1238663&gt;</code>_</li> <li><code>Product quantization for nearest neighbor search   &lt;https://hal.inria.fr/inria-00514462v2/document&gt;</code>_</li> </ul>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.create_scalar_index","title":"<code>create_scalar_index(column, index_type, name=None, *, replace=True, **kwargs)</code>","text":"<p>Create a scalar index on a column.</p> <p>Scalar indices, like vector indices, can be used to speed up scans.  A scalar index can speed up scans that contain filter expressions on the indexed column. For example, the following scan will be faster if the column <code>my_col</code> has a scalar index:</p> <p>.. code-block:: python</p> <pre><code>import lance\n\ndataset = lance.dataset(\"/tmp/images.lance\")\nmy_table = dataset.scanner(filter=\"my_col != 7\").to_table()\n</code></pre> <p>Vector search with pre-filers can also benefit from scalar indices. For example,</p> <p>.. code-block:: python</p> <pre><code>import lance\n\ndataset = lance.dataset(\"/tmp/images.lance\")\nmy_table = dataset.scanner(\n    nearest=dict(\n       column=\"vector\",\n       q=[1, 2, 3, 4],\n       k=10,\n    )\n    filter=\"my_col != 7\",\n    prefilter=True\n)\n</code></pre> <p>There are 5 types of scalar indices available today.</p> <ul> <li><code>BTREE</code>. The most common type is <code>BTREE</code>. This index is inspired   by the btree data structure although only the first few layers of the btree   are cached in memory.  It will   perform well on columns with a large number of unique values and few rows per   value.</li> <li><code>BITMAP</code>. This index stores a bitmap for each unique value in the column.   This index is useful for columns with a small number of unique values and   many rows per value.</li> <li><code>LABEL_LIST</code>. A special index that is used to index list   columns whose values have small cardinality.  For example, a column that   contains lists of tags (e.g. <code>[\"tag1\", \"tag2\", \"tag3\"]</code>) can be indexed   with a <code>LABEL_LIST</code> index.  This index can only speedup queries with   <code>array_has_any</code> or <code>array_has_all</code> filters.</li> <li><code>NGRAM</code>. A special index that is used to index string columns.  This index   creates a bitmap for each ngram in the string.  By default we use trigrams.   This index can currently speed up queries using the <code>contains</code> function   in filters.</li> <li><code>FTS/INVERTED</code>. It is used to index document columns. This index   can conduct full-text searches. For example, a column that contains any word   of query string \"hello world\". The results will be ranked by BM25.</li> </ul> <p>Note that the <code>LANCE_BYPASS_SPILLING</code> environment variable can be used to bypass spilling to disk. Setting this to true can avoid memory exhaustion issues (see https://github.com/apache/datafusion/issues/10073 for more info).</p> <p>Experimental API</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.create_scalar_index--parameters","title":"Parameters","text":"<p>column : str     The column to be indexed.  Must be a boolean, integer, float,     or string column. index_type : str     The type of the index.  One of <code>\"BTREE\"</code>, <code>\"BITMAP\"</code>,     <code>\"LABEL_LIST\"</code>, <code>\"NGRAM\"</code>, <code>\"FTS\"</code> or <code>\"INVERTED\"</code>. name : str, optional     The index name. If not provided, it will be generated from the     column name. replace : bool, default True     Replace the existing index if it exists.</p> bool, default True <p>This is for the <code>INVERTED</code> index. If True, the index will store the positions of the words in the document, so that you can conduct phrase query. This will significantly increase the index size. It won't impact the performance of non-phrase queries even if it is set to True.</p> <p>base_tokenizer: str, default \"simple\"     This is for the <code>INVERTED</code> index. The base tokenizer to use. The value     can be:     * \"simple\": splits tokens on whitespace and punctuation.     * \"whitespace\": splits tokens on whitespace.     * \"raw\": no tokenization. language: str, default \"English\"     This is for the <code>INVERTED</code> index. The language for stemming     and stop words. This is only used when <code>stem</code> or <code>remove_stop_words</code> is true max_token_length: Optional[int], default 40     This is for the <code>INVERTED</code> index. The maximum token length.     Any token longer than this will be removed. lower_case: bool, default True     This is for the <code>INVERTED</code> index. If True, the index will convert all     text to lowercase. stem: bool, default False     This is for the <code>INVERTED</code> index. If True, the index will stem the     tokens. remove_stop_words: bool, default False     This is for the <code>INVERTED</code> index. If True, the index will remove     stop words. ascii_folding: bool, default False     This is for the <code>INVERTED</code> index. If True, the index will convert     non-ascii characters to ascii characters if possible.     This would remove accents like \"\u00e9\" -&gt; \"e\".</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.create_scalar_index--examples","title":"Examples","text":"<p>.. code-block:: python</p> <pre><code>import lance\n\ndataset = lance.dataset(\"/tmp/images.lance\")\ndataset.create_index(\n    \"category\",\n    \"BTREE\",\n)\n</code></pre> <p>Scalar indices can only speed up scans for basic filters using equality, comparison, range (e.g. <code>my_col BETWEEN 0 AND 100</code>), and set membership (e.g. <code>my_col IN (0, 1, 2)</code>)</p> <p>Scalar indices can be used if the filter contains multiple indexed columns and the filter criteria are AND'd or OR'd together (e.g. <code>my_col &lt; 0 AND other_col&gt; 100</code>)</p> <p>Scalar indices may be used if the filter contains non-indexed columns but, depending on the structure of the filter, they may not be usable.  For example, if the column <code>not_indexed</code> does not have a scalar index then the filter <code>my_col = 0 OR not_indexed = 1</code> will not be able to use any scalar index on <code>my_col</code>.</p> <p>To determine if a scan is making use of a scalar index you can use <code>explain_plan</code> to look at the query plan that lance has created.  Queries that use scalar indices will either have a <code>ScalarIndexQuery</code> relation or a <code>MaterializeIndex</code> operator.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.delete","title":"<code>delete(predicate)</code>","text":"<p>Delete rows from the dataset.</p> <p>This marks rows as deleted, but does not physically remove them from the files. This keeps the existing indexes still valid.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.delete--parameters","title":"Parameters","text":"<p>predicate : str or pa.compute.Expression     The predicate to use to select rows to delete. May either be a SQL     string or a pyarrow Expression.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.delete--examples","title":"Examples","text":"<p>import lance import pyarrow as pa table = pa.table({\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"]}) dataset = lance.write_dataset(table, \"example\") dataset.delete(\"a = 1 or b in ('a', 'b')\") dataset.to_table() pyarrow.Table a: int64 b: string</p> <p>a: [[3]] b: [[\"c\"]]</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.drop_columns","title":"<code>drop_columns(columns)</code>","text":"<p>Drop one or more columns from the dataset</p> <p>This is a metadata-only operation and does not remove the data from the underlying storage. In order to remove the data, you must subsequently call <code>compact_files</code> to rewrite the data without the removed columns and then call <code>cleanup_old_versions</code> to remove the old files.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.drop_columns--parameters","title":"Parameters","text":"<p>columns : list of str     The names of the columns to drop. These can be nested column references     (e.g. \"a.b.c\") or top-level column names (e.g. \"a\").</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.drop_columns--examples","title":"Examples","text":"<p>import lance import pyarrow as pa table = pa.table({\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"]}) dataset = lance.write_dataset(table, \"example\") dataset.drop_columns([\"a\"]) dataset.to_table().to_pandas()    b 0  a 1  b 2  c</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.drop_index","title":"<code>drop_index(name)</code>","text":"<p>Drops an index from the dataset</p> <p>Note: Indices are dropped by \"index name\".  This is not the same as the field name. If you did not specify a name when you created the index then a name was generated for you.  You can use the <code>list_indices</code> method to get the names of the indices.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.get_fragment","title":"<code>get_fragment(fragment_id)</code>","text":"<p>Get the fragment with fragment id.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.get_fragments","title":"<code>get_fragments(filter=None)</code>","text":"<p>Get all fragments from the dataset.</p> <p>Note: filter is not supported yet.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.head","title":"<code>head(num_rows, **kwargs)</code>","text":"<p>Load the first N rows of the dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.head--parameters","title":"Parameters","text":"<p>num_rows : int     The number of rows to load. **kwargs : dict, optional     See scanner() method for full parameter description.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.head--returns","title":"Returns","text":"<p>table : Table</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.insert","title":"<code>insert(data, *, mode='append', **kwargs)</code>","text":"<p>Insert data into the dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.insert--parameters","title":"Parameters","text":"<p>data_obj: Reader-like     The data to be written. Acceptable types are:     - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader     - Huggingface dataset mode: str, default 'append'     The mode to use when writing the data. Options are:         create - create a new dataset (raises if uri already exists).         overwrite - create a new snapshot version         append - create a new version that is the concat of the input the         latest version (raises if uri does not exist) **kwargs : dict, optional     Additional keyword arguments to pass to :func:<code>write_dataset</code>.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.join","title":"<code>join(right_dataset, keys, right_keys=None, join_type='left outer', left_suffix=None, right_suffix=None, coalesce_keys=True, use_threads=True)</code>","text":"<p>Not implemented (just override pyarrow dataset to prevent segfault)</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.merge","title":"<code>merge(data_obj, left_on, right_on=None, schema=None)</code>","text":"<p>Merge another dataset into this one.</p> <p>Performs a left join, where the dataset is the left side and data_obj is the right side. Rows existing in the dataset but not on the left will be filled with null values, unless Lance doesn't support null values for some types, in which case an error will be raised.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.merge--parameters","title":"Parameters","text":"<p>data_obj: Reader-like     The data to be merged. Acceptable types are:     - Pandas DataFrame, Pyarrow Table, Dataset, Scanner,     Iterator[RecordBatch], or RecordBatchReader left_on: str     The name of the column in the dataset to join on. right_on: str or None     The name of the column in data_obj to join on. If None, defaults to     left_on.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.merge--examples","title":"Examples","text":"<p>import lance import pyarrow as pa df = pa.table({'x': [1, 2, 3], 'y': ['a', 'b', 'c']}) dataset = lance.write_dataset(df, \"dataset\") dataset.to_table().to_pandas()    x  y 0  1  a 1  2  b 2  3  c new_df = pa.table({'x': [1, 2, 3], 'z': ['d', 'e', 'f']}) dataset.merge(new_df, 'x') dataset.to_table().to_pandas()    x  y  z 0  1  a  d 1  2  b  e 2  3  c  f</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.merge--see-also","title":"See Also","text":"<p>LanceDataset.add_columns :     Add new columns by computing batch-by-batch.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.merge_insert","title":"<code>merge_insert(on)</code>","text":"<p>Returns a builder that can be used to create a \"merge insert\" operation</p> <p>This operation can add rows, update rows, and remove rows in a single transaction. It is a very generic tool that can be used to create behaviors like \"insert if not exists\", \"update or insert (i.e. upsert)\", or even replace a portion of existing data with new data (e.g. replace all data where month=\"january\")</p> <p>The merge insert operation works by combining new data from a source table with existing data in a target table by using a join.  There are three categories of records.</p> <p>\"Matched\" records are records that exist in both the source table and the target table. \"Not matched\" records exist only in the source table (e.g. these are new data). \"Not matched by source\" records exist only in the target table (this is old data).</p> <p>The builder returned by this method can be used to customize what should happen for each category of data.</p> <p>Please note that the data will be reordered as part of this operation.  This is because updated rows will be deleted from the dataset and then reinserted at the end with the new values.  The order of the newly inserted rows may fluctuate randomly because a hash-join operation is used internally.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.merge_insert--parameters","title":"Parameters","text":"Union[str, Iterable[str]] <p>A column (or columns) to join on.  This is how records from the source table and target table are matched.  Typically this is some kind of key or id column.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.merge_insert--examples","title":"Examples","text":"<p>Use <code>when_matched_update_all()</code> and <code>when_not_matched_insert_all()</code> to perform an \"upsert\" operation.  This will update rows that already exist in the dataset and insert rows that do not exist.</p> <p>import lance import pyarrow as pa table = pa.table({\"a\": [2, 1, 3], \"b\": [\"a\", \"b\", \"c\"]}) dataset = lance.write_dataset(table, \"example\") new_table = pa.table({\"a\": [2, 3, 4], \"b\": [\"x\", \"y\", \"z\"]})</p> <p>Use <code>when_not_matched_insert_all()</code> to perform an \"insert if not exists\" operation.  This will only insert rows that do not already exist in the dataset.</p> <p>import lance import pyarrow as pa table = pa.table({\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"]}) dataset = lance.write_dataset(table, \"example2\") new_table = pa.table({\"a\": [2, 3, 4], \"b\": [\"x\", \"y\", \"z\"]})</p> <p>You are not required to provide all the columns. If you only want to update a subset of columns, you can omit columns you don't want to update. Omitted columns will keep their existing values if they are updated, or will be null if they are inserted.</p> <p>import lance import pyarrow as pa table = pa.table({\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"], \\ ...                   \"c\": [\"x\", \"y\", \"z\"]}) dataset = lance.write_dataset(table, \"example3\") new_table = pa.table({\"a\": [2, 3, 4], \"b\": [\"x\", \"y\", \"z\"]})</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.merge_insert--perform-a-upsert-operation","title":"Perform a \"upsert\" operation","text":"<p>dataset.merge_insert(\"a\")     \\ ...             .when_matched_update_all()     \\ ...             .when_not_matched_insert_all() \\ ...             .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0} dataset.to_table().sort_by(\"a\").to_pandas()    a  b 0  1  b 1  2  x 2  3  y 3  4  z</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.merge_insert--perform-an-insert-if-not-exists-operation","title":"Perform an \"insert if not exists\" operation","text":"<p>dataset.merge_insert(\"a\")     \\ ...             .when_not_matched_insert_all() \\ ...             .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 0, 'num_deleted_rows': 0} dataset.to_table().sort_by(\"a\").to_pandas()    a  b 0  1  a 1  2  b 2  3  c 3  4  z</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.merge_insert--perform-an-upsert-operation-only-updating-column-a","title":"Perform an \"upsert\" operation, only updating column \"a\"","text":"<p>dataset.merge_insert(\"a\")     \\ ...             .when_matched_update_all()     \\ ...             .when_not_matched_insert_all() \\ ...             .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0} dataset.to_table().sort_by(\"a\").to_pandas()    a  b     c 0  1  a     x 1  2  x     y 2  3  y     z 3  4  z  None</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.migrate_manifest_paths_v2","title":"<code>migrate_manifest_paths_v2()</code>","text":"<p>Migrate the manifest paths to the new format.</p> <p>This will update the manifest to use the new v2 format for paths.</p> <p>This function is idempotent, and can be run multiple times without changing the state of the object store.</p> <p>DANGER: this should not be run while other concurrent operations are happening. And it should also run until completion before resuming other operations.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.prewarm_index","title":"<code>prewarm_index(name)</code>","text":"<p>Prewarm an index</p> <p>This will load the entire index into memory.  This can help avoid cold start issues with index queries.  If the index does not fit in the index cache, then this will result in wasted I/O.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.prewarm_index--parameters","title":"Parameters","text":"<p>name: str     The name of the index to prewarm.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.replace_field_metadata","title":"<code>replace_field_metadata(field_name, new_metadata)</code>","text":"<p>Replace the metadata of a field in the schema</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.replace_field_metadata--parameters","title":"Parameters","text":"<p>field_name: str     The name of the field to replace the metadata for new_metadata: dict     The new metadata to set</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.replace_schema","title":"<code>replace_schema(schema)</code>","text":"<p>Not implemented (just override pyarrow dataset to prevent segfault)</p> <p>See method:<code>replace_schema_metadata</code> or method:<code>replace_field_metadata</code></p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.replace_schema_metadata","title":"<code>replace_schema_metadata(new_metadata)</code>","text":"<p>Replace the schema metadata of the dataset</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.replace_schema_metadata--parameters","title":"Parameters","text":"<p>new_metadata: dict     The new metadata to set</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.restore","title":"<code>restore()</code>","text":"<p>Restore the currently checked out version as the latest version of the dataset.</p> <p>This creates a new commit.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.sample","title":"<code>sample(num_rows, columns=None, randomize_order=True, **kwargs)</code>","text":"<p>Select a random sample of data</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.sample--parameters","title":"Parameters","text":"<p>num_rows: int     number of rows to retrieve columns: list of str, or dict of str to str default None     List of column names to be fetched.     Or a dictionary of column names to SQL expressions.     All columns are fetched if None or unspecified. **kwargs : dict, optional     see scanner() method for full parameter description.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.sample--returns","title":"Returns","text":"<p>table : Table</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.scanner","title":"<code>scanner(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, fragments=None, full_text_query=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, fast_search=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, include_deleted_rows=None, scan_stats_callback=None, strict_batch_size=None)</code>","text":"<p>Return a Scanner that can support various pushdowns.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.scanner--parameters","title":"Parameters","text":"<p>columns: list of str, or dict of str to str default None     List of column names to be fetched.     Or a dictionary of column names to SQL expressions.     All columns are fetched if None or unspecified. filter: pa.compute.Expression or str     Expression or str that is a valid SQL where clause. See     <code>Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;</code>_     for valid SQL expressions. limit: int, default None     Fetch up to this many rows. All rows if None or unspecified. offset: int, default None     Fetch starting with this row. 0 if None or unspecified. nearest: dict, default None     Get the rows corresponding to the K most similar vectors. Example:</p> <pre><code>.. code-block:: python\n\n    {\n        \"column\": &lt;embedding col name&gt;,\n        \"q\": &lt;query vector as pa.Float32Array&gt;,\n        \"k\": 10,\n        \"nprobes\": 1,\n        \"refine_factor\": 1\n    }\n</code></pre> int, default None <p>The target size of batches returned.  In some cases batches can be up to twice this size (but never larger than this).  In some cases batches can be smaller than this size.</p> <p>io_buffer_size: int, default None     The size of the IO buffer.  See <code>ScannerBuilder.io_buffer_size</code>     for more information. batch_readahead: int, optional     The number of batches to read ahead. fragment_readahead: int, optional     The number of fragments to read ahead. scan_in_order: bool, default True     Whether to read the fragments and batches in order. If false,     throughput may be higher, but batches will be returned out of order     and memory use might increase. fragments: iterable of LanceFragment, default None     If specified, only scan these fragments. If scan_in_order is True, then     the fragments will be scanned in the order given. prefilter: bool, default False     If True then the filter will be applied before the vector query is run.     This will generate more correct results but it may be a more costly     query.  It's generally good when the filter is highly selective.</p> <pre><code>If False then the filter will be applied after the vector query is run.\nThis will perform well but the results may have fewer than the requested\nnumber of rows (or be empty) if the rows closest to the query do not\nmatch the filter.  It's generally good when the filter is not very\nselective.\n</code></pre> <p>use_scalar_index: bool, default True     Lance will automatically use scalar indices to optimize a query.  In some     corner cases this can make query performance worse and this parameter can     be used to disable scalar indices in these cases. late_materialization: bool or List[str], default None     Allows custom control over late materialization.  Late materialization     fetches non-query columns using a take operation after the filter.  This     is useful when there are few results or columns are very large.</p> <pre><code>Early materialization can be better when there are many results or the\ncolumns are very narrow.\n\nIf True, then all columns are late materialized.\nIf False, then all columns are early materialized.\nIf a list of strings, then only the columns in the list are\nlate materialized.\n\nThe default uses a heuristic that assumes filters will select about 0.1%\nof the rows.  If your filter is more selective (e.g. find by id) you may\nwant to set this to True.  If your filter is not very selective (e.g.\nmatches 20% of the rows) you may want to set this to False.\n</code></pre> <p>full_text_query: str or dict, optional     query string to search for, the results will be ranked by BM25.     e.g. \"hello world\", would match documents containing \"hello\" or \"world\".     or a dictionary with the following keys:</p> <pre><code>- columns: list[str]\n    The columns to search,\n    currently only supports a single column in the columns list.\n- query: str\n    The query string to search for.\n</code></pre> <p>fast_search:  bool, default False     If True, then the search will only be performed on the indexed data, which     yields faster search time. scan_stats_callback: Callable[[ScanStatistics], None], default None     A callback function that will be called with the scan statistics after the     scan is complete.  Errors raised by the callback will be logged but not     re-raised. include_deleted_rows: bool, default False     If True, then rows that have been deleted, but are still present in the     fragment, will be returned.  These rows will have the _rowid column set     to null.  All other columns will reflect the value stored on disk and may     not be null.</p> <pre><code>Note: if this is a search operation, or a take operation (including scalar\nindexed scans) then deleted rows cannot be returned.\n</code></pre> <p>.. note::</p> <pre><code>For now, if BOTH filter and nearest is specified, then:\n\n1. nearest is executed first.\n2. The results are filtered afterwards.\n</code></pre> <p>For debugging ANN results, you can choose to not use the index even if present by specifying <code>use_index=False</code>. For example, the following will always return exact KNN results:</p> <p>.. code-block:: python</p> <pre><code>dataset.to_table(nearest={\n    \"column\": \"vector\",\n    \"k\": 10,\n    \"q\": &lt;query vector&gt;,\n    \"use_index\": False\n}\n</code></pre>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.session","title":"<code>session()</code>","text":"<p>Return the dataset session, which holds the dataset's state.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.take","title":"<code>take(indices, columns=None)</code>","text":"<p>Select rows of data by index.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.take--parameters","title":"Parameters","text":"<p>indices : Array or array-like     indices of rows to select in the dataset. columns: list of str, or dict of str to str default None     List of column names to be fetched.     Or a dictionary of column names to SQL expressions.     All columns are fetched if None or unspecified.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.take--returns","title":"Returns","text":"<p>table : pyarrow.Table</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.take_blobs","title":"<code>take_blobs(blob_column, ids=None, addresses=None, indices=None)</code>","text":"<p>Select blobs by row IDs.</p> <p>Instead of loading large binary blob data into memory before processing it, this API allows you to open binary blob data as a regular Python file-like object. For more details, see class:<code>lance.BlobFile</code>.</p> <p>Exactly one of ids, addresses, or indices must be specified. Parameters</p> <p>blob_column : str     The name of the blob column to select. ids : Integer Array or array-like     row IDs to select in the dataset. addresses: Integer Array or array-like     The (unstable) row addresses to select in the dataset. indices : Integer Array or array-like     The offset / indices of the row in the dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.take_blobs--returns","title":"Returns","text":"<p>blob_files : List[BlobFile]</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.to_batches","title":"<code>to_batches(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, full_text_query=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, strict_batch_size=None, **kwargs)</code>","text":"<p>Read the dataset as materialized record batches.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.to_batches--parameters","title":"Parameters","text":"<p>**kwargs : dict, optional     Arguments for meth:<code>~LanceDataset.scanner</code>.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.to_batches--returns","title":"Returns","text":"<p>record_batches : Iterator of class:<code>~pyarrow.RecordBatch</code></p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.to_table","title":"<code>to_table(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, fast_search=None, full_text_query=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, include_deleted_rows=None)</code>","text":"<p>Read the data into memory as a class:<code>pyarrow.Table</code></p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.to_table--parameters","title":"Parameters","text":"<p>columns: list of str, or dict of str to str default None     List of column names to be fetched.     Or a dictionary of column names to SQL expressions.     All columns are fetched if None or unspecified. filter : pa.compute.Expression or str     Expression or str that is a valid SQL where clause. See     <code>Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;</code>_     for valid SQL expressions. limit: int, default None     Fetch up to this many rows. All rows if None or unspecified. offset: int, default None     Fetch starting with this row. 0 if None or unspecified. nearest: dict, default None     Get the rows corresponding to the K most similar vectors. Example:</p> <pre><code>.. code-block:: python\n\n    {\n        \"column\": &lt;embedding col name&gt;,\n        \"q\": &lt;query vector as pa.Float32Array&gt;,\n        \"k\": 10,\n        \"metric\": \"cosine\",\n        \"nprobes\": 1,\n        \"refine_factor\": 1\n    }\n</code></pre> int, optional <p>The number of rows to read at a time.</p> <p>io_buffer_size: int, default None     The size of the IO buffer.  See <code>ScannerBuilder.io_buffer_size</code>     for more information. batch_readahead: int, optional     The number of batches to read ahead. fragment_readahead: int, optional     The number of fragments to read ahead. scan_in_order: bool, optional, default True     Whether to read the fragments and batches in order. If false,     throughput may be higher, but batches will be returned out of order     and memory use might increase. prefilter: bool, optional, default False     Run filter before the vector search. late_materialization: bool or List[str], default None     Allows custom control over late materialization.  See     <code>ScannerBuilder.late_materialization</code> for more information. use_scalar_index: bool, default True     Allows custom control over scalar index usage.  See     <code>ScannerBuilder.use_scalar_index</code> for more information. with_row_id: bool, optional, default False     Return row ID. with_row_address: bool, optional, default False     Return row address use_stats: bool, optional, default True     Use stats pushdown during filters. fast_search: bool, optional, default False full_text_query: str or dict, optional     query string to search for, the results will be ranked by BM25.     e.g. \"hello world\", would match documents contains \"hello\" or \"world\".     or a dictionary with the following keys:</p> <pre><code>- columns: list[str]\n    The columns to search,\n    currently only supports a single column in the columns list.\n- query: str\n    The query string to search for.\n</code></pre> <p>include_deleted_rows: bool, optional, default False     If True, then rows that have been deleted, but are still present in the     fragment, will be returned.  These rows will have the _rowid column set     to null.  All other columns will reflect the value stored on disk and may     not be null.</p> <pre><code>Note: if this is a search operation, or a take operation (including scalar\nindexed scans) then deleted rows cannot be returned.\n</code></pre>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.to_table--notes","title":"Notes","text":"<p>If BOTH filter and nearest is specified, then:</p> <ol> <li>nearest is executed first.</li> <li>The results are filtered afterward, unless pre-filter sets to True.</li> </ol>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.update","title":"<code>update(updates, where=None)</code>","text":"<p>Update column values for rows matching where.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.update--parameters","title":"Parameters","text":"<p>updates : dict of str to str     A mapping of column names to a SQL expression. where : str, optional     A SQL predicate indicating which rows should be updated.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.update--returns","title":"Returns","text":"<p>updates : dict     A dictionary containing the number of rows updated.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.update--examples","title":"Examples","text":"<p>import lance import pyarrow as pa table = pa.table({\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"]}) dataset = lance.write_dataset(table, \"example\") update_stats = dataset.update(dict(a = 'a + 2'), where=\"b != 'a'\") update_stats[\"num_updated_rows\"] = 2 dataset.to_table().to_pandas()    a  b 0  1  a 1  4  b 2  5  c</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.validate","title":"<code>validate()</code>","text":"<p>Validate the dataset.</p> <p>This checks the integrity of the dataset and will raise an exception if the dataset is corrupted.</p>"},{"location":"api/py_modules/#lance.dataset.LanceDataset.versions","title":"<code>versions()</code>","text":"<p>Return all versions in this dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation","title":"<code>LanceOperation</code>","text":""},{"location":"api/py_modules/#lance.dataset.LanceOperation.Append","title":"<code>Append</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>Append new rows to the dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Append--attributes","title":"Attributes","text":"<p>fragments: list[FragmentMetadata]     The fragments that contain the new rows.</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Append--warning","title":"Warning","text":"<p>This is an advanced API for distributed operations. To append to a dataset on a single machine, use :func:<code>lance.write_dataset</code>.</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Append--examples","title":"Examples","text":"<p>To append new rows to a dataset, first use :meth:<code>lance.fragment.LanceFragment.create</code> to create fragments. Then collect the fragment metadata into a list and pass it to this class. Finally, pass the operation to the :meth:<code>LanceDataset.commit</code> method to create the new dataset.</p> <p>import lance import pyarrow as pa tab1 = pa.table({\"a\": [1, 2], \"b\": [\"a\", \"b\"]}) dataset = lance.write_dataset(tab1, \"example\") tab2 = pa.table({\"a\": [3, 4], \"b\": [\"c\", \"d\"]}) fragment = lance.fragment.LanceFragment.create(\"example\", tab2) operation = lance.LanceOperation.Append([fragment]) dataset = lance.LanceDataset.commit(\"example\", operation, ...                                     read_version=dataset.version) dataset.to_table().to_pandas()    a  b 0  1  a 1  2  b 2  3  c 3  4  d</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.BaseOperation","title":"<code>BaseOperation</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for operations that can be applied to a dataset.</p> <p>See available operations under :class:<code>LanceOperation</code>.</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.CreateIndex","title":"<code>CreateIndex</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>Operation that creates an index on the dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.DataReplacement","title":"<code>DataReplacement</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>Operation that replaces existing datafiles in the dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.DataReplacementGroup","title":"<code>DataReplacementGroup</code>  <code>dataclass</code>","text":"<p>Group of data replacements</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Delete","title":"<code>Delete</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>Remove fragments or rows from the dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Delete--attributes","title":"Attributes","text":"<p>updated_fragments: list[FragmentMetadata]     The fragments that have been updated with new deletion vectors. deleted_fragment_ids: list[int]     The ids of the fragments that have been deleted entirely. These are     the fragments where :meth:<code>LanceFragment.delete()</code> returned None. predicate: str     The original SQL predicate used to select the rows to delete.</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Delete--warning","title":"Warning","text":"<p>This is an advanced API for distributed operations. To delete rows from dataset on a single machine, use :meth:<code>lance.LanceDataset.delete</code>.</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Delete--examples","title":"Examples","text":"<p>To delete rows from a dataset, call :meth:<code>lance.fragment.LanceFragment.delete</code> on each of the fragments. If that returns a new fragment, add that to the <code>updated_fragments</code> list. If it returns None, that means the whole fragment was deleted, so add the fragment id to the <code>deleted_fragment_ids</code>. Finally, pass the operation to the :meth:<code>LanceDataset.commit</code> method to complete the deletion operation.</p> <p>import lance import pyarrow as pa table = pa.table({\"a\": [1, 2], \"b\": [\"a\", \"b\"]}) dataset = lance.write_dataset(table, \"example\") table = pa.table({\"a\": [3, 4], \"b\": [\"c\", \"d\"]}) dataset = lance.write_dataset(table, \"example\", mode=\"append\") dataset.to_table().to_pandas()    a  b 0  1  a 1  2  b 2  3  c 3  4  d predicate = \"a &gt;= 2\" updated_fragments = [] deleted_fragment_ids = [] for fragment in dataset.get_fragments(): ...     new_fragment = fragment.delete(predicate) ...     if new_fragment is not None: ...         updated_fragments.append(new_fragment) ...     else: ...         deleted_fragment_ids.append(fragment.fragment_id) operation = lance.LanceOperation.Delete(updated_fragments, ...                                         deleted_fragment_ids, ...                                         predicate) dataset = lance.LanceDataset.commit(\"example\", operation, ...                                     read_version=dataset.version) dataset.to_table().to_pandas()    a  b 0  1  a</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Merge","title":"<code>Merge</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>Operation that adds columns. Unlike Overwrite, this should not change the structure of the fragments, allowing existing indices to be kept.</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Merge--attributes","title":"Attributes","text":"<p>fragments: iterable of FragmentMetadata     The fragments that make up the new dataset. schema: LanceSchema or pyarrow.Schema     The schema of the new dataset. Passing a LanceSchema is preferred,     and passing a pyarrow.Schema is deprecated.</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Merge--warning","title":"Warning","text":"<p>This is an advanced API for distributed operations. To overwrite or create new dataset on a single machine, use :func:<code>lance.write_dataset</code>.</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Merge--examples","title":"Examples","text":"<p>To add new columns to a dataset, first define a method that will create the new columns based on the existing columns. Then use :meth:<code>lance.fragment.LanceFragment.add_columns</code></p> <p>import lance import pyarrow as pa import pyarrow.compute as pc table = pa.table({\"a\": [1, 2, 3, 4], \"b\": [\"a\", \"b\", \"c\", \"d\"]}) dataset = lance.write_dataset(table, \"example\") dataset.to_table().to_pandas()    a  b 0  1  a 1  2  b 2  3  c 3  4  d def double_a(batch: pa.RecordBatch) -&gt; pa.RecordBatch: ...     doubled = pc.multiply(batch[\"a\"], 2) ...     return pa.record_batch([doubled], [\"a_doubled\"]) fragments = [] for fragment in dataset.get_fragments(): ...     new_fragment, new_schema = fragment.merge_columns(double_a, ...                                                       columns=['a']) ...     fragments.append(new_fragment) operation = lance.LanceOperation.Merge(fragments, new_schema) dataset = lance.LanceDataset.commit(\"example\", operation, ...                                     read_version=dataset.version) dataset.to_table().to_pandas()    a  b  a_doubled 0  1  a          2 1  2  b          4 2  3  c          6 3  4  d          8</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Overwrite","title":"<code>Overwrite</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>Overwrite or create a new dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Overwrite--attributes","title":"Attributes","text":"<p>new_schema: pyarrow.Schema     The schema of the new dataset. fragments: list[FragmentMetadata]     The fragments that make up the new dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Overwrite--warning","title":"Warning","text":"<p>This is an advanced API for distributed operations. To overwrite or create new dataset on a single machine, use :func:<code>lance.write_dataset</code>.</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Overwrite--examples","title":"Examples","text":"<p>To create or overwrite a dataset, first use :meth:<code>lance.fragment.LanceFragment.create</code> to create fragments. Then collect the fragment metadata into a list and pass it along with the schema to this class. Finally, pass the operation to the :meth:<code>LanceDataset.commit</code> method to create the new dataset.</p> <p>import lance import pyarrow as pa tab1 = pa.table({\"a\": [1, 2], \"b\": [\"a\", \"b\"]}) tab2 = pa.table({\"a\": [3, 4], \"b\": [\"c\", \"d\"]}) fragment1 = lance.fragment.LanceFragment.create(\"example\", tab1) fragment2 = lance.fragment.LanceFragment.create(\"example\", tab2) fragments = [fragment1, fragment2] operation = lance.LanceOperation.Overwrite(tab1.schema, fragments) dataset = lance.LanceDataset.commit(\"example\", operation) dataset.to_table().to_pandas()    a  b 0  1  a 1  2  b 2  3  c 3  4  d</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Project","title":"<code>Project</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>Operation that project columns. Use this operator for drop column or rename/swap column.</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Project--attributes","title":"Attributes","text":"<p>schema: LanceSchema     The lance schema of the new dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Project--examples","title":"Examples","text":"<p>Use the projece operator to swap column:</p> <p>import lance import pyarrow as pa import pyarrow.compute as pc from lance.schema import LanceSchema table = pa.table({\"a\": [1, 2], \"b\": [\"a\", \"b\"], \"b1\": [\"c\", \"d\"]}) dataset = lance.write_dataset(table, \"example\") dataset.to_table().to_pandas()    a  b b1 0  1  a  c 1  2  b  d</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Project--rename-column-b-into-b0-and-rename-b1-into-b","title":"rename column <code>b</code> into <code>b0</code> and rename b1 into <code>b</code>","text":"<p>table = pa.table({\"a\": [3, 4], \"b0\": [\"a\", \"b\"], \"b\": [\"c\", \"d\"]}) lance_schema = LanceSchema.from_pyarrow(table.schema) operation = lance.LanceOperation.Project(lance_schema) dataset = lance.LanceDataset.commit(\"example\", operation, read_version=1) dataset.to_table().to_pandas()    a b0  b 0  1  a  c 1  2  b  d</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Restore","title":"<code>Restore</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>Operation that restores a previous version of the dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Rewrite","title":"<code>Rewrite</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>Operation that rewrites one or more files and indices into one or more files and indices.</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Rewrite--attributes","title":"Attributes","text":"<p>groups: list[RewriteGroup]     Groups of files that have been rewritten. rewritten_indices: list[RewrittenIndex]     Indices that have been rewritten.</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Rewrite--warning","title":"Warning","text":"<p>This is an advanced API not intended for general use.</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.RewriteGroup","title":"<code>RewriteGroup</code>  <code>dataclass</code>","text":"<p>Collection of rewritten files</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.RewrittenIndex","title":"<code>RewrittenIndex</code>  <code>dataclass</code>","text":"<p>An index that has been rewritten</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Update","title":"<code>Update</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>Operation that updates rows in the dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceOperation.Update--attributes","title":"Attributes","text":"<p>removed_fragment_ids: list[int]     The ids of the fragments that have been removed entirely. updated_fragments: list[FragmentMetadata]     The fragments that have been updated with new deletion vectors. new_fragments: list[FragmentMetadata]     The fragments that contain the new rows.</p>"},{"location":"api/py_modules/#lance.dataset.LanceScanner","title":"<code>LanceScanner</code>","text":"<p>               Bases: <code>Scanner</code></p>"},{"location":"api/py_modules/#lance.dataset.LanceScanner.dataset_schema","title":"<code>dataset_schema</code>  <code>property</code>","text":"<p>The schema with which batches will be read from fragments.</p>"},{"location":"api/py_modules/#lance.dataset.LanceScanner.analyze_plan","title":"<code>analyze_plan()</code>","text":"<p>Execute the plan for this scanner and display with runtime metrics.</p>"},{"location":"api/py_modules/#lance.dataset.LanceScanner.analyze_plan--parameters","title":"Parameters","text":"<p>verbose : bool, default False     Use a verbose output format.</p>"},{"location":"api/py_modules/#lance.dataset.LanceScanner.analyze_plan--returns","title":"Returns","text":"<p>plan : str</p>"},{"location":"api/py_modules/#lance.dataset.LanceScanner.count_rows","title":"<code>count_rows()</code>","text":"<p>Count rows matching the scanner filter.</p>"},{"location":"api/py_modules/#lance.dataset.LanceScanner.count_rows--returns","title":"Returns","text":"<p>count : int</p>"},{"location":"api/py_modules/#lance.dataset.LanceScanner.explain_plan","title":"<code>explain_plan(verbose=False)</code>","text":"<p>Return the execution plan for this scanner.</p>"},{"location":"api/py_modules/#lance.dataset.LanceScanner.explain_plan--parameters","title":"Parameters","text":"<p>verbose : bool, default False     Use a verbose output format.</p>"},{"location":"api/py_modules/#lance.dataset.LanceScanner.explain_plan--returns","title":"Returns","text":"<p>plan : str</p>"},{"location":"api/py_modules/#lance.dataset.LanceScanner.from_batches","title":"<code>from_batches(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Not implemented</p>"},{"location":"api/py_modules/#lance.dataset.LanceScanner.from_dataset","title":"<code>from_dataset(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Not implemented</p>"},{"location":"api/py_modules/#lance.dataset.LanceScanner.from_fragment","title":"<code>from_fragment(*args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Not implemented</p>"},{"location":"api/py_modules/#lance.dataset.LanceScanner.head","title":"<code>head(num_rows)</code>","text":"<p>Load the first N rows of the dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceScanner.head--parameters","title":"Parameters","text":"<p>num_rows : int     The number of rows to load.</p>"},{"location":"api/py_modules/#lance.dataset.LanceScanner.head--returns","title":"Returns","text":"<p>Table</p>"},{"location":"api/py_modules/#lance.dataset.LanceScanner.scan_batches","title":"<code>scan_batches()</code>","text":"<p>Consume a Scanner in record batches with corresponding fragments.</p>"},{"location":"api/py_modules/#lance.dataset.LanceScanner.scan_batches--returns","title":"Returns","text":"<p>record_batches : iterator of TaggedRecordBatch</p>"},{"location":"api/py_modules/#lance.dataset.LanceScanner.take","title":"<code>take(indices)</code>","text":"<p>Not implemented</p>"},{"location":"api/py_modules/#lance.dataset.LanceScanner.to_table","title":"<code>to_table()</code>","text":"<p>Read the data into memory and return a pyarrow Table.</p>"},{"location":"api/py_modules/#lance.dataset.LanceStats","title":"<code>LanceStats</code>","text":"<p>Statistics about a LanceDataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceStats.data_stats","title":"<code>data_stats()</code>","text":"<p>Statistics about the data in the dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceStats.dataset_stats","title":"<code>dataset_stats(max_rows_per_group=1024)</code>","text":"<p>Statistics about the dataset.</p>"},{"location":"api/py_modules/#lance.dataset.LanceStats.index_stats","title":"<code>index_stats(index_name)</code>","text":"<p>Statistics about an index.</p>"},{"location":"api/py_modules/#lance.dataset.LanceStats.index_stats--parameters","title":"Parameters","text":"<p>index_name: str     The name of the index to get statistics for.</p>"},{"location":"api/py_modules/#lance.dataset.MergeInsertBuilder","title":"<code>MergeInsertBuilder</code>","text":"<p>               Bases: <code>_MergeInsertBuilder</code></p>"},{"location":"api/py_modules/#lance.dataset.MergeInsertBuilder.conflict_retries","title":"<code>conflict_retries(max_retries)</code>","text":"<p>Set number of times to retry the operation if there is contention.</p> <p>If this is set &gt; 0, then the operation will keep a copy of the input data either in memory or on disk (depending on the size of the data) and will retry the operation if there is contention.</p> <p>Default is 10.</p>"},{"location":"api/py_modules/#lance.dataset.MergeInsertBuilder.execute","title":"<code>execute(data_obj, *, schema=None)</code>","text":"<p>Executes the merge insert operation</p> <p>This function updates the original dataset and returns a dictionary with information about merge statistics - i.e. the number of inserted, updated, and deleted rows.</p>"},{"location":"api/py_modules/#lance.dataset.MergeInsertBuilder.execute--parameters","title":"Parameters","text":"ReaderLike <p>The new data to use as the source table for the operation.  This parameter can be any source of data (e.g. table / dataset) that :func:<code>~lance.write_dataset</code> accepts.</p> <p>schema: Optional[pa.Schema]     The schema of the data.  This only needs to be supplied whenever the data     source is some kind of generator.</p>"},{"location":"api/py_modules/#lance.dataset.MergeInsertBuilder.execute_uncommitted","title":"<code>execute_uncommitted(data_obj, *, schema=None)</code>","text":"<p>Executes the merge insert operation without committing</p> <p>This function updates the original dataset and returns a dictionary with information about merge statistics - i.e. the number of inserted, updated, and deleted rows.</p>"},{"location":"api/py_modules/#lance.dataset.MergeInsertBuilder.execute_uncommitted--parameters","title":"Parameters","text":"ReaderLike <p>The new data to use as the source table for the operation.  This parameter can be any source of data (e.g. table / dataset) that :func:<code>~lance.write_dataset</code> accepts.</p> <p>schema: Optional[pa.Schema]     The schema of the data.  This only needs to be supplied whenever the data     source is some kind of generator.</p>"},{"location":"api/py_modules/#lance.dataset.MergeInsertBuilder.retry_timeout","title":"<code>retry_timeout(timeout)</code>","text":"<p>Set the timeout used to limit retries.</p> <p>This is the maximum time to spend on the operation before giving up. At least one attempt will be made, regardless of how long it takes to complete. Subsequent attempts will be cancelled once this timeout is reached. If the timeout has been reached during the first attempt, the operation will be cancelled immediately before making a second attempt.</p> <p>The default is 30 seconds.</p>"},{"location":"api/py_modules/#lance.dataset.MergeInsertBuilder.when_matched_update_all","title":"<code>when_matched_update_all(condition=None)</code>","text":"<p>Configure the operation to update matched rows</p> <p>After this method is called, when the merge insert operation executes, any rows that match both the source table and the target table will be updated.  The rows from the target table will be removed and the rows from the source table will be added.</p> <p>An optional condition may be specified.  This should be an SQL filter and, if present, then only matched rows that also satisfy this filter will be updated.  The SQL filter should use the prefix <code>target.</code> to refer to columns in the target table and the prefix <code>source.</code> to refer to columns in the source table.  For example, <code>source.last_update &lt; target.last_update</code>.</p> <p>If a condition is specified and rows do not satisfy the condition then these rows will not be updated.  Failure to satisfy the filter does not cause a \"matched\" row to become a \"not matched\" row.</p>"},{"location":"api/py_modules/#lance.dataset.MergeInsertBuilder.when_not_matched_by_source_delete","title":"<code>when_not_matched_by_source_delete(expr=None)</code>","text":"<p>Configure the operation to delete source rows that do not match</p> <p>After this method is called, when the merge insert operation executes, any rows that exist only in the target table will be deleted.  An optional filter can be specified to limit the scope of the delete operation.  If given (as an SQL filter) then only rows which match the filter will be deleted.</p>"},{"location":"api/py_modules/#lance.dataset.MergeInsertBuilder.when_not_matched_insert_all","title":"<code>when_not_matched_insert_all()</code>","text":"<p>Configure the operation to insert not matched rows</p> <p>After this method is called, when the merge insert operation executes, any rows that exist only in the source table will be inserted into the target table.</p>"},{"location":"api/py_modules/#lance.dataset.ScannerBuilder","title":"<code>ScannerBuilder</code>","text":""},{"location":"api/py_modules/#lance.dataset.ScannerBuilder.batch_readahead","title":"<code>batch_readahead(nbatches=None)</code>","text":"<p>This parameter is ignored when reading v2 files</p>"},{"location":"api/py_modules/#lance.dataset.ScannerBuilder.batch_size","title":"<code>batch_size(batch_size)</code>","text":"<p>Set batch size for Scanner</p>"},{"location":"api/py_modules/#lance.dataset.ScannerBuilder.fast_search","title":"<code>fast_search(flag)</code>","text":"<p>Enable fast search, which only perform search on the indexed data.</p> <p>Users can use <code>Table::optimize()</code> or <code>create_index()</code> to include the new data into index, thus make new data searchable.</p>"},{"location":"api/py_modules/#lance.dataset.ScannerBuilder.full_text_search","title":"<code>full_text_search(query, columns=None)</code>","text":"<p>Filter rows by full text searching. Experimental API, may remove it after we support to do this within <code>filter</code> SQL-like expression</p> <p>Must create inverted index on the given column before searching,</p>"},{"location":"api/py_modules/#lance.dataset.ScannerBuilder.full_text_search--parameters","title":"Parameters","text":"<p>query : str | Query     If str, the query string to search for, a match query would be performed.     If Query, the query object to search for,     and the <code>columns</code> parameter will be ignored. columns : list of str, optional     The columns to search in. If None, search in all indexed columns.</p>"},{"location":"api/py_modules/#lance.dataset.ScannerBuilder.include_deleted_rows","title":"<code>include_deleted_rows(flag)</code>","text":"<p>Include deleted rows</p> <p>Rows which have been deleted, but are still present in the fragment, will be returned.  These rows will have all columns (except _rowaddr) set to null</p>"},{"location":"api/py_modules/#lance.dataset.ScannerBuilder.io_buffer_size","title":"<code>io_buffer_size(io_buffer_size)</code>","text":"<p>Set the I/O buffer size for the Scanner</p> <p>This is the amount of RAM that will be reserved for holding I/O received from storage before it is processed.  This is used to control the amount of memory used by the scanner.  If the buffer is full then the scanner will block until the buffer is processed.</p> <p>Generally this should scale with the number of concurrent I/O threads.  The default is 2GiB which comfortably provides enough space for somewhere between 32 and 256 concurrent I/O threads.</p> <p>This value is not a hard cap on the amount of RAM the scanner will use.  Some space is used for the compute (which can be controlled by the batch size) and Lance does not keep track of memory after it is returned to the user.</p> <p>Currently, if there is a single batch of data which is larger than the io buffer size then the scanner will deadlock.  This is a known issue and will be fixed in a future release.</p> <p>This parameter is only used when reading v2 files</p>"},{"location":"api/py_modules/#lance.dataset.ScannerBuilder.scan_in_order","title":"<code>scan_in_order(scan_in_order=True)</code>","text":"<p>Whether to scan the dataset in order of fragments and batches.</p> <p>If set to False, the scanner may read fragments concurrently and yield batches out of order. This may improve performance since it allows more concurrency in the scan, but can also use more memory.</p> <p>This parameter is ignored when using v2 files.  In the v2 file format there is no penalty to scanning in order and so all scans will scan in order.</p>"},{"location":"api/py_modules/#lance.dataset.ScannerBuilder.scan_stats_callback","title":"<code>scan_stats_callback(callback)</code>","text":"<p>Set a callback function that will be called with the scan statistics after the scan is complete.  Errors raised by the callback will be logged but not re-raised.</p>"},{"location":"api/py_modules/#lance.dataset.ScannerBuilder.strict_batch_size","title":"<code>strict_batch_size(strict_batch_size=False)</code>","text":"<p>If True, then all batches except the last batch will have exactly <code>batch_size</code> rows. By default, it is false. If this is true then small batches will need to be merged together which will require a data copy and incur a (typically very small) performance penalty.</p>"},{"location":"api/py_modules/#lance.dataset.ScannerBuilder.use_scalar_index","title":"<code>use_scalar_index(use_scalar_index=True)</code>","text":"<p>Set whether scalar indices should be used in a query</p> <p>Scans will use scalar indices, when available, to optimize queries with filters. However, in some corner cases, scalar indices may make performance worse.  This parameter allows users to disable scalar indices in these cases.</p>"},{"location":"api/py_modules/#lance.dataset.ScannerBuilder.use_stats","title":"<code>use_stats(use_stats=True)</code>","text":"<p>Enable use of statistics for query planning.</p> <p>Disabling statistics is used for debugging and benchmarking purposes. This should be left on for normal use.</p>"},{"location":"api/py_modules/#lance.dataset.ScannerBuilder.with_row_address","title":"<code>with_row_address(with_row_address=True)</code>","text":"<p>Enables returns with row addresses.</p> <p>Row addresses are a unique but unstable identifier for each row in the dataset that consists of the fragment id (upper 32 bits) and the row offset in the fragment (lower 32 bits).  Row IDs are generally preferred since they do not change when a row is modified or compacted.  However, row addresses may be useful in some advanced use cases.</p>"},{"location":"api/py_modules/#lance.dataset.ScannerBuilder.with_row_id","title":"<code>with_row_id(with_row_id=True)</code>","text":"<p>Enable returns with row IDs.</p>"},{"location":"api/py_modules/#lance.dataset.Tags","title":"<code>Tags</code>","text":"<p>Dataset tag manager.</p>"},{"location":"api/py_modules/#lance.dataset.Tags.create","title":"<code>create(tag, version)</code>","text":"<p>Create a tag for a given dataset version.</p>"},{"location":"api/py_modules/#lance.dataset.Tags.create--parameters","title":"Parameters","text":"<p>tag: str,     The name of the tag to create. This name must be unique among all tag     names for the dataset. version: int,     The dataset version to tag.</p>"},{"location":"api/py_modules/#lance.dataset.Tags.delete","title":"<code>delete(tag)</code>","text":"<p>Delete tag from the dataset.</p>"},{"location":"api/py_modules/#lance.dataset.Tags.delete--parameters","title":"Parameters","text":"<p>tag: str,     The name of the tag to delete.</p>"},{"location":"api/py_modules/#lance.dataset.Tags.list","title":"<code>list()</code>","text":"<p>List all dataset tags.</p>"},{"location":"api/py_modules/#lance.dataset.Tags.list--returns","title":"Returns","text":"<p>dict[str, Tag]     A dictionary mapping tag names to version numbers.</p>"},{"location":"api/py_modules/#lance.dataset.Tags.update","title":"<code>update(tag, version)</code>","text":"<p>Update tag to a new version.</p>"},{"location":"api/py_modules/#lance.dataset.Tags.update--parameters","title":"Parameters","text":"<p>tag: str,     The name of the tag to update. version: int,     The new dataset version to tag.</p>"},{"location":"api/py_modules/#lance.dataset.VectorIndexReader","title":"<code>VectorIndexReader</code>","text":"<p>This class allows you to initialize a reader for a specific vector index, retrieve the number of partitions, access the centroids of the index, and read specific partitions of the index.</p>"},{"location":"api/py_modules/#lance.dataset.VectorIndexReader--parameters","title":"Parameters","text":"<p>dataset: LanceDataset     The dataset containing the index. index_name: str     The name of the vector index to read.</p>"},{"location":"api/py_modules/#lance.dataset.VectorIndexReader--examples","title":"Examples","text":"<p>.. code-block:: python</p> <pre><code>import lance\nfrom lance.dataset import VectorIndexReader\nimport numpy as np\nimport pyarrow as pa\nvectors = np.random.rand(256, 2)\ndata = pa.table({\"vector\": pa.array(vectors.tolist(),\n    type=pa.list_(pa.float32(), 2))})\ndataset = lance.write_dataset(data, \"/tmp/index_reader_demo\")\ndataset.create_index(\"vector\", index_type=\"IVF_PQ\",\n    num_partitions=4, num_sub_vectors=2)\nreader = VectorIndexReader(dataset, \"vector_idx\")\nassert reader.num_partitions() == 4\npartition = reader.read_partition(0)\nassert \"_rowid\" in partition.column_names\n</code></pre>"},{"location":"api/py_modules/#lance.dataset.VectorIndexReader--exceptions","title":"Exceptions","text":"<p>ValueError     If the specified index is not a vector index.</p>"},{"location":"api/py_modules/#lance.dataset.VectorIndexReader.centroids","title":"<code>centroids()</code>","text":"<p>Returns the centroids of the index</p>"},{"location":"api/py_modules/#lance.dataset.VectorIndexReader.centroids--returns","title":"Returns","text":"<p>np.ndarray     The centroids of IVF     with shape (num_partitions, dim)</p>"},{"location":"api/py_modules/#lance.dataset.VectorIndexReader.num_partitions","title":"<code>num_partitions()</code>","text":"<p>Returns the number of partitions in the dataset.</p>"},{"location":"api/py_modules/#lance.dataset.VectorIndexReader.num_partitions--returns","title":"Returns","text":"<p>int     The number of partitions.</p>"},{"location":"api/py_modules/#lance.dataset.VectorIndexReader.read_partition","title":"<code>read_partition(partition_id, *, with_vector=False)</code>","text":"<p>Returns a pyarrow table for the given IVF partition</p>"},{"location":"api/py_modules/#lance.dataset.VectorIndexReader.read_partition--parameters","title":"Parameters","text":"<p>partition_id: int     The id of the partition to read with_vector: bool, default False     Whether to include the vector column in the reader,     for IVF_PQ, the vector column is PQ codes</p>"},{"location":"api/py_modules/#lance.dataset.VectorIndexReader.read_partition--returns","title":"Returns","text":"<p>pa.Table     A pyarrow table for the given partition,     containing the row IDs, and quantized vectors (if with_vector is True).</p>"},{"location":"api/py_modules/#lance.dataset.write_dataset","title":"<code>write_dataset(data_obj, uri, schema=None, mode='create', *, max_rows_per_file=1024 * 1024, max_rows_per_group=1024, max_bytes_per_file=90 * 1024 * 1024 * 1024, commit_lock=None, progress=None, storage_options=None, data_storage_version=None, use_legacy_format=None, enable_v2_manifest_paths=False, enable_move_stable_row_ids=False)</code>","text":"<p>Write a given data_obj to the given uri</p>"},{"location":"api/py_modules/#lance.dataset.write_dataset--parameters","title":"Parameters","text":"<p>data_obj: Reader-like     The data to be written. Acceptable types are:     - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader     - Huggingface dataset uri: str, Path, or LanceDataset     Where to write the dataset to (directory). If a LanceDataset is passed,     the session will be reused. schema: Schema, optional     If specified and the input is a pandas DataFrame, use this schema     instead of the default pandas to arrow table conversion. mode: str     create - create a new dataset (raises if uri already exists).     overwrite - create a new snapshot version     append - create a new version that is the concat of the input the     latest version (raises if uri does not exist) max_rows_per_file: int, default 1024 * 1024     The max number of rows to write before starting a new file max_rows_per_group: int, default 1024     The max number of rows before starting a new group (in the same file) max_bytes_per_file: int, default 90 * 1024 * 1024 * 1024     The max number of bytes to write before starting a new file. This is a     soft limit. This limit is checked after each group is written, which     means larger groups may cause this to be overshot meaningfully. This     defaults to 90 GB, since we have a hard limit of 100 GB per file on     object stores. commit_lock : CommitLock, optional     A custom commit lock.  Only needed if your object store does not support     atomic commits.  See the user guide for more details. progress: FragmentWriteProgress, optional     Experimental API. Progress tracking for writing the fragment. Pass     a custom class that defines hooks to be called when each fragment is     starting to write and finishing writing. storage_options : optional, dict     Extra options that make sense for a particular storage connection. This is     used to store connection parameters like credentials, endpoint, etc. data_storage_version: optional, str, default None     The version of the data storage format to use. Newer versions are more     efficient but require newer versions of lance to read.  The default (None)     will use the latest stable version.  See the user guide for more details. use_legacy_format : optional, bool, default None     Deprecated method for setting the data storage version. Use the     <code>data_storage_version</code> parameter instead. enable_v2_manifest_paths : bool, optional     If True, and this is a new dataset, uses the new V2 manifest paths.     These paths provide more efficient opening of datasets with many     versions on object stores. This parameter has no effect if the dataset     already exists. To migrate an existing dataset, instead use the     :meth:<code>LanceDataset.migrate_manifest_paths_v2</code> method. Default is False. enable_move_stable_row_ids : bool, optional     Experimental parameter: if set to true, the writer will use move-stable row ids.     These row ids are stable after compaction operations, but not after updates.     This makes compaction more efficient, since with stable row ids no     secondary indices need to be updated to point to new row ids.</p>"},{"location":"api/python/","title":"Python API Guide","text":"<p><code>Lance</code> is a columnar format that is specifically designed for efficient multi-modal data processing.</p>"},{"location":"api/python/#lance-dataset","title":"Lance Dataset","text":"<p>The core of Lance is the <code>LanceDataset</code> class. User can open a dataset by using <code>lance.dataset</code>.</p> <p>For complete API reference, see the Python Modules page.</p>"},{"location":"api/python/#basic-ios","title":"Basic IOs","text":"<p>The following functions are used to read and write data in Lance format.</p>"},{"location":"api/python/#insert-data","title":"Insert Data","text":"<p>Insert data into the dataset.</p>"},{"location":"api/python/#lance.dataset.LanceDataset.insert--parameters","title":"Parameters","text":"<p>data_obj: Reader-like     The data to be written. Acceptable types are:     - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader     - Huggingface dataset mode: str, default 'append'     The mode to use when writing the data. Options are:         create - create a new dataset (raises if uri already exists).         overwrite - create a new snapshot version         append - create a new version that is the concat of the input the         latest version (raises if uri does not exist) **kwargs : dict, optional     Additional keyword arguments to pass to :func:<code>write_dataset</code>.</p> Source code in <code>lance/dataset.py</code> <pre><code>def insert(\n    self,\n    data: ReaderLike,\n    *,\n    mode=\"append\",\n    **kwargs,\n):\n    \"\"\"\n    Insert data into the dataset.\n\n    Parameters\n    ----------\n    data_obj: Reader-like\n        The data to be written. Acceptable types are:\n        - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader\n        - Huggingface dataset\n    mode: str, default 'append'\n        The mode to use when writing the data. Options are:\n            **create** - create a new dataset (raises if uri already exists).\n            **overwrite** - create a new snapshot version\n            **append** - create a new version that is the concat of the input the\n            latest version (raises if uri does not exist)\n    **kwargs : dict, optional\n        Additional keyword arguments to pass to :func:`write_dataset`.\n    \"\"\"\n    new_ds = write_dataset(data, self, mode=mode, **kwargs)\n    self._ds = new_ds._ds\n</code></pre>"},{"location":"api/python/#scanner","title":"Scanner","text":"<p>Return a Scanner that can support various pushdowns.</p>"},{"location":"api/python/#lance.dataset.LanceDataset.scanner--parameters","title":"Parameters","text":"<p>columns: list of str, or dict of str to str default None     List of column names to be fetched.     Or a dictionary of column names to SQL expressions.     All columns are fetched if None or unspecified. filter: pa.compute.Expression or str     Expression or str that is a valid SQL where clause. See     <code>Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;</code>_     for valid SQL expressions. limit: int, default None     Fetch up to this many rows. All rows if None or unspecified. offset: int, default None     Fetch starting with this row. 0 if None or unspecified. nearest: dict, default None     Get the rows corresponding to the K most similar vectors. Example:</p> <pre><code>.. code-block:: python\n\n    {\n        \"column\": &lt;embedding col name&gt;,\n        \"q\": &lt;query vector as pa.Float32Array&gt;,\n        \"k\": 10,\n        \"nprobes\": 1,\n        \"refine_factor\": 1\n    }\n</code></pre> int, default None <p>The target size of batches returned.  In some cases batches can be up to twice this size (but never larger than this).  In some cases batches can be smaller than this size.</p> <p>io_buffer_size: int, default None     The size of the IO buffer.  See <code>ScannerBuilder.io_buffer_size</code>     for more information. batch_readahead: int, optional     The number of batches to read ahead. fragment_readahead: int, optional     The number of fragments to read ahead. scan_in_order: bool, default True     Whether to read the fragments and batches in order. If false,     throughput may be higher, but batches will be returned out of order     and memory use might increase. fragments: iterable of LanceFragment, default None     If specified, only scan these fragments. If scan_in_order is True, then     the fragments will be scanned in the order given. prefilter: bool, default False     If True then the filter will be applied before the vector query is run.     This will generate more correct results but it may be a more costly     query.  It's generally good when the filter is highly selective.</p> <pre><code>If False then the filter will be applied after the vector query is run.\nThis will perform well but the results may have fewer than the requested\nnumber of rows (or be empty) if the rows closest to the query do not\nmatch the filter.  It's generally good when the filter is not very\nselective.\n</code></pre> <p>use_scalar_index: bool, default True     Lance will automatically use scalar indices to optimize a query.  In some     corner cases this can make query performance worse and this parameter can     be used to disable scalar indices in these cases. late_materialization: bool or List[str], default None     Allows custom control over late materialization.  Late materialization     fetches non-query columns using a take operation after the filter.  This     is useful when there are few results or columns are very large.</p> <pre><code>Early materialization can be better when there are many results or the\ncolumns are very narrow.\n\nIf True, then all columns are late materialized.\nIf False, then all columns are early materialized.\nIf a list of strings, then only the columns in the list are\nlate materialized.\n\nThe default uses a heuristic that assumes filters will select about 0.1%\nof the rows.  If your filter is more selective (e.g. find by id) you may\nwant to set this to True.  If your filter is not very selective (e.g.\nmatches 20% of the rows) you may want to set this to False.\n</code></pre> <p>full_text_query: str or dict, optional     query string to search for, the results will be ranked by BM25.     e.g. \"hello world\", would match documents containing \"hello\" or \"world\".     or a dictionary with the following keys:</p> <pre><code>- columns: list[str]\n    The columns to search,\n    currently only supports a single column in the columns list.\n- query: str\n    The query string to search for.\n</code></pre> <p>fast_search:  bool, default False     If True, then the search will only be performed on the indexed data, which     yields faster search time. scan_stats_callback: Callable[[ScanStatistics], None], default None     A callback function that will be called with the scan statistics after the     scan is complete.  Errors raised by the callback will be logged but not     re-raised. include_deleted_rows: bool, default False     If True, then rows that have been deleted, but are still present in the     fragment, will be returned.  These rows will have the _rowid column set     to null.  All other columns will reflect the value stored on disk and may     not be null.</p> <pre><code>Note: if this is a search operation, or a take operation (including scalar\nindexed scans) then deleted rows cannot be returned.\n</code></pre> <p>.. note::</p> <pre><code>For now, if BOTH filter and nearest is specified, then:\n\n1. nearest is executed first.\n2. The results are filtered afterwards.\n</code></pre> <p>For debugging ANN results, you can choose to not use the index even if present by specifying <code>use_index=False</code>. For example, the following will always return exact KNN results:</p> <p>.. code-block:: python</p> <pre><code>dataset.to_table(nearest={\n    \"column\": \"vector\",\n    \"k\": 10,\n    \"q\": &lt;query vector&gt;,\n    \"use_index\": False\n}\n</code></pre> Source code in <code>lance/dataset.py</code> <pre><code>def scanner(\n    self,\n    columns: Optional[Union[List[str], Dict[str, str]]] = None,\n    filter: Optional[Union[str, pa.compute.Expression]] = None,\n    limit: Optional[int] = None,\n    offset: Optional[int] = None,\n    nearest: Optional[dict] = None,\n    batch_size: Optional[int] = None,\n    batch_readahead: Optional[int] = None,\n    fragment_readahead: Optional[int] = None,\n    scan_in_order: Optional[bool] = None,\n    fragments: Optional[Iterable[LanceFragment]] = None,\n    full_text_query: Optional[Union[str, dict, FullTextQuery]] = None,\n    *,\n    prefilter: Optional[bool] = None,\n    with_row_id: Optional[bool] = None,\n    with_row_address: Optional[bool] = None,\n    use_stats: Optional[bool] = None,\n    fast_search: Optional[bool] = None,\n    io_buffer_size: Optional[int] = None,\n    late_materialization: Optional[bool | List[str]] = None,\n    use_scalar_index: Optional[bool] = None,\n    include_deleted_rows: Optional[bool] = None,\n    scan_stats_callback: Optional[Callable[[ScanStatistics], None]] = None,\n    strict_batch_size: Optional[bool] = None,\n) -&gt; LanceScanner:\n    \"\"\"Return a Scanner that can support various pushdowns.\n\n    Parameters\n    ----------\n    columns: list of str, or dict of str to str default None\n        List of column names to be fetched.\n        Or a dictionary of column names to SQL expressions.\n        All columns are fetched if None or unspecified.\n    filter: pa.compute.Expression or str\n        Expression or str that is a valid SQL where clause. See\n        `Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;`_\n        for valid SQL expressions.\n    limit: int, default None\n        Fetch up to this many rows. All rows if None or unspecified.\n    offset: int, default None\n        Fetch starting with this row. 0 if None or unspecified.\n    nearest: dict, default None\n        Get the rows corresponding to the K most similar vectors. Example:\n\n        .. code-block:: python\n\n            {\n                \"column\": &lt;embedding col name&gt;,\n                \"q\": &lt;query vector as pa.Float32Array&gt;,\n                \"k\": 10,\n                \"nprobes\": 1,\n                \"refine_factor\": 1\n            }\n\n    batch_size: int, default None\n        The target size of batches returned.  In some cases batches can be up to\n        twice this size (but never larger than this).  In some cases batches can\n        be smaller than this size.\n    io_buffer_size: int, default None\n        The size of the IO buffer.  See ``ScannerBuilder.io_buffer_size``\n        for more information.\n    batch_readahead: int, optional\n        The number of batches to read ahead.\n    fragment_readahead: int, optional\n        The number of fragments to read ahead.\n    scan_in_order: bool, default True\n        Whether to read the fragments and batches in order. If false,\n        throughput may be higher, but batches will be returned out of order\n        and memory use might increase.\n    fragments: iterable of LanceFragment, default None\n        If specified, only scan these fragments. If scan_in_order is True, then\n        the fragments will be scanned in the order given.\n    prefilter: bool, default False\n        If True then the filter will be applied before the vector query is run.\n        This will generate more correct results but it may be a more costly\n        query.  It's generally good when the filter is highly selective.\n\n        If False then the filter will be applied after the vector query is run.\n        This will perform well but the results may have fewer than the requested\n        number of rows (or be empty) if the rows closest to the query do not\n        match the filter.  It's generally good when the filter is not very\n        selective.\n    use_scalar_index: bool, default True\n        Lance will automatically use scalar indices to optimize a query.  In some\n        corner cases this can make query performance worse and this parameter can\n        be used to disable scalar indices in these cases.\n    late_materialization: bool or List[str], default None\n        Allows custom control over late materialization.  Late materialization\n        fetches non-query columns using a take operation after the filter.  This\n        is useful when there are few results or columns are very large.\n\n        Early materialization can be better when there are many results or the\n        columns are very narrow.\n\n        If True, then all columns are late materialized.\n        If False, then all columns are early materialized.\n        If a list of strings, then only the columns in the list are\n        late materialized.\n\n        The default uses a heuristic that assumes filters will select about 0.1%\n        of the rows.  If your filter is more selective (e.g. find by id) you may\n        want to set this to True.  If your filter is not very selective (e.g.\n        matches 20% of the rows) you may want to set this to False.\n    full_text_query: str or dict, optional\n        query string to search for, the results will be ranked by BM25.\n        e.g. \"hello world\", would match documents containing \"hello\" or \"world\".\n        or a dictionary with the following keys:\n\n        - columns: list[str]\n            The columns to search,\n            currently only supports a single column in the columns list.\n        - query: str\n            The query string to search for.\n    fast_search:  bool, default False\n        If True, then the search will only be performed on the indexed data, which\n        yields faster search time.\n    scan_stats_callback: Callable[[ScanStatistics], None], default None\n        A callback function that will be called with the scan statistics after the\n        scan is complete.  Errors raised by the callback will be logged but not\n        re-raised.\n    include_deleted_rows: bool, default False\n        If True, then rows that have been deleted, but are still present in the\n        fragment, will be returned.  These rows will have the _rowid column set\n        to null.  All other columns will reflect the value stored on disk and may\n        not be null.\n\n        Note: if this is a search operation, or a take operation (including scalar\n        indexed scans) then deleted rows cannot be returned.\n\n\n    .. note::\n\n        For now, if BOTH filter and nearest is specified, then:\n\n        1. nearest is executed first.\n        2. The results are filtered afterwards.\n\n\n    For debugging ANN results, you can choose to not use the index\n    even if present by specifying ``use_index=False``. For example,\n    the following will always return exact KNN results:\n\n\n    .. code-block:: python\n\n        dataset.to_table(nearest={\n            \"column\": \"vector\",\n            \"k\": 10,\n            \"q\": &lt;query vector&gt;,\n            \"use_index\": False\n        }\n\n    \"\"\"\n    builder = ScannerBuilder(self)\n    builder = self._apply_default_scan_options(builder)\n\n    # Calls the setter if the user provided a non-None value\n    # We need to avoid calling the setter with a None value so\n    # we don't override any defaults from _default_scan_options\n    def setopt(opt, val):\n        if val is not None:\n            opt(val)\n\n    setopt(builder.filter, filter)\n    setopt(builder.prefilter, prefilter)\n    setopt(builder.limit, limit)\n    setopt(builder.offset, offset)\n    setopt(builder.batch_size, batch_size)\n    setopt(builder.io_buffer_size, io_buffer_size)\n    setopt(builder.batch_readahead, batch_readahead)\n    setopt(builder.fragment_readahead, fragment_readahead)\n    setopt(builder.scan_in_order, scan_in_order)\n    setopt(builder.with_fragments, fragments)\n    setopt(builder.late_materialization, late_materialization)\n    setopt(builder.with_row_id, with_row_id)\n    setopt(builder.with_row_address, with_row_address)\n    setopt(builder.use_stats, use_stats)\n    setopt(builder.use_scalar_index, use_scalar_index)\n    setopt(builder.fast_search, fast_search)\n    setopt(builder.include_deleted_rows, include_deleted_rows)\n    setopt(builder.scan_stats_callback, scan_stats_callback)\n    setopt(builder.strict_batch_size, strict_batch_size)\n    # columns=None has a special meaning. we can't treat it as \"user didn't specify\"\n    if self._default_scan_options is None:\n        # No defaults, use user-provided, if any\n        builder = builder.columns(columns)\n    else:\n        default_columns = self._default_scan_options.get(\"columns\", None)\n        if default_columns is None:\n            # No default_columns, use user-provided, if any\n            builder = builder.columns(columns)\n        else:\n            if columns is not None:\n                # User supplied None, fallback to default (no way to override\n                # default to None)\n                builder = builder.columns(columns)\n            else:\n                # User supplied non-None, use that\n                builder = builder.columns(default_columns)\n\n    if full_text_query is not None:\n        if isinstance(full_text_query, (str, FullTextQuery)):\n            builder = builder.full_text_search(full_text_query)\n        elif isinstance(full_text_query, dict):\n            builder = builder.full_text_search(**full_text_query)\n    if nearest is not None:\n        builder = builder.nearest(**nearest)\n    return builder.to_scanner()\n</code></pre>"},{"location":"api/python/#convert-to-batches","title":"Convert to Batches","text":"<p>Read the dataset as materialized record batches.</p>"},{"location":"api/python/#lance.dataset.LanceDataset.to_batches--parameters","title":"Parameters","text":"<p>**kwargs : dict, optional     Arguments for meth:<code>~LanceDataset.scanner</code>.</p>"},{"location":"api/python/#lance.dataset.LanceDataset.to_batches--returns","title":"Returns","text":"<p>record_batches : Iterator of class:<code>~pyarrow.RecordBatch</code></p> Source code in <code>lance/dataset.py</code> <pre><code>def to_batches(\n    self,\n    columns: Optional[Union[List[str], Dict[str, str]]] = None,\n    filter: Optional[Union[str, pa.compute.Expression]] = None,\n    limit: Optional[int] = None,\n    offset: Optional[int] = None,\n    nearest: Optional[dict] = None,\n    batch_size: Optional[int] = None,\n    batch_readahead: Optional[int] = None,\n    fragment_readahead: Optional[int] = None,\n    scan_in_order: Optional[bool] = None,\n    *,\n    prefilter: Optional[bool] = None,\n    with_row_id: Optional[bool] = None,\n    with_row_address: Optional[bool] = None,\n    use_stats: Optional[bool] = None,\n    full_text_query: Optional[Union[str, dict]] = None,\n    io_buffer_size: Optional[int] = None,\n    late_materialization: Optional[bool | List[str]] = None,\n    use_scalar_index: Optional[bool] = None,\n    strict_batch_size: Optional[bool] = None,\n    **kwargs,\n) -&gt; Iterator[pa.RecordBatch]:\n    \"\"\"Read the dataset as materialized record batches.\n\n    Parameters\n    ----------\n    **kwargs : dict, optional\n        Arguments for :py:meth:`~LanceDataset.scanner`.\n\n    Returns\n    -------\n    record_batches : Iterator of :py:class:`~pyarrow.RecordBatch`\n    \"\"\"\n    return self.scanner(\n        columns=columns,\n        filter=filter,\n        limit=limit,\n        offset=offset,\n        nearest=nearest,\n        batch_size=batch_size,\n        io_buffer_size=io_buffer_size,\n        batch_readahead=batch_readahead,\n        fragment_readahead=fragment_readahead,\n        late_materialization=late_materialization,\n        use_scalar_index=use_scalar_index,\n        scan_in_order=scan_in_order,\n        prefilter=prefilter,\n        with_row_id=with_row_id,\n        with_row_address=with_row_address,\n        use_stats=use_stats,\n        full_text_query=full_text_query,\n        strict_batch_size=strict_batch_size,\n    ).to_batches()\n</code></pre>"},{"location":"api/python/#convert-to-table","title":"Convert to Table","text":"<p>Read the data into memory as a class:<code>pyarrow.Table</code></p>"},{"location":"api/python/#lance.dataset.LanceDataset.to_table--parameters","title":"Parameters","text":"<p>columns: list of str, or dict of str to str default None     List of column names to be fetched.     Or a dictionary of column names to SQL expressions.     All columns are fetched if None or unspecified. filter : pa.compute.Expression or str     Expression or str that is a valid SQL where clause. See     <code>Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;</code>_     for valid SQL expressions. limit: int, default None     Fetch up to this many rows. All rows if None or unspecified. offset: int, default None     Fetch starting with this row. 0 if None or unspecified. nearest: dict, default None     Get the rows corresponding to the K most similar vectors. Example:</p> <pre><code>.. code-block:: python\n\n    {\n        \"column\": &lt;embedding col name&gt;,\n        \"q\": &lt;query vector as pa.Float32Array&gt;,\n        \"k\": 10,\n        \"metric\": \"cosine\",\n        \"nprobes\": 1,\n        \"refine_factor\": 1\n    }\n</code></pre> int, optional <p>The number of rows to read at a time.</p> <p>io_buffer_size: int, default None     The size of the IO buffer.  See <code>ScannerBuilder.io_buffer_size</code>     for more information. batch_readahead: int, optional     The number of batches to read ahead. fragment_readahead: int, optional     The number of fragments to read ahead. scan_in_order: bool, optional, default True     Whether to read the fragments and batches in order. If false,     throughput may be higher, but batches will be returned out of order     and memory use might increase. prefilter: bool, optional, default False     Run filter before the vector search. late_materialization: bool or List[str], default None     Allows custom control over late materialization.  See     <code>ScannerBuilder.late_materialization</code> for more information. use_scalar_index: bool, default True     Allows custom control over scalar index usage.  See     <code>ScannerBuilder.use_scalar_index</code> for more information. with_row_id: bool, optional, default False     Return row ID. with_row_address: bool, optional, default False     Return row address use_stats: bool, optional, default True     Use stats pushdown during filters. fast_search: bool, optional, default False full_text_query: str or dict, optional     query string to search for, the results will be ranked by BM25.     e.g. \"hello world\", would match documents contains \"hello\" or \"world\".     or a dictionary with the following keys:</p> <pre><code>- columns: list[str]\n    The columns to search,\n    currently only supports a single column in the columns list.\n- query: str\n    The query string to search for.\n</code></pre> <p>include_deleted_rows: bool, optional, default False     If True, then rows that have been deleted, but are still present in the     fragment, will be returned.  These rows will have the _rowid column set     to null.  All other columns will reflect the value stored on disk and may     not be null.</p> <pre><code>Note: if this is a search operation, or a take operation (including scalar\nindexed scans) then deleted rows cannot be returned.\n</code></pre>"},{"location":"api/python/#lance.dataset.LanceDataset.to_table--notes","title":"Notes","text":"<p>If BOTH filter and nearest is specified, then:</p> <ol> <li>nearest is executed first.</li> <li>The results are filtered afterward, unless pre-filter sets to True.</li> </ol> Source code in <code>lance/dataset.py</code> <pre><code>def to_table(\n    self,\n    columns: Optional[Union[List[str], Dict[str, str]]] = None,\n    filter: Optional[Union[str, pa.compute.Expression]] = None,\n    limit: Optional[int] = None,\n    offset: Optional[int] = None,\n    nearest: Optional[dict] = None,\n    batch_size: Optional[int] = None,\n    batch_readahead: Optional[int] = None,\n    fragment_readahead: Optional[int] = None,\n    scan_in_order: Optional[bool] = None,\n    *,\n    prefilter: Optional[bool] = None,\n    with_row_id: Optional[bool] = None,\n    with_row_address: Optional[bool] = None,\n    use_stats: Optional[bool] = None,\n    fast_search: Optional[bool] = None,\n    full_text_query: Optional[Union[str, dict, FullTextQuery]] = None,\n    io_buffer_size: Optional[int] = None,\n    late_materialization: Optional[bool | List[str]] = None,\n    use_scalar_index: Optional[bool] = None,\n    include_deleted_rows: Optional[bool] = None,\n) -&gt; pa.Table:\n    \"\"\"Read the data into memory as a :py:class:`pyarrow.Table`\n\n    Parameters\n    ----------\n    columns: list of str, or dict of str to str default None\n        List of column names to be fetched.\n        Or a dictionary of column names to SQL expressions.\n        All columns are fetched if None or unspecified.\n    filter : pa.compute.Expression or str\n        Expression or str that is a valid SQL where clause. See\n        `Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;`_\n        for valid SQL expressions.\n    limit: int, default None\n        Fetch up to this many rows. All rows if None or unspecified.\n    offset: int, default None\n        Fetch starting with this row. 0 if None or unspecified.\n    nearest: dict, default None\n        Get the rows corresponding to the K most similar vectors. Example:\n\n        .. code-block:: python\n\n            {\n                \"column\": &lt;embedding col name&gt;,\n                \"q\": &lt;query vector as pa.Float32Array&gt;,\n                \"k\": 10,\n                \"metric\": \"cosine\",\n                \"nprobes\": 1,\n                \"refine_factor\": 1\n            }\n\n    batch_size: int, optional\n        The number of rows to read at a time.\n    io_buffer_size: int, default None\n        The size of the IO buffer.  See ``ScannerBuilder.io_buffer_size``\n        for more information.\n    batch_readahead: int, optional\n        The number of batches to read ahead.\n    fragment_readahead: int, optional\n        The number of fragments to read ahead.\n    scan_in_order: bool, optional, default True\n        Whether to read the fragments and batches in order. If false,\n        throughput may be higher, but batches will be returned out of order\n        and memory use might increase.\n    prefilter: bool, optional, default False\n        Run filter before the vector search.\n    late_materialization: bool or List[str], default None\n        Allows custom control over late materialization.  See\n        ``ScannerBuilder.late_materialization`` for more information.\n    use_scalar_index: bool, default True\n        Allows custom control over scalar index usage.  See\n        ``ScannerBuilder.use_scalar_index`` for more information.\n    with_row_id: bool, optional, default False\n        Return row ID.\n    with_row_address: bool, optional, default False\n        Return row address\n    use_stats: bool, optional, default True\n        Use stats pushdown during filters.\n    fast_search: bool, optional, default False\n    full_text_query: str or dict, optional\n        query string to search for, the results will be ranked by BM25.\n        e.g. \"hello world\", would match documents contains \"hello\" or \"world\".\n        or a dictionary with the following keys:\n\n        - columns: list[str]\n            The columns to search,\n            currently only supports a single column in the columns list.\n        - query: str\n            The query string to search for.\n    include_deleted_rows: bool, optional, default False\n        If True, then rows that have been deleted, but are still present in the\n        fragment, will be returned.  These rows will have the _rowid column set\n        to null.  All other columns will reflect the value stored on disk and may\n        not be null.\n\n        Note: if this is a search operation, or a take operation (including scalar\n        indexed scans) then deleted rows cannot be returned.\n\n    Notes\n    -----\n    If BOTH filter and nearest is specified, then:\n\n    1. nearest is executed first.\n    2. The results are filtered afterward, unless pre-filter sets to True.\n    \"\"\"\n    return self.scanner(\n        columns=columns,\n        filter=filter,\n        limit=limit,\n        offset=offset,\n        nearest=nearest,\n        batch_size=batch_size,\n        io_buffer_size=io_buffer_size,\n        batch_readahead=batch_readahead,\n        fragment_readahead=fragment_readahead,\n        late_materialization=late_materialization,\n        use_scalar_index=use_scalar_index,\n        scan_in_order=scan_in_order,\n        prefilter=prefilter,\n        with_row_id=with_row_id,\n        with_row_address=with_row_address,\n        use_stats=use_stats,\n        fast_search=fast_search,\n        full_text_query=full_text_query,\n        include_deleted_rows=include_deleted_rows,\n    ).to_table()\n</code></pre>"},{"location":"api/python/#random-access","title":"Random Access","text":"<p>Lance stands out with its super fast random access, unlike other columnar formats.</p>"},{"location":"api/python/#take-rows","title":"Take Rows","text":"<p>Select rows of data by index.</p>"},{"location":"api/python/#lance.dataset.LanceDataset.take--parameters","title":"Parameters","text":"<p>indices : Array or array-like     indices of rows to select in the dataset. columns: list of str, or dict of str to str default None     List of column names to be fetched.     Or a dictionary of column names to SQL expressions.     All columns are fetched if None or unspecified.</p>"},{"location":"api/python/#lance.dataset.LanceDataset.take--returns","title":"Returns","text":"<p>table : pyarrow.Table</p> Source code in <code>lance/dataset.py</code> <pre><code>def take(\n    self,\n    indices: Union[List[int], pa.Array],\n    columns: Optional[Union[List[str], Dict[str, str]]] = None,\n) -&gt; pa.Table:\n    \"\"\"Select rows of data by index.\n\n    Parameters\n    ----------\n    indices : Array or array-like\n        indices of rows to select in the dataset.\n    columns: list of str, or dict of str to str default None\n        List of column names to be fetched.\n        Or a dictionary of column names to SQL expressions.\n        All columns are fetched if None or unspecified.\n\n    Returns\n    -------\n    table : pyarrow.Table\n    \"\"\"\n    columns_with_transform = None\n    if isinstance(columns, dict):\n        columns_with_transform = list(columns.items())\n        columns = None\n    return pa.Table.from_batches(\n        [self._ds.take(indices, columns, columns_with_transform)]\n    )\n</code></pre>"},{"location":"api/python/#take-blobs","title":"Take Blobs","text":"<p>Select blobs by row IDs.</p> <p>Instead of loading large binary blob data into memory before processing it, this API allows you to open binary blob data as a regular Python file-like object. For more details, see class:<code>lance.BlobFile</code>.</p> <p>Exactly one of ids, addresses, or indices must be specified. Parameters</p> <p>blob_column : str     The name of the blob column to select. ids : Integer Array or array-like     row IDs to select in the dataset. addresses: Integer Array or array-like     The (unstable) row addresses to select in the dataset. indices : Integer Array or array-like     The offset / indices of the row in the dataset.</p>"},{"location":"api/python/#lance.dataset.LanceDataset.take_blobs--returns","title":"Returns","text":"<p>blob_files : List[BlobFile]</p> Source code in <code>lance/dataset.py</code> <pre><code>def take_blobs(\n    self,\n    blob_column: str,\n    ids: Optional[Union[List[int], pa.Array]] = None,\n    addresses: Optional[Union[List[int], pa.Array]] = None,\n    indices: Optional[Union[List[int], pa.Array]] = None,\n) -&gt; List[BlobFile]:\n    \"\"\"\n    Select blobs by row IDs.\n\n    Instead of loading large binary blob data into memory before processing it,\n    this API allows you to open binary blob data as a regular Python file-like\n    object. For more details, see :py:class:`lance.BlobFile`.\n\n    Exactly one of ids, addresses, or indices must be specified.\n    Parameters\n    ----------\n    blob_column : str\n        The name of the blob column to select.\n    ids : Integer Array or array-like\n        row IDs to select in the dataset.\n    addresses: Integer Array or array-like\n        The (unstable) row addresses to select in the dataset.\n    indices : Integer Array or array-like\n        The offset / indices of the row in the dataset.\n\n    Returns\n    -------\n    blob_files : List[BlobFile]\n    \"\"\"\n    if sum([bool(v is not None) for v in [ids, addresses, indices]]) != 1:\n        raise ValueError(\n            \"Exactly one of ids, indices, or addresses must be specified\"\n        )\n\n    if ids is not None:\n        lance_blob_files = self._ds.take_blobs(ids, blob_column)\n    elif addresses is not None:\n        # ROW ids and Row address are the same until stable ROW ID is implemented.\n        lance_blob_files = self._ds.take_blobs(addresses, blob_column)\n    elif indices is not None:\n        lance_blob_files = self._ds.take_blobs_by_indices(indices, blob_column)\n    else:\n        raise ValueError(\"Either ids or indices must be specified\")\n    return [BlobFile(lance_blob_file) for lance_blob_file in lance_blob_files]\n</code></pre>"},{"location":"api/python/#schema-evolution","title":"Schema Evolution","text":"<p>Lance supports schema evolution, which means that you can add new columns to the dataset cheaply.</p>"},{"location":"api/python/#add-columns","title":"Add Columns","text":"<p>Add new columns with defined values.</p> <p>There are several ways to specify the new columns. First, you can provide SQL expressions for each new column. Second you can provide a UDF that takes a batch of existing data and returns a new batch with the new columns. These new columns will be appended to the dataset.</p> <p>You can also provide a RecordBatchReader which will read the new column values from some external source.  This is often useful when the new column values have already been staged to files (often by some distributed process)</p> <p>See the :func:<code>lance.add_columns_udf</code> decorator for more information on writing UDFs.</p>"},{"location":"api/python/#lance.dataset.LanceDataset.add_columns--parameters","title":"Parameters","text":"<p>transforms : dict or AddColumnsUDF or ReaderLike     If this is a dictionary, then the keys are the names of the new     columns and the values are SQL expression strings. These strings can     reference existing columns in the dataset.     If this is a AddColumnsUDF, then it is a UDF that takes a batch of     existing data and returns a new batch with the new columns.     If this is :class:<code>pyarrow.Field</code> or :class:<code>pyarrow.Schema</code>, it adds     all NULL columns with the given schema, in a metadata-only operation. read_columns : list of str, optional     The names of the columns that the UDF will read. If None, then the     UDF will read all columns. This is only used when transforms is a     UDF. Otherwise, the read columns are inferred from the SQL expressions. reader_schema: pa.Schema, optional     Only valid if transforms is a <code>ReaderLike</code> object.  This will be used to     determine the schema of the reader. batch_size: int, optional     The number of rows to read at a time from the source dataset when applying     the transform.  This is ignored if the dataset is a v1 dataset.</p>"},{"location":"api/python/#lance.dataset.LanceDataset.add_columns--examples","title":"Examples","text":"<p>import lance import pyarrow as pa table = pa.table({\"a\": [1, 2, 3]}) dataset = lance.write_dataset(table, \"my_dataset\") @lance.batch_udf() ... def double_a(batch): ...     df = batch.to_pandas() ...     return pd.DataFrame({'double_a': 2 * df['a']}) dataset.add_columns(double_a) dataset.to_table().to_pandas()    a  double_a 0  1         2 1  2         4 2  3         6 dataset.add_columns({\"triple_a\": \"a * 3\"}) dataset.to_table().to_pandas()    a  double_a  triple_a 0  1         2         3 1  2         4         6 2  3         6         9</p>"},{"location":"api/python/#lance.dataset.LanceDataset.add_columns--see-also","title":"See Also","text":"<p>LanceDataset.merge :     Merge a pre-computed set of columns into the dataset.</p> Source code in <code>lance/dataset.py</code> <pre><code>def add_columns(\n    self,\n    transforms: Dict[str, str]\n    | BatchUDF\n    | ReaderLike\n    | pyarrow.Field\n    | List[pyarrow.Field]\n    | pyarrow.Schema,\n    read_columns: List[str] | None = None,\n    reader_schema: Optional[pa.Schema] = None,\n    batch_size: Optional[int] = None,\n):\n    \"\"\"\n    Add new columns with defined values.\n\n    There are several ways to specify the new columns. First, you can provide\n    SQL expressions for each new column. Second you can provide a UDF that\n    takes a batch of existing data and returns a new batch with the new\n    columns. These new columns will be appended to the dataset.\n\n    You can also provide a RecordBatchReader which will read the new column\n    values from some external source.  This is often useful when the new column\n    values have already been staged to files (often by some distributed process)\n\n    See the :func:`lance.add_columns_udf` decorator for more information on\n    writing UDFs.\n\n    Parameters\n    ----------\n    transforms : dict or AddColumnsUDF or ReaderLike\n        If this is a dictionary, then the keys are the names of the new\n        columns and the values are SQL expression strings. These strings can\n        reference existing columns in the dataset.\n        If this is a AddColumnsUDF, then it is a UDF that takes a batch of\n        existing data and returns a new batch with the new columns.\n        If this is :class:`pyarrow.Field` or :class:`pyarrow.Schema`, it adds\n        all NULL columns with the given schema, in a metadata-only operation.\n    read_columns : list of str, optional\n        The names of the columns that the UDF will read. If None, then the\n        UDF will read all columns. This is only used when transforms is a\n        UDF. Otherwise, the read columns are inferred from the SQL expressions.\n    reader_schema: pa.Schema, optional\n        Only valid if transforms is a `ReaderLike` object.  This will be used to\n        determine the schema of the reader.\n    batch_size: int, optional\n        The number of rows to read at a time from the source dataset when applying\n        the transform.  This is ignored if the dataset is a v1 dataset.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import lance\n    &gt;&gt;&gt; import pyarrow as pa\n    &gt;&gt;&gt; table = pa.table({\"a\": [1, 2, 3]})\n    &gt;&gt;&gt; dataset = lance.write_dataset(table, \"my_dataset\")\n    &gt;&gt;&gt; @lance.batch_udf()\n    ... def double_a(batch):\n    ...     df = batch.to_pandas()\n    ...     return pd.DataFrame({'double_a': 2 * df['a']})\n    &gt;&gt;&gt; dataset.add_columns(double_a)\n    &gt;&gt;&gt; dataset.to_table().to_pandas()\n       a  double_a\n    0  1         2\n    1  2         4\n    2  3         6\n    &gt;&gt;&gt; dataset.add_columns({\"triple_a\": \"a * 3\"})\n    &gt;&gt;&gt; dataset.to_table().to_pandas()\n       a  double_a  triple_a\n    0  1         2         3\n    1  2         4         6\n    2  3         6         9\n\n    See Also\n    --------\n    LanceDataset.merge :\n        Merge a pre-computed set of columns into the dataset.\n    \"\"\"\n    if isinstance(transforms, pa.Field):\n        transforms = [transforms]\n    if (\n        isinstance(transforms, list)\n        and len(transforms) &gt; 0\n        and isinstance(transforms[0], pa.Field)\n    ):\n        transforms = pa.schema(transforms)\n    if isinstance(transforms, pa.Schema):\n        self._ds.add_columns_with_schema(transforms)\n        return\n\n    transforms = normalize_transform(transforms, self, read_columns, reader_schema)\n    if isinstance(transforms, pa.RecordBatchReader):\n        self._ds.add_columns_from_reader(transforms, batch_size)\n        return\n    else:\n        self._ds.add_columns(transforms, read_columns, batch_size)\n\n        if isinstance(transforms, BatchUDF):\n            if transforms.cache is not None:\n                transforms.cache.cleanup()\n</code></pre>"},{"location":"api/python/#drop-columns","title":"Drop Columns","text":"<p>Drop one or more columns from the dataset</p> <p>This is a metadata-only operation and does not remove the data from the underlying storage. In order to remove the data, you must subsequently call <code>compact_files</code> to rewrite the data without the removed columns and then call <code>cleanup_old_versions</code> to remove the old files.</p>"},{"location":"api/python/#lance.dataset.LanceDataset.drop_columns--parameters","title":"Parameters","text":"<p>columns : list of str     The names of the columns to drop. These can be nested column references     (e.g. \"a.b.c\") or top-level column names (e.g. \"a\").</p>"},{"location":"api/python/#lance.dataset.LanceDataset.drop_columns--examples","title":"Examples","text":"<p>import lance import pyarrow as pa table = pa.table({\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"]}) dataset = lance.write_dataset(table, \"example\") dataset.drop_columns([\"a\"]) dataset.to_table().to_pandas()    b 0  a 1  b 2  c</p> Source code in <code>lance/dataset.py</code> <pre><code>def drop_columns(self, columns: List[str]):\n    \"\"\"Drop one or more columns from the dataset\n\n    This is a metadata-only operation and does not remove the data from the\n    underlying storage. In order to remove the data, you must subsequently\n    call ``compact_files`` to rewrite the data without the removed columns and\n    then call ``cleanup_old_versions`` to remove the old files.\n\n    Parameters\n    ----------\n    columns : list of str\n        The names of the columns to drop. These can be nested column references\n        (e.g. \"a.b.c\") or top-level column names (e.g. \"a\").\n\n    Examples\n    --------\n    &gt;&gt;&gt; import lance\n    &gt;&gt;&gt; import pyarrow as pa\n    &gt;&gt;&gt; table = pa.table({\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"]})\n    &gt;&gt;&gt; dataset = lance.write_dataset(table, \"example\")\n    &gt;&gt;&gt; dataset.drop_columns([\"a\"])\n    &gt;&gt;&gt; dataset.to_table().to_pandas()\n       b\n    0  a\n    1  b\n    2  c\n    \"\"\"\n    self._ds.drop_columns(columns)\n    # Indices might have changed\n    self._list_indices_res = None\n</code></pre>"},{"location":"api/python/#indexing-and-searching","title":"Indexing and Searching","text":""},{"location":"api/python/#create-vector-index","title":"Create Vector Index","text":"<p>Create index on column.</p> <p>Experimental API</p>"},{"location":"api/python/#lance.dataset.LanceDataset.create_index--parameters","title":"Parameters","text":"<p>column : str     The column to be indexed. index_type : str     The type of the index.     <code>\"IVF_PQ, IVF_HNSW_PQ and IVF_HNSW_SQ\"</code> are supported now. name : str, optional     The index name. If not provided, it will be generated from the     column name. metric : str     The distance metric type, i.e., \"L2\" (alias to \"euclidean\"), \"cosine\"     or \"dot\" (dot product). Default is \"L2\". replace : bool     Replace the existing index if it exists. num_partitions : int, optional     The number of partitions of IVF (Inverted File Index). ivf_centroids : optional     It can be either class:<code>np.ndarray</code>,     class:<code>pyarrow.FixedSizeListArray</code> or     class:<code>pyarrow.FixedShapeTensorArray</code>.     A <code>num_partitions x dimension</code> array of existing K-mean centroids     for IVF clustering. If not provided, a new KMeans model will be trained. pq_codebook : optional,     It can be class:<code>np.ndarray</code>, class:<code>pyarrow.FixedSizeListArray</code>,     or class:<code>pyarrow.FixedShapeTensorArray</code>.     A <code>num_sub_vectors x (2 ^ nbits * dimensions // num_sub_vectors)</code>     array of K-mean centroids for PQ codebook.</p> <pre><code>Note: ``nbits`` is always 8 for now.\nIf not provided, a new PQ model will be trained.\n</code></pre> <p>num_sub_vectors : int, optional     The number of sub-vectors for PQ (Product Quantization). accelerator : str or <code>torch.Device</code>, optional     If set, use an accelerator to speed up the training process.     Accepted accelerator: \"cuda\" (Nvidia GPU) and \"mps\" (Apple Silicon GPU).     If not set, use the CPU. index_cache_size : int, optional     The size of the index cache in number of entries. Default value is 256. shuffle_partition_batches : int, optional     The number of batches, using the row group size of the dataset, to include     in each shuffle partition. Default value is 10240.</p> <pre><code>Assuming the row group size is 1024, each shuffle partition will hold\n10240 * 1024 = 10,485,760 rows. By making this value smaller, this shuffle\nwill consume less memory but will take longer to complete, and vice versa.\n</code></pre> <p>shuffle_partition_concurrency : int, optional     The number of shuffle partitions to process concurrently. Default value is 2</p> <pre><code>By making this value smaller, this shuffle will consume less memory but will\ntake longer to complete, and vice versa.\n</code></pre> <p>storage_options : optional, dict     Extra options that make sense for a particular storage connection. This is     used to store connection parameters like credentials, endpoint, etc. filter_nan: bool     Defaults to True. False is UNSAFE, and will cause a crash if any null/nan     values are present (and otherwise will not). Disables the null filter used     for nullable columns. Obtains a small speed boost. one_pass_ivfpq: bool     Defaults to False. If enabled, index type must be \"IVF_PQ\". Reduces disk IO. kwargs :     Parameters passed to the index building process.</p> <p>The SQ (Scalar Quantization) is available for only <code>IVF_HNSW_SQ</code> index type, this quantization method is used to reduce the memory usage of the index, it maps the float vectors to integer vectors, each integer is of <code>num_bits</code>, now only 8 bits are supported.</p> <p>If <code>index_type</code> is \"IVF_*\", then the following parameters are required:     num_partitions</p> <p>If <code>index_type</code> is with \"PQ\", then the following parameters are required:     num_sub_vectors</p> <p>Optional parameters for <code>IVF_PQ</code>:</p> <pre><code>- ivf_centroids\n    Existing K-mean centroids for IVF clustering.\n- num_bits\n    The number of bits for PQ (Product Quantization). Default is 8.\n    Only 4, 8 are supported.\n</code></pre> <p>Optional parameters for <code>IVF_HNSW_*</code>:     max_level         Int, the maximum number of levels in the graph.     m         Int, the number of edges per node in the graph.     ef_construction         Int, the number of nodes to examine during the construction.</p>"},{"location":"api/python/#lance.dataset.LanceDataset.create_index--examples","title":"Examples","text":"<p>.. code-block:: python</p> <pre><code>import lance\n\ndataset = lance.dataset(\"/tmp/sift.lance\")\ndataset.create_index(\n    \"vector\",\n    \"IVF_PQ\",\n    num_partitions=256,\n    num_sub_vectors=16\n)\n</code></pre> <p>.. code-block:: python</p> <pre><code>import lance\n\ndataset = lance.dataset(\"/tmp/sift.lance\")\ndataset.create_index(\n    \"vector\",\n    \"IVF_HNSW_SQ\",\n    num_partitions=256,\n)\n</code></pre> <p>Experimental Accelerator (GPU) support:</p> <ul> <li>accelerate: use GPU to train IVF partitions.     Only supports CUDA (Nvidia) or MPS (Apple) currently.     Requires PyTorch being installed.</li> </ul> <p>.. code-block:: python</p> <pre><code>import lance\n\ndataset = lance.dataset(\"/tmp/sift.lance\")\ndataset.create_index(\n    \"vector\",\n    \"IVF_PQ\",\n    num_partitions=256,\n    num_sub_vectors=16,\n    accelerator=\"cuda\"\n)\n</code></pre>"},{"location":"api/python/#lance.dataset.LanceDataset.create_index--references","title":"References","text":"<ul> <li><code>Faiss Index &lt;https://github.com/facebookresearch/faiss/wiki/Faiss-indexes&gt;</code>_</li> <li>IVF introduced in <code>Video Google: a text retrieval approach to object matching   in videos &lt;https://ieeexplore.ieee.org/abstract/document/1238663&gt;</code>_</li> <li><code>Product quantization for nearest neighbor search   &lt;https://hal.inria.fr/inria-00514462v2/document&gt;</code>_</li> </ul> Source code in <code>lance/dataset.py</code> <pre><code>def create_index(\n    self,\n    column: Union[str, List[str]],\n    index_type: str,\n    name: Optional[str] = None,\n    metric: str = \"L2\",\n    replace: bool = False,\n    num_partitions: Optional[int] = None,\n    ivf_centroids: Optional[\n        Union[np.ndarray, pa.FixedSizeListArray, pa.FixedShapeTensorArray]\n    ] = None,\n    pq_codebook: Optional[\n        Union[np.ndarray, pa.FixedSizeListArray, pa.FixedShapeTensorArray]\n    ] = None,\n    num_sub_vectors: Optional[int] = None,\n    accelerator: Optional[Union[str, \"torch.Device\"]] = None,\n    index_cache_size: Optional[int] = None,\n    shuffle_partition_batches: Optional[int] = None,\n    shuffle_partition_concurrency: Optional[int] = None,\n    # experimental parameters\n    ivf_centroids_file: Optional[str] = None,\n    precomputed_partition_dataset: Optional[str] = None,\n    storage_options: Optional[Dict[str, str]] = None,\n    filter_nan: bool = True,\n    one_pass_ivfpq: bool = False,\n    **kwargs,\n) -&gt; LanceDataset:\n    \"\"\"Create index on column.\n\n    **Experimental API**\n\n    Parameters\n    ----------\n    column : str\n        The column to be indexed.\n    index_type : str\n        The type of the index.\n        ``\"IVF_PQ, IVF_HNSW_PQ and IVF_HNSW_SQ\"`` are supported now.\n    name : str, optional\n        The index name. If not provided, it will be generated from the\n        column name.\n    metric : str\n        The distance metric type, i.e., \"L2\" (alias to \"euclidean\"), \"cosine\"\n        or \"dot\" (dot product). Default is \"L2\".\n    replace : bool\n        Replace the existing index if it exists.\n    num_partitions : int, optional\n        The number of partitions of IVF (Inverted File Index).\n    ivf_centroids : optional\n        It can be either :py:class:`np.ndarray`,\n        :py:class:`pyarrow.FixedSizeListArray` or\n        :py:class:`pyarrow.FixedShapeTensorArray`.\n        A ``num_partitions x dimension`` array of existing K-mean centroids\n        for IVF clustering. If not provided, a new KMeans model will be trained.\n    pq_codebook : optional,\n        It can be :py:class:`np.ndarray`, :py:class:`pyarrow.FixedSizeListArray`,\n        or :py:class:`pyarrow.FixedShapeTensorArray`.\n        A ``num_sub_vectors x (2 ^ nbits * dimensions // num_sub_vectors)``\n        array of K-mean centroids for PQ codebook.\n\n        Note: ``nbits`` is always 8 for now.\n        If not provided, a new PQ model will be trained.\n    num_sub_vectors : int, optional\n        The number of sub-vectors for PQ (Product Quantization).\n    accelerator : str or ``torch.Device``, optional\n        If set, use an accelerator to speed up the training process.\n        Accepted accelerator: \"cuda\" (Nvidia GPU) and \"mps\" (Apple Silicon GPU).\n        If not set, use the CPU.\n    index_cache_size : int, optional\n        The size of the index cache in number of entries. Default value is 256.\n    shuffle_partition_batches : int, optional\n        The number of batches, using the row group size of the dataset, to include\n        in each shuffle partition. Default value is 10240.\n\n        Assuming the row group size is 1024, each shuffle partition will hold\n        10240 * 1024 = 10,485,760 rows. By making this value smaller, this shuffle\n        will consume less memory but will take longer to complete, and vice versa.\n    shuffle_partition_concurrency : int, optional\n        The number of shuffle partitions to process concurrently. Default value is 2\n\n        By making this value smaller, this shuffle will consume less memory but will\n        take longer to complete, and vice versa.\n    storage_options : optional, dict\n        Extra options that make sense for a particular storage connection. This is\n        used to store connection parameters like credentials, endpoint, etc.\n    filter_nan: bool\n        Defaults to True. False is UNSAFE, and will cause a crash if any null/nan\n        values are present (and otherwise will not). Disables the null filter used\n        for nullable columns. Obtains a small speed boost.\n    one_pass_ivfpq: bool\n        Defaults to False. If enabled, index type must be \"IVF_PQ\". Reduces disk IO.\n    kwargs :\n        Parameters passed to the index building process.\n\n\n\n    The SQ (Scalar Quantization) is available for only ``IVF_HNSW_SQ`` index type,\n    this quantization method is used to reduce the memory usage of the index,\n    it maps the float vectors to integer vectors, each integer is of ``num_bits``,\n    now only 8 bits are supported.\n\n    If ``index_type`` is \"IVF_*\", then the following parameters are required:\n        num_partitions\n\n    If ``index_type`` is with \"PQ\", then the following parameters are required:\n        num_sub_vectors\n\n    Optional parameters for `IVF_PQ`:\n\n        - ivf_centroids\n            Existing K-mean centroids for IVF clustering.\n        - num_bits\n            The number of bits for PQ (Product Quantization). Default is 8.\n            Only 4, 8 are supported.\n\n    Optional parameters for `IVF_HNSW_*`:\n        max_level\n            Int, the maximum number of levels in the graph.\n        m\n            Int, the number of edges per node in the graph.\n        ef_construction\n            Int, the number of nodes to examine during the construction.\n\n    Examples\n    --------\n\n    .. code-block:: python\n\n        import lance\n\n        dataset = lance.dataset(\"/tmp/sift.lance\")\n        dataset.create_index(\n            \"vector\",\n            \"IVF_PQ\",\n            num_partitions=256,\n            num_sub_vectors=16\n        )\n\n    .. code-block:: python\n\n        import lance\n\n        dataset = lance.dataset(\"/tmp/sift.lance\")\n        dataset.create_index(\n            \"vector\",\n            \"IVF_HNSW_SQ\",\n            num_partitions=256,\n        )\n\n    Experimental Accelerator (GPU) support:\n\n    - *accelerate*: use GPU to train IVF partitions.\n        Only supports CUDA (Nvidia) or MPS (Apple) currently.\n        Requires PyTorch being installed.\n\n    .. code-block:: python\n\n        import lance\n\n        dataset = lance.dataset(\"/tmp/sift.lance\")\n        dataset.create_index(\n            \"vector\",\n            \"IVF_PQ\",\n            num_partitions=256,\n            num_sub_vectors=16,\n            accelerator=\"cuda\"\n        )\n\n    References\n    ----------\n    * `Faiss Index &lt;https://github.com/facebookresearch/faiss/wiki/Faiss-indexes&gt;`_\n    * IVF introduced in `Video Google: a text retrieval approach to object matching\n      in videos &lt;https://ieeexplore.ieee.org/abstract/document/1238663&gt;`_\n    * `Product quantization for nearest neighbor search\n      &lt;https://hal.inria.fr/inria-00514462v2/document&gt;`_\n\n    \"\"\"\n    # Only support building index for 1 column from the API aspect, however\n    # the internal implementation might support building multi-column index later.\n    if isinstance(column, str):\n        column = [column]\n\n    # validate args\n    for c in column:\n        if c not in self.schema.names:\n            raise KeyError(f\"{c} not found in schema\")\n        field = self.schema.field(c)\n        is_multivec = False\n        if pa.types.is_fixed_size_list(field.type):\n            dimension = field.type.list_size\n        elif pa.types.is_list(field.type) and pa.types.is_fixed_size_list(\n            field.type.value_type\n        ):\n            dimension = field.type.value_type.list_size\n            is_multivec = True\n        elif (\n            isinstance(field.type, pa.FixedShapeTensorType)\n            and len(field.type.shape) == 1\n        ):\n            dimension = field.type.shape[0]\n        else:\n            raise TypeError(\n                f\"Vector column {c} must be FixedSizeListArray \"\n                f\"1-dimensional FixedShapeTensorArray, got {field.type}\"\n            )\n\n        if num_sub_vectors is not None and dimension % num_sub_vectors != 0:\n            raise ValueError(\n                f\"dimension ({dimension}) must be divisible by num_sub_vectors\"\n                f\" ({num_sub_vectors})\"\n            )\n\n        element_type = field.type.value_type\n        if is_multivec:\n            element_type = field.type.value_type.value_type\n        if not (\n            pa.types.is_floating(element_type) or pa.types.is_uint8(element_type)\n        ):\n            raise TypeError(\n                f\"Vector column {c} must have floating value type, \"\n                f\"got {field.type.value_type}\"\n            )\n\n    if not isinstance(metric, str) or metric.lower() not in [\n        \"l2\",\n        \"cosine\",\n        \"euclidean\",\n        \"dot\",\n        \"hamming\",\n    ]:\n        raise ValueError(f\"Metric {metric} not supported.\")\n\n    kwargs[\"metric_type\"] = metric\n\n    index_type = index_type.upper()\n    valid_index_types = [\"IVF_FLAT\", \"IVF_PQ\", \"IVF_HNSW_PQ\", \"IVF_HNSW_SQ\"]\n    if index_type not in valid_index_types:\n        raise NotImplementedError(\n            f\"Only {valid_index_types} index types supported. Got {index_type}\"\n        )\n    if index_type != \"IVF_PQ\" and one_pass_ivfpq:\n        raise ValueError(\n            f'one_pass_ivfpq requires index_type=\"IVF_PQ\", got {index_type}'\n        )\n\n    # Handle timing for various parts of accelerated builds\n    timers = {}\n    if one_pass_ivfpq and accelerator is not None:\n        from .vector import (\n            one_pass_assign_ivf_pq_on_accelerator,\n            one_pass_train_ivf_pq_on_accelerator,\n        )\n\n        LOGGER.info(\"Doing one-pass ivfpq accelerated computations\")\n\n        timers[\"ivf+pq_train:start\"] = time.time()\n        (\n            ivf_centroids,\n            ivf_kmeans,\n            pq_codebook,\n            pq_kmeans_list,\n        ) = one_pass_train_ivf_pq_on_accelerator(\n            self,\n            column[0],\n            num_partitions,\n            metric,\n            accelerator,\n            num_sub_vectors=num_sub_vectors,\n            batch_size=20480,\n            filter_nan=filter_nan,\n        )\n        timers[\"ivf+pq_train:end\"] = time.time()\n        ivfpq_train_time = timers[\"ivf+pq_train:end\"] - timers[\"ivf+pq_train:start\"]\n        LOGGER.info(\"ivf+pq training time: %ss\", ivfpq_train_time)\n        timers[\"ivf+pq_assign:start\"] = time.time()\n        shuffle_output_dir, shuffle_buffers = one_pass_assign_ivf_pq_on_accelerator(\n            self,\n            column[0],\n            metric,\n            accelerator,\n            ivf_kmeans,\n            pq_kmeans_list,\n            batch_size=20480,\n            filter_nan=filter_nan,\n        )\n        timers[\"ivf+pq_assign:end\"] = time.time()\n        ivfpq_assign_time = (\n            timers[\"ivf+pq_assign:end\"] - timers[\"ivf+pq_assign:start\"]\n        )\n        LOGGER.info(\"ivf+pq transform time: %ss\", ivfpq_assign_time)\n\n        kwargs[\"precomputed_shuffle_buffers\"] = shuffle_buffers\n        kwargs[\"precomputed_shuffle_buffers_path\"] = os.path.join(\n            shuffle_output_dir, \"data\"\n        )\n    if index_type.startswith(\"IVF\"):\n        if (ivf_centroids is not None) and (ivf_centroids_file is not None):\n            raise ValueError(\n                \"ivf_centroids and ivf_centroids_file\"\n                \" cannot be provided at the same time\"\n            )\n\n        if ivf_centroids_file is not None:\n            from pyarrow.fs import FileSystem\n\n            fs, path = FileSystem.from_uri(ivf_centroids_file)\n            with fs.open_input_file(path) as f:\n                ivf_centroids = np.load(f)\n            num_partitions = ivf_centroids.shape[0]\n\n        if num_partitions is None:\n            raise ValueError(\n                \"num_partitions and num_sub_vectors are required for IVF_PQ\"\n            )\n        if isinstance(num_partitions, float):\n            warnings.warn(\"num_partitions is float, converting to int\")\n            num_partitions = int(num_partitions)\n        elif not isinstance(num_partitions, int):\n            raise TypeError(\n                f\"num_partitions must be int, got {type(num_partitions)}\"\n            )\n        kwargs[\"num_partitions\"] = num_partitions\n\n        if (precomputed_partition_dataset is not None) and (ivf_centroids is None):\n            raise ValueError(\n                \"ivf_centroids must be provided when\"\n                \" precomputed_partition_dataset is provided\"\n            )\n        if precomputed_partition_dataset is not None:\n            LOGGER.info(\"Using provided precomputed partition dataset\")\n            precomputed_ds = LanceDataset(\n                precomputed_partition_dataset, storage_options=storage_options\n            )\n            if not (\n                \"PQ\" in index_type\n                and pq_codebook is None\n                and accelerator is not None\n                and \"precomputed_partitions_file\" in kwargs\n            ):\n                # In this case, the precomputed partitions file would be used\n                # without being turned into a set of precomputed buffers, so it\n                # needs to have a very specific format\n                if len(precomputed_ds.get_fragments()) != 1:\n                    raise ValueError(\n                        \"precomputed_partition_dataset must have only one fragment\"\n                    )\n                files = precomputed_ds.get_fragments()[0].data_files()\n                if len(files) != 1:\n                    raise ValueError(\n                        \"precomputed_partition_dataset must have only one files\"\n                    )\n            kwargs[\"precomputed_partitions_file\"] = precomputed_partition_dataset\n\n        if accelerator is not None and ivf_centroids is None and not one_pass_ivfpq:\n            LOGGER.info(\"Computing new precomputed partition dataset\")\n            # Use accelerator to train ivf centroids\n            from .vector import (\n                compute_partitions,\n                train_ivf_centroids_on_accelerator,\n            )\n\n            timers[\"ivf_train:start\"] = time.time()\n            ivf_centroids, kmeans = train_ivf_centroids_on_accelerator(\n                self,\n                column[0],\n                num_partitions,\n                metric,\n                accelerator,\n                filter_nan=filter_nan,\n            )\n            timers[\"ivf_train:end\"] = time.time()\n            ivf_train_time = timers[\"ivf_train:end\"] - timers[\"ivf_train:start\"]\n            LOGGER.info(\"ivf training time: %ss\", ivf_train_time)\n            timers[\"ivf_assign:start\"] = time.time()\n            num_sub_vectors_cur = None\n            if \"PQ\" in index_type and pq_codebook is None:\n                # compute residual subspace columns in the same pass\n                num_sub_vectors_cur = num_sub_vectors\n            partitions_file = compute_partitions(\n                self,\n                column[0],\n                kmeans,\n                batch_size=20480,\n                num_sub_vectors=num_sub_vectors_cur,\n                filter_nan=filter_nan,\n            )\n            timers[\"ivf_assign:end\"] = time.time()\n            ivf_assign_time = timers[\"ivf_assign:end\"] - timers[\"ivf_assign:start\"]\n            LOGGER.info(\"ivf transform time: %ss\", ivf_assign_time)\n            kwargs[\"precomputed_partitions_file\"] = partitions_file\n\n        if (ivf_centroids is None) and (pq_codebook is not None):\n            raise ValueError(\n                \"ivf_centroids must be specified when pq_codebook is provided\"\n            )\n\n        if ivf_centroids is not None:\n            # User provided IVF centroids\n            if _check_for_numpy(ivf_centroids) and isinstance(\n                ivf_centroids, np.ndarray\n            ):\n                if (\n                    len(ivf_centroids.shape) != 2\n                    or ivf_centroids.shape[0] != num_partitions\n                ):\n                    raise ValueError(\n                        f\"Ivf centroids must be 2D array: (clusters, dim), \"\n                        f\"got {ivf_centroids.shape}\"\n                    )\n                if ivf_centroids.dtype not in [np.float16, np.float32, np.float64]:\n                    raise TypeError(\n                        \"IVF centroids must be floating number\"\n                        + f\"got {ivf_centroids.dtype}\"\n                    )\n                dim = ivf_centroids.shape[1]\n                values = pa.array(ivf_centroids.reshape(-1))\n                ivf_centroids = pa.FixedSizeListArray.from_arrays(values, dim)\n            # Convert it to RecordBatch because Rust side only accepts RecordBatch.\n            ivf_centroids_batch = pa.RecordBatch.from_arrays(\n                [ivf_centroids], [\"_ivf_centroids\"]\n            )\n            kwargs[\"ivf_centroids\"] = ivf_centroids_batch\n\n    if \"PQ\" in index_type:\n        if num_sub_vectors is None:\n            raise ValueError(\n                \"num_partitions and num_sub_vectors are required for IVF_PQ\"\n            )\n        kwargs[\"num_sub_vectors\"] = num_sub_vectors\n\n        if (\n            pq_codebook is None\n            and accelerator is not None\n            and \"precomputed_partitions_file\" in kwargs\n            and not one_pass_ivfpq\n        ):\n            LOGGER.info(\"Computing new precomputed shuffle buffers for PQ.\")\n            partitions_file = kwargs[\"precomputed_partitions_file\"]\n            del kwargs[\"precomputed_partitions_file\"]\n\n            partitions_ds = LanceDataset(partitions_file)\n            # Use accelerator to train pq codebook\n            from .vector import (\n                compute_pq_codes,\n                train_pq_codebook_on_accelerator,\n            )\n\n            timers[\"pq_train:start\"] = time.time()\n            pq_codebook, kmeans_list = train_pq_codebook_on_accelerator(\n                partitions_ds,\n                metric,\n                accelerator=accelerator,\n                num_sub_vectors=num_sub_vectors,\n                dtype=element_type.to_pandas_dtype(),\n            )\n            timers[\"pq_train:end\"] = time.time()\n            pq_train_time = timers[\"pq_train:end\"] - timers[\"pq_train:start\"]\n            LOGGER.info(\"pq training time: %ss\", pq_train_time)\n            timers[\"pq_assign:start\"] = time.time()\n            shuffle_output_dir, shuffle_buffers = compute_pq_codes(\n                partitions_ds,\n                kmeans_list,\n                batch_size=20480,\n            )\n            timers[\"pq_assign:end\"] = time.time()\n            pq_assign_time = timers[\"pq_assign:end\"] - timers[\"pq_assign:start\"]\n            LOGGER.info(\"pq transform time: %ss\", pq_assign_time)\n            # Save disk space\n            if precomputed_partition_dataset is not None and os.path.exists(\n                partitions_file\n            ):\n                LOGGER.info(\n                    \"Temporary partitions file stored at %s,\"\n                    \"you may want to delete it.\",\n                    partitions_file,\n                )\n\n            kwargs[\"precomputed_shuffle_buffers\"] = shuffle_buffers\n            kwargs[\"precomputed_shuffle_buffers_path\"] = os.path.join(\n                shuffle_output_dir, \"data\"\n            )\n\n        if pq_codebook is not None:\n            # User provided IVF centroids\n            if _check_for_numpy(pq_codebook) and isinstance(\n                pq_codebook, np.ndarray\n            ):\n                if (\n                    len(pq_codebook.shape) != 3\n                    or pq_codebook.shape[0] != num_sub_vectors\n                    or pq_codebook.shape[1] != 256\n                ):\n                    raise ValueError(\n                        f\"PQ codebook must be 3D array: (sub_vectors, 256, dim), \"\n                        f\"got {pq_codebook.shape}\"\n                    )\n                if pq_codebook.dtype not in [np.float16, np.float32, np.float64]:\n                    raise TypeError(\n                        \"PQ codebook must be floating number\"\n                        + f\"got {pq_codebook.dtype}\"\n                    )\n                values = pa.array(pq_codebook.reshape(-1))\n                pq_codebook = pa.FixedSizeListArray.from_arrays(\n                    values, pq_codebook.shape[2]\n                )\n            pq_codebook_batch = pa.RecordBatch.from_arrays(\n                [pq_codebook], [\"_pq_codebook\"]\n            )\n            kwargs[\"pq_codebook\"] = pq_codebook_batch\n\n    if shuffle_partition_batches is not None:\n        kwargs[\"shuffle_partition_batches\"] = shuffle_partition_batches\n    if shuffle_partition_concurrency is not None:\n        kwargs[\"shuffle_partition_concurrency\"] = shuffle_partition_concurrency\n\n    timers[\"final_create_index:start\"] = time.time()\n    self._ds.create_index(\n        column, index_type, name, replace, storage_options, kwargs\n    )\n    timers[\"final_create_index:end\"] = time.time()\n    final_create_index_time = (\n        timers[\"final_create_index:end\"] - timers[\"final_create_index:start\"]\n    )\n    LOGGER.info(\"Final create_index rust time: %ss\", final_create_index_time)\n    # Save disk space\n    if \"precomputed_shuffle_buffers_path\" in kwargs.keys() and os.path.exists(\n        kwargs[\"precomputed_shuffle_buffers_path\"]\n    ):\n        LOGGER.info(\n            \"Temporary shuffle buffers stored at %s, you may want to delete it.\",\n            kwargs[\"precomputed_shuffle_buffers_path\"],\n        )\n    return self\n</code></pre>"},{"location":"api/python/#create-scalar-index","title":"Create Scalar Index","text":"<p>Create a scalar index on a column.</p> <p>Scalar indices, like vector indices, can be used to speed up scans.  A scalar index can speed up scans that contain filter expressions on the indexed column. For example, the following scan will be faster if the column <code>my_col</code> has a scalar index:</p> <p>.. code-block:: python</p> <pre><code>import lance\n\ndataset = lance.dataset(\"/tmp/images.lance\")\nmy_table = dataset.scanner(filter=\"my_col != 7\").to_table()\n</code></pre> <p>Vector search with pre-filers can also benefit from scalar indices. For example,</p> <p>.. code-block:: python</p> <pre><code>import lance\n\ndataset = lance.dataset(\"/tmp/images.lance\")\nmy_table = dataset.scanner(\n    nearest=dict(\n       column=\"vector\",\n       q=[1, 2, 3, 4],\n       k=10,\n    )\n    filter=\"my_col != 7\",\n    prefilter=True\n)\n</code></pre> <p>There are 5 types of scalar indices available today.</p> <ul> <li><code>BTREE</code>. The most common type is <code>BTREE</code>. This index is inspired   by the btree data structure although only the first few layers of the btree   are cached in memory.  It will   perform well on columns with a large number of unique values and few rows per   value.</li> <li><code>BITMAP</code>. This index stores a bitmap for each unique value in the column.   This index is useful for columns with a small number of unique values and   many rows per value.</li> <li><code>LABEL_LIST</code>. A special index that is used to index list   columns whose values have small cardinality.  For example, a column that   contains lists of tags (e.g. <code>[\"tag1\", \"tag2\", \"tag3\"]</code>) can be indexed   with a <code>LABEL_LIST</code> index.  This index can only speedup queries with   <code>array_has_any</code> or <code>array_has_all</code> filters.</li> <li><code>NGRAM</code>. A special index that is used to index string columns.  This index   creates a bitmap for each ngram in the string.  By default we use trigrams.   This index can currently speed up queries using the <code>contains</code> function   in filters.</li> <li><code>FTS/INVERTED</code>. It is used to index document columns. This index   can conduct full-text searches. For example, a column that contains any word   of query string \"hello world\". The results will be ranked by BM25.</li> </ul> <p>Note that the <code>LANCE_BYPASS_SPILLING</code> environment variable can be used to bypass spilling to disk. Setting this to true can avoid memory exhaustion issues (see https://github.com/apache/datafusion/issues/10073 for more info).</p> <p>Experimental API</p>"},{"location":"api/python/#lance.dataset.LanceDataset.create_scalar_index--parameters","title":"Parameters","text":"<p>column : str     The column to be indexed.  Must be a boolean, integer, float,     or string column. index_type : str     The type of the index.  One of <code>\"BTREE\"</code>, <code>\"BITMAP\"</code>,     <code>\"LABEL_LIST\"</code>, <code>\"NGRAM\"</code>, <code>\"FTS\"</code> or <code>\"INVERTED\"</code>. name : str, optional     The index name. If not provided, it will be generated from the     column name. replace : bool, default True     Replace the existing index if it exists.</p> bool, default True <p>This is for the <code>INVERTED</code> index. If True, the index will store the positions of the words in the document, so that you can conduct phrase query. This will significantly increase the index size. It won't impact the performance of non-phrase queries even if it is set to True.</p> <p>base_tokenizer: str, default \"simple\"     This is for the <code>INVERTED</code> index. The base tokenizer to use. The value     can be:     * \"simple\": splits tokens on whitespace and punctuation.     * \"whitespace\": splits tokens on whitespace.     * \"raw\": no tokenization. language: str, default \"English\"     This is for the <code>INVERTED</code> index. The language for stemming     and stop words. This is only used when <code>stem</code> or <code>remove_stop_words</code> is true max_token_length: Optional[int], default 40     This is for the <code>INVERTED</code> index. The maximum token length.     Any token longer than this will be removed. lower_case: bool, default True     This is for the <code>INVERTED</code> index. If True, the index will convert all     text to lowercase. stem: bool, default False     This is for the <code>INVERTED</code> index. If True, the index will stem the     tokens. remove_stop_words: bool, default False     This is for the <code>INVERTED</code> index. If True, the index will remove     stop words. ascii_folding: bool, default False     This is for the <code>INVERTED</code> index. If True, the index will convert     non-ascii characters to ascii characters if possible.     This would remove accents like \"\u00e9\" -&gt; \"e\".</p>"},{"location":"api/python/#lance.dataset.LanceDataset.create_scalar_index--examples","title":"Examples","text":"<p>.. code-block:: python</p> <pre><code>import lance\n\ndataset = lance.dataset(\"/tmp/images.lance\")\ndataset.create_index(\n    \"category\",\n    \"BTREE\",\n)\n</code></pre> <p>Scalar indices can only speed up scans for basic filters using equality, comparison, range (e.g. <code>my_col BETWEEN 0 AND 100</code>), and set membership (e.g. <code>my_col IN (0, 1, 2)</code>)</p> <p>Scalar indices can be used if the filter contains multiple indexed columns and the filter criteria are AND'd or OR'd together (e.g. <code>my_col &lt; 0 AND other_col&gt; 100</code>)</p> <p>Scalar indices may be used if the filter contains non-indexed columns but, depending on the structure of the filter, they may not be usable.  For example, if the column <code>not_indexed</code> does not have a scalar index then the filter <code>my_col = 0 OR not_indexed = 1</code> will not be able to use any scalar index on <code>my_col</code>.</p> <p>To determine if a scan is making use of a scalar index you can use <code>explain_plan</code> to look at the query plan that lance has created.  Queries that use scalar indices will either have a <code>ScalarIndexQuery</code> relation or a <code>MaterializeIndex</code> operator.</p> Source code in <code>lance/dataset.py</code> <pre><code>def create_scalar_index(\n    self,\n    column: str,\n    index_type: Union[\n        Literal[\"BTREE\"],\n        Literal[\"BITMAP\"],\n        Literal[\"LABEL_LIST\"],\n        Literal[\"INVERTED\"],\n        Literal[\"FTS\"],\n        Literal[\"NGRAM\"],\n    ],\n    name: Optional[str] = None,\n    *,\n    replace: bool = True,\n    **kwargs,\n):\n    \"\"\"Create a scalar index on a column.\n\n    Scalar indices, like vector indices, can be used to speed up scans.  A scalar\n    index can speed up scans that contain filter expressions on the indexed column.\n    For example, the following scan will be faster if the column ``my_col`` has\n    a scalar index:\n\n    .. code-block:: python\n\n        import lance\n\n        dataset = lance.dataset(\"/tmp/images.lance\")\n        my_table = dataset.scanner(filter=\"my_col != 7\").to_table()\n\n    Vector search with pre-filers can also benefit from scalar indices. For example,\n\n    .. code-block:: python\n\n        import lance\n\n        dataset = lance.dataset(\"/tmp/images.lance\")\n        my_table = dataset.scanner(\n            nearest=dict(\n               column=\"vector\",\n               q=[1, 2, 3, 4],\n               k=10,\n            )\n            filter=\"my_col != 7\",\n            prefilter=True\n        )\n\n\n    There are 5 types of scalar indices available today.\n\n    * ``BTREE``. The most common type is ``BTREE``. This index is inspired\n      by the btree data structure although only the first few layers of the btree\n      are cached in memory.  It will\n      perform well on columns with a large number of unique values and few rows per\n      value.\n    * ``BITMAP``. This index stores a bitmap for each unique value in the column.\n      This index is useful for columns with a small number of unique values and\n      many rows per value.\n    * ``LABEL_LIST``. A special index that is used to index list\n      columns whose values have small cardinality.  For example, a column that\n      contains lists of tags (e.g. ``[\"tag1\", \"tag2\", \"tag3\"]``) can be indexed\n      with a ``LABEL_LIST`` index.  This index can only speedup queries with\n      ``array_has_any`` or ``array_has_all`` filters.\n    * ``NGRAM``. A special index that is used to index string columns.  This index\n      creates a bitmap for each ngram in the string.  By default we use trigrams.\n      This index can currently speed up queries using the ``contains`` function\n      in filters.\n    * ``FTS/INVERTED``. It is used to index document columns. This index\n      can conduct full-text searches. For example, a column that contains any word\n      of query string \"hello world\". The results will be ranked by BM25.\n\n    Note that the ``LANCE_BYPASS_SPILLING`` environment variable can be used to\n    bypass spilling to disk. Setting this to true can avoid memory exhaustion\n    issues (see https://github.com/apache/datafusion/issues/10073 for more info).\n\n    **Experimental API**\n\n    Parameters\n    ----------\n    column : str\n        The column to be indexed.  Must be a boolean, integer, float,\n        or string column.\n    index_type : str\n        The type of the index.  One of ``\"BTREE\"``, ``\"BITMAP\"``,\n        ``\"LABEL_LIST\"``, ``\"NGRAM\"``, ``\"FTS\"`` or ``\"INVERTED\"``.\n    name : str, optional\n        The index name. If not provided, it will be generated from the\n        column name.\n    replace : bool, default True\n        Replace the existing index if it exists.\n\n    with_position: bool, default True\n        This is for the ``INVERTED`` index. If True, the index will store the\n        positions of the words in the document, so that you can conduct phrase\n        query. This will significantly increase the index size.\n        It won't impact the performance of non-phrase queries even if it is set to\n        True.\n    base_tokenizer: str, default \"simple\"\n        This is for the ``INVERTED`` index. The base tokenizer to use. The value\n        can be:\n        * \"simple\": splits tokens on whitespace and punctuation.\n        * \"whitespace\": splits tokens on whitespace.\n        * \"raw\": no tokenization.\n    language: str, default \"English\"\n        This is for the ``INVERTED`` index. The language for stemming\n        and stop words. This is only used when `stem` or `remove_stop_words` is true\n    max_token_length: Optional[int], default 40\n        This is for the ``INVERTED`` index. The maximum token length.\n        Any token longer than this will be removed.\n    lower_case: bool, default True\n        This is for the ``INVERTED`` index. If True, the index will convert all\n        text to lowercase.\n    stem: bool, default False\n        This is for the ``INVERTED`` index. If True, the index will stem the\n        tokens.\n    remove_stop_words: bool, default False\n        This is for the ``INVERTED`` index. If True, the index will remove\n        stop words.\n    ascii_folding: bool, default False\n        This is for the ``INVERTED`` index. If True, the index will convert\n        non-ascii characters to ascii characters if possible.\n        This would remove accents like \"\u00e9\" -&gt; \"e\".\n\n    Examples\n    --------\n\n    .. code-block:: python\n\n        import lance\n\n        dataset = lance.dataset(\"/tmp/images.lance\")\n        dataset.create_index(\n            \"category\",\n            \"BTREE\",\n        )\n\n    Scalar indices can only speed up scans for basic filters using\n    equality, comparison, range (e.g. ``my_col BETWEEN 0 AND 100``), and set\n    membership (e.g. `my_col IN (0, 1, 2)`)\n\n    Scalar indices can be used if the filter contains multiple indexed columns and\n    the filter criteria are AND'd or OR'd together\n    (e.g. ``my_col &lt; 0 AND other_col&gt; 100``)\n\n    Scalar indices may be used if the filter contains non-indexed columns but,\n    depending on the structure of the filter, they may not be usable.  For example,\n    if the column ``not_indexed`` does not have a scalar index then the filter\n    ``my_col = 0 OR not_indexed = 1`` will not be able to use any scalar index on\n    ``my_col``.\n\n    To determine if a scan is making use of a scalar index you can use\n    ``explain_plan`` to look at the query plan that lance has created.  Queries\n    that use scalar indices will either have a ``ScalarIndexQuery`` relation or a\n    ``MaterializeIndex`` operator.\n\n    \"\"\"\n    if isinstance(column, str):\n        column = [column]\n\n    if len(column) &gt; 1:\n        raise NotImplementedError(\n            \"Scalar indices currently only support a single column\"\n        )\n\n    column = column[0]\n    if column not in self.schema.names:\n        raise KeyError(f\"{column} not found in schema\")\n\n    index_type = index_type.upper()\n    if index_type not in [\"BTREE\", \"BITMAP\", \"NGRAM\", \"LABEL_LIST\", \"INVERTED\"]:\n        raise NotImplementedError(\n            (\n                'Only \"BTREE\", \"LABEL_LIST\", \"INVERTED\", \"NGRAM\", '\n                'or \"BITMAP\" are supported for '\n                f\"scalar columns.  Received {index_type}\",\n            )\n        )\n\n    field = self.schema.field(column)\n\n    field_type = field.type\n    if hasattr(field_type, \"storage_type\"):\n        field_type = field_type.storage_type\n\n    if index_type in [\"BTREE\", \"BITMAP\"]:\n        if (\n            not pa.types.is_integer(field_type)\n            and not pa.types.is_floating(field_type)\n            and not pa.types.is_boolean(field_type)\n            and not pa.types.is_string(field_type)\n            and not pa.types.is_temporal(field_type)\n            and not pa.types.is_fixed_size_binary(field_type)\n        ):\n            raise TypeError(\n                f\"BTREE/BITMAP index column {column} must be int\",\n                \", float, bool, str, fixed-size-binary, or temporal \",\n            )\n    elif index_type == \"LABEL_LIST\":\n        if not pa.types.is_list(field_type):\n            raise TypeError(f\"LABEL_LIST index column {column} must be a list\")\n    elif index_type == \"NGRAM\":\n        if not pa.types.is_string(field_type):\n            raise TypeError(f\"NGRAM index column {column} must be a string\")\n    elif index_type in [\"INVERTED\", \"FTS\"]:\n        value_type = field_type\n        if pa.types.is_list(field_type) or pa.types.is_large_list(field_type):\n            value_type = field_type.value_type\n        if not pa.types.is_string(value_type) and not pa.types.is_large_string(\n            value_type\n        ):\n            raise TypeError(\n                f\"INVERTED index column {column} must be string, large string\"\n                \" or list of strings, but got {value_type}\"\n            )\n\n    if pa.types.is_duration(field_type):\n        raise TypeError(\n            f\"Scalar index column {column} cannot currently be a duration\"\n        )\n\n    self._ds.create_index([column], index_type, name, replace, None, kwargs)\n</code></pre>"},{"location":"api/python/#drop-index","title":"Drop Index","text":"<p>Drops an index from the dataset</p> <p>Note: Indices are dropped by \"index name\".  This is not the same as the field name. If you did not specify a name when you created the index then a name was generated for you.  You can use the <code>list_indices</code> method to get the names of the indices.</p> Source code in <code>lance/dataset.py</code> <pre><code>def drop_index(self, name: str):\n    \"\"\"\n    Drops an index from the dataset\n\n    Note: Indices are dropped by \"index name\".  This is not the same as the field\n    name. If you did not specify a name when you created the index then a name was\n    generated for you.  You can use the `list_indices` method to get the names of\n    the indices.\n    \"\"\"\n    return self._ds.drop_index(name)\n</code></pre>"},{"location":"api/python/#api-reference","title":"API Reference","text":"<p>More information can be found in the API reference.</p>"},{"location":"examples/artefact_management/","title":"Deep Learning Artefact Management using Lance","text":"<p>Along with datasets, Lance file format can also be used for saving and versioning deep learning model weights. In fact deep learning artefact management can be made more streamlined (compared to vanilla weight saving methods) using Lance file format for PyTorch model weights.</p> <p>In this example we will be demonstrating how you save, version and load a PyTorch model\\'s weights using Lance. More specifically we will be loading a pre-trained ResNet model, saving it in Lance file format, loading it back to PyTorch and verifying if the weights are still indeed the same. We will also be demonstrating how you can version your model weights in a single lance dataset thanks to our Zero-copy, automatic versioning.</p> <p>Key Idea: When you save a model\\'s weights (read: state dictionary) in PyTorch, weights are stored as key-value pairs in an <code>OrderedDict</code> with the keys representing the weight\\'s name and the value representing the corresponding weight tensor. To emulate this as closely as possible, we will be saving the weights in three columns. The first column will have the name of the weight, the second will have the weight itself but flattened in a list and the third will have the original shape of the weights so they can be reconstructed for loading into a model.</p>"},{"location":"examples/artefact_management/#imports-and-setup","title":"Imports and Setup","text":"<p>We will start by importing and loading all the necessary modules.</p> <pre><code>import os\nimport shutil\nimport lance\nimport pyarrow as pa\nimport torch\nfrom collections import OrderedDict\n</code></pre> <p>We will also define a <code>GLOBAL_SCHEMA</code> that will dictate how the weights table will look like.</p> <pre><code>GLOBAL_SCHEMA = pa.schema(\n    [\n        pa.field(\"name\", pa.string()),\n        pa.field(\"value\", pa.list_(pa.float64(), -1)),\n        pa.field(\"shape\", pa.list_(pa.int64(), -1)), # Is a list with variable shape because weights can have any number of dims\n    ]\n)\n</code></pre> <p>As we covered earlier, the weights table will have three columns - one for storing the weight name, one for storing the flattened weight value and one for storing the original weight shape for loading them back.</p>"},{"location":"examples/artefact_management/#saving-and-versioning-models","title":"Saving and Versioning Models","text":"<p>First we will focus on the model saving part. Let\\'s start by writing a utility function that will take a model\\'s state dict, goes over each weight, flatten it and then return the weight name, flattened weight and weight\\'s original shape in a pyarrow <code>RecordBatch</code>{.interpreted-text role=\"meth\"}.</p> <pre><code>def _save_model_writer(state_dict):\n    \"\"\"Yields a RecordBatch for each parameter in the model state dict\"\"\"\n    for param_name, param in state_dict.items():\n        param_shape = list(param.size())\n        param_value = param.flatten().tolist()\n        yield pa.RecordBatch.from_arrays(\n            [\n                pa.array(\n                    [param_name],\n                    pa.string(),\n                ),\n                pa.array(\n                    [param_value],\n                    pa.list_(pa.float64(), -1),\n                ),\n                pa.array(\n                    [param_shape],\n                    pa.list_(pa.int64(), -1),\n                ),\n            ],\n            [\"name\", \"value\", \"shape\"],\n        )\n</code></pre> <p>Now about versioning: Let\\'s say you trained your model on some new data but don\\'t want to overwrite your old checkpoint, you can now just save these newly trained model weights as a version in Lance weights dataset. This will allow you to load specific version of weights from one lance weight dataset instead of making separate folders for each model checkpoint to make.</p> <p>Let\\'s write a function that handles the work for saving the model, whether with versions or without them.</p> <pre><code>def save_model(state_dict: OrderedDict, file_name: str, version=False):\n    \"\"\"Saves a PyTorch model in lance file format\n\n    Args:\n        state_dict (OrderedDict): Model state dict\n        file_name (str): Lance model name\n        version (bool): Whether to save as a new version or overwrite the existing versions,\n            if the lance file already exists\n    \"\"\"\n    # Create a reader\n    reader = pa.RecordBatchReader.from_batches(\n        GLOBAL_SCHEMA, _save_model_writer(state_dict)\n    )\n\n    if os.path.exists(file_name):\n        if version:\n            # If we want versioning, we use the overwrite mode to create a new version\n            lance.write_dataset(\n                reader, file_name, schema=GLOBAL_SCHEMA, mode=\"overwrite\"\n            )\n        else:\n            # If we don't want versioning, we delete the existing file and write a new one\n            shutil.rmtree(file_name)\n            lance.write_dataset(reader, file_name, schema=GLOBAL_SCHEMA)\n    else:\n        # If the file doesn't exist, we write a new one\n        lance.write_dataset(reader, file_name, schema=GLOBAL_SCHEMA)\n</code></pre> <p>The above function will take in the model state dict, the lance saved file name and the weights version. The function will start by making a <code>RecordBatchReader</code> using the global schema and the utility function we wrote above. If the weights lance dataset already exists in the directory, we will just save it as a new version (if versioning is enabled) or delete the old file and save the weights as new. Otherwise the weights saving will be done normally.</p>"},{"location":"examples/artefact_management/#loading-models","title":"Loading Models","text":"<p>Loading weights from a Lance weight dataset into a model is just the reverse of saving them. The key part is to reshape the flattened weights back to their original shape, which is easier thanks to the shape that you saved corresponding to the weights. We will divide this into three functions for better readability.</p> <p>The first function will be the <code>_load_weight</code>{.interpreted-text role=\"meth\"} function which will take a \\\"weight\\\" retrieved from the Lance weight dataset and return the weight as a torch tensor in its original shape. The \\\"weight\\\" that we retrieve from the Lance weight dataset will be a dict with value corresponding to each column in form of a key.</p> <pre><code>def _load_weight(weight: dict) -&gt; torch.Tensor:\n    \"\"\"Converts a weight dict to a torch tensor\"\"\"\n    return torch.tensor(weight[\"value\"], dtype=torch.float64).reshape(weight[\"shape\"])\n</code></pre> <p>Optionally, you could also add an option to specify the datatype of the weights.</p> <p>The next function will be on loading all the weights from the lance weight dataset into a state dictionary, which is what PyTorch will expect when we load the weights into our model.</p> <pre><code>def _load_state_dict(file_name: str, version: int = 1, map_location=None) -&gt; OrderedDict:\n    \"\"\"Reads the model weights from lance file and returns a model state dict\n    If the model weights are too large, this function will fail with a memory error.\n\n    Args:\n        file_name (str): Lance model name\n        version (int): Version of the model to load\n        map_location (str): Device to load the model on\n\n    Returns:\n        OrderedDict: Model state dict\n    \"\"\"\n    ds = lance.dataset(file_name, version=version)\n    weights = ds.take([x for x in range(ds.count_rows())]).to_pylist()\n    state_dict = OrderedDict()\n\n    for weight in weights:\n        state_dict[weight[\"name\"]] = _load_weight(weight).to(map_location)\n\n    return state_dict\n</code></pre> <p>The <code>load_state_dict</code> function will expect a lance weight dataset file name, a version and a device where the weights will be loaded into. We essentially load all the weights from the lance weight dataset into our memory and iteratively convert them into weights using the utility function we wrote earlier and then put them on the device.</p> <p>One thing to note here is that this function will fail if the saved weights are larger than memory. For the sake of simplicity, we assume the weights to be loaded can fit in the memory and we don\\'t have to deal with any sharding.</p> <p>Finally, we will write a higher level function is the only one we will call to load the weights.</p> <pre><code>def load_model(\n    model: torch.nn.Module, file_name: str, version: int = 1, map_location=None\n):\n    \"\"\"Loads the model weights from lance file and sets them to the model\n\n    Args:\n        model (torch.nn.Module): PyTorch model\n        file_name (str): Lance model name\n        version (int): Version of the model to load\n        map_location (str): Device to load the model on\n    \"\"\"\n    state_dict = _load_state_dict(file_name, version=version, map_location=map_location)\n    model.load_state_dict(state_dict)\n</code></pre> <p>The <code>load_model</code> function will require the model, the lance weight dataset name, the version of weights to load in and the map location. This will just call the <code>_load_state_dict</code> utility to get the state dict and then load that state dict into the model.</p>"},{"location":"examples/artefact_management/#conclusion","title":"Conclusion","text":"<p>In conclusion, you only need to call the two functions: <code>save_model</code> and <code>load_model</code> to save and load the models respectively and as long as the weights can be fit in the memory and are in PyTorch, it should be fine.</p> <p>Although experimental, this approach defines a new way of doing deep learning artefact management.</p>"},{"location":"examples/clip_training/","title":"Training Multi-Modal models using a Lance dataset","text":"<p>In this example we will be training a CLIP model for natural image based search using a Lance image-text dataset. In particular, we will be using the flickr_8k Lance dataset.</p> <p>The model architecture and part of the training code are adapted from Manan Goel\\'s Implementing CLIP with PyTorch Lightning with necessary changes to for a minimal, lance-compatible training example.</p>"},{"location":"examples/clip_training/#imports-and-setup","title":"Imports and Setup","text":"<p>Along with Lance, we will be needing PyTorch and timm for our CLIP model to train.</p> <pre><code>import cv2\nimport lance\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nimport timm\nfrom transformers import AutoModel, AutoTokenizer\n\nimport itertools\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.simplefilter('ignore')\n</code></pre> <p>Now, we will define a Config class that will house all the hyper-parameters required for training.</p> <pre><code>class Config:\nimg_size = (128, 128)\nbs = 32\nhead_lr = 1e-3\nimg_enc_lr = 1e-4\ntext_enc_lr = 1e-5\nmax_len = 18\nimg_embed_dim = 2048\ntext_embed_dim = 768\nprojection_dim = 256\ntemperature = 1.0\nnum_epochs = 2\nimg_encoder_model = 'resnet50'\ntext_encoder_model = 'bert-base-cased'\n</code></pre> <p>And also two utility functions that will help us load the images and texts from the lance dataset. Remember, our Lance dataset has images, image names and all the captions for a given image. We only need the images and one of those captions. For simplicity, when loading captions, we will be choosing the one that is the longest (with the rather naive assumption that it has more information about the image).</p> <pre><code>def load_image(ds, idx):\n    # Utility function to load an image at an index and convert it from bytes format to img format\n    raw_img = ds.take([idx], columns=['image']).to_pydict()\n    raw_img = np.frombuffer(b''.join(raw_img['image']), dtype=np.uint8)\n    img = cv2.imdecode(raw_img, cv2.IMREAD_COLOR)\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n    return img\n\ndef load_caption(ds, idx):\n    # Utility function to load an image's caption. Currently we return the longest caption of all\n    captions = ds.take([idx], columns=['captions']).to_pydict()['captions'][0]\n    return max(captions, key=len)\n</code></pre> <p>Since the images are stored as bytes in the lance dataset, the <code>load_image()</code> function will load the bytes corresponding to an image and then use numpy and opencv to convert it into an image.</p>"},{"location":"examples/clip_training/#dataset-and-augmentations","title":"Dataset and Augmentations","text":"<p>Since our CLIP model will expect images of same size and tokenized captions, we will define a custom PyTorch dataset that will take the lance dataset path along with any augmentation (for the image) and return a pre-processed image and a tokenized caption (as a dictionary).</p> <pre><code>class CLIPLanceDataset(Dataset):\n    \"\"\"Custom Dataset to load images and their corresponding captions\"\"\"\n    def __init__(self, lance_path, max_len=18, tokenizer=None, transforms=None):\n        self.ds = lance.dataset(lance_path)\n        self.max_len = max_len\n        # Init a new tokenizer if not specified already\n        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-cased') if not tokenizer else tokenizer\n        self.transforms = transforms\n\n    def __len__(self):\n        return self.ds.count_rows()\n\n    def __getitem__(self, idx):\n        # Load the image and caption\n        img = load_image(self.ds, idx)\n        caption = load_caption(self.ds, idx)\n\n        # Apply transformations to the images\n        if self.transforms:\n            img = self.transforms(img)\n\n        # Tokenize the caption\n        caption = self.tokenizer(\n            caption,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n        # Flatten each component of tokenized caption otherwise they will cause size mismatch errors during training\n        caption = {k: v.flatten() for k, v in caption.items()}\n\n        return img, caption\n</code></pre> <p>Now that our custom dataset is ready, we also define some very basic augmentations for our images.</p> <pre><code>train_augments = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Resize(Config.img_size),\n        transforms.Normalize([0.5], [0.5]),\n    ]\n)\n</code></pre> <p>The transformations are very basic: resizing all the images to be of the same shape and then normalizing them to stabilize the training later on.</p>"},{"location":"examples/clip_training/#model-and-setup","title":"Model and Setup","text":"<p>Since we our training a CLIP model, we have the following: * <code>ImageEncoder</code> that uses a pre-trained vision model (<code>resnet50</code> in this case) to convert images into feature vectors. * <code>TextEncoder</code>{.interpreted-text role=\"meth\"} that uses a pre-trained language model (<code>bert-base-cased</code> in this case) to transform text captions into feature vectors. * <code>Head</code> which is a Projection module projects these feature vectors into a common embedding space.</p> <p>Going into deeper details of the CLIP model and its architectural nuances are out of the scope of this example, however if you wish to read more on it, you can read the official paper here.</p> <p>Now that we have understood the general summary of the model, let\\'s define all the required modules.</p> <pre><code>class ImageEncoder(nn.Module):\n    \"\"\"Encodes the Image\"\"\"\n    def __init__(self, model_name, pretrained = True):\n        super().__init__()\n        self.backbone = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            num_classes=0,\n            global_pool=\"avg\"\n        )\n\n        for param in self.backbone.parameters():\n            param.requires_grad = True\n\n    def forward(self, img):\n        return self.backbone(img)\n\nclass TextEncoder(nn.Module):\n    \"\"\"Encodes the Caption\"\"\"\n    def __init__(self, model_name):\n        super().__init__()\n\n        self.backbone = AutoModel.from_pretrained(model_name)\n\n        for param in self.backbone.parameters():\n            param.requires_grad = True\n\n    def forward(self, captions):\n        output = self.backbone(**captions)\n        return output.last_hidden_state[:, 0, :]\n\nclass Head(nn.Module):\n    \"\"\"Projects both into Embedding space\"\"\"\n    def __init__(self, embedding_dim, projection_dim):\n        super().__init__()\n        self.projection = nn.Linear(embedding_dim, projection_dim)\n        self.gelu = nn.GELU()\n        self.fc = nn.Linear(projection_dim, projection_dim)\n\n        self.dropout = nn.Dropout(0.3)\n        self.layer_norm = nn.LayerNorm(projection_dim)\n\n    def forward(self, x):\n        projected = self.projection(x)\n        x = self.gelu(projected)\n        x = self.fc(x)\n        x = self.dropout(x)\n        x += projected\n\n        return self.layer_norm(x)\n</code></pre> <p>Along with the model definition, we will be defining two utility functions to simplify the training: <code>forward()</code>{.interpreted-text role=\"meth\"} which will do one forward pass through the combined models and <code>loss_fn()</code> which will take the image and text embeddings output from <code>forward</code> function and then calculate the loss using them.</p> <pre><code>def loss_fn(img_embed, text_embed, temperature=0.2):\n    \"\"\"\n    https://arxiv.org/abs/2103.00020/\n    \"\"\"\n    # Calculate logits, image similarity and text similarity\n    logits = (text_embed @ img_embed.T) / temperature\n    img_sim = img_embed @ img_embed.T\n    text_sim = text_embed @ text_embed.T\n    # Calculate targets by taking the softmax of the similarities\n    targets = F.softmax(\n        (img_sim + text_sim) / 2 * temperature, dim=-1\n    )\n    img_loss = (-targets.T * nn.LogSoftmax(dim=-1)(logits.T)).sum(1)\n    text_loss = (-targets * nn.LogSoftmax(dim=-1)(logits)).sum(1)\n    return (img_loss + text_loss) / 2.0\n\ndef forward(img, caption):\n    # Transfer to device\n    img = img.to('cuda')\n    for k, v in caption.items():\n        caption[k] = v.to('cuda')\n\n    # Get embeddings for both img and caption\n    img_embed = img_head(img_encoder(img))\n    text_embed = text_head(text_encoder(caption))\n\n    return img_embed, text_embed\n</code></pre> <p>In order for us to train, we will define the models, tokenizer and the optimizer to be used in the next section</p> <pre><code># Define image encoder, image head, text encoder, text head and a tokenizer for tokenizing the caption\nimg_encoder = ImageEncoder(model_name=Config.img_encoder_model).to('cuda')\nimg_head = Head(Config.img_embed_dim, Config.projection_dim).to('cuda')\n\ntokenizer = AutoTokenizer.from_pretrained(Config.text_encoder_model)\ntext_encoder = TextEncoder(model_name=Config.text_encoder_model).to('cuda')\ntext_head = Head(Config.text_embed_dim, Config.projection_dim).to('cuda')\n\n# Since we are optimizing two different models together, we will define parameters manually\nparameters = [\n    {\"params\": img_encoder.parameters(), \"lr\": Config.img_enc_lr},\n    {\"params\": text_encoder.parameters(), \"lr\": Config.text_enc_lr},\n    {\n        \"params\": itertools.chain(\n            img_head.parameters(),\n            text_head.parameters(),\n        ),\n        \"lr\": Config.head_lr,\n    },\n]\n\noptimizer = torch.optim.Adam(parameters)\n</code></pre>"},{"location":"examples/clip_training/#training","title":"Training","text":"<p>Before we actually train the model, one last step remains: which is to initialize our Lance dataset and a dataloader.</p> <pre><code># We assume the flickr8k.lance dataset is in the same directory\ndataset = CLIPLanceDataset(\n    lance_path=\"flickr8k.lance\",\n    max_len=Config.max_len,\n    tokenizer=tokenizer,\n    transforms=train_augments\n)\n\ndataloader = DataLoader(\n    dataset,\n    shuffle=False,\n    batch_size=Config.bs,\n    pin_memory=True\n)\n</code></pre> <p>Now that our dataloader is initialized, let\\'s train the model.</p> <pre><code>img_encoder.train()\nimg_head.train()\ntext_encoder.train()\ntext_head.train()\n\nfor epoch in range(Config.num_epochs):\n    print(f\"{'='*20} Epoch: {epoch+1} / {Config.num_epochs} {'='*20}\")\n\n    prog_bar = tqdm(dataloader)\n    for img, caption in prog_bar:\n        optimizer.zero_grad(set_to_none=True)\n\n        img_embed, text_embed = forward(img, caption)\n        loss = loss_fn(img_embed, text_embed, temperature=Config.temperature).mean()\n\n        loss.backward()\n        optimizer.step()\n\n        prog_bar.set_description(f\"loss: {loss.item():.4f}\")\n    print()\n</code></pre> <p>The training loop is quite self-explanatory. We set image encoder, image head, text encoder and text head models to training mode. Then in each epoch, we iterate over our lance dataset, training the model and reporting the lance to the progress bar.</p> <pre><code>==================== Epoch: 1 / 2 ====================\nloss: 2.0799: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 253/253 [02:14&lt;00:00,  1.88it/s]\n\n==================== Epoch: 2 / 2 ====================\nloss: 1.3064: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 253/253 [02:10&lt;00:00,  1.94it/s]\n</code></pre> <p>And that\\'s basically it! Using Lance dataset for training any type of model is very similar to using any other type of dataset but it also comes with increased speed and ease of use!</p>"},{"location":"examples/examples/","title":"Examples","text":"<ul> <li>Creating text dataset for LLM training using Lance</li> <li>Training LLMs using a Lance text dataset</li> <li>Reading and writing a Lance dataset in Rust</li> <li>Creating Multi-Modal datasets using Lance</li> <li>Training Multi-Modal models using a Lance dataset</li> <li>Deep Learning Artefact Management using Lance</li> <li>Reading and writing a Lance dataset via Spark DataSource</li> </ul>"},{"location":"examples/flickr8k_dataset_creation/","title":"Creating Multi-Modal datasets using Lance","text":"<p>Thanks to Lance file format\\'s ability to store data of different modalities, one of the important use-cases that Lance shines in is storing Multi-modal datasets. In this brief example we will be going over how you can take a Multi-modal dataset and store it in Lance file format.</p> <p>The dataset of choice here is Flickr8k dataset. Flickr8k is a benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. The images were chosen from six different Flickr groups, and tend not to contain any well-known people or locations, but were manually selected to depict a variety of scenes and situations.</p> <p>We will be creating an Image-caption pair dataset for Multi-modal model training by using the above mentioned Flickr8k dataset, saving it in form of a Lance dataset with image file names, all captions for every image (order preserved) and the image itself (in binary format).</p>"},{"location":"examples/flickr8k_dataset_creation/#imports-and-setup","title":"Imports and Setup","text":"<p>We assume that you downloaded the dataset, more specifically the \\\"Flickr8k.token.txt\\\" file and the \\\"Flicker8k_Dataset/\\\" folder and both are present in the current directory. These can be downloaded from here (download both the dataset and text zip files).</p> <p>We also assume you have pyarrow and pylance installed as well as opencv (for reading in images) and tqdm (for pretty progress bars).</p> <p>Now let\\'s start with imports and defining the caption file and image dataset folder.</p> <pre><code>import os\nimport cv2\nimport random\n\nimport lance\nimport pyarrow as pa\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm.auto import tqdm\n\ncaptions = \"Flickr8k.token.txt\"\nimage_folder = \"Flicker8k_Dataset/\"\n</code></pre>"},{"location":"examples/flickr8k_dataset_creation/#loading-and-processing","title":"Loading and Processing","text":"<p>In flickr8k dataset, each image has multiple corresponding captions that are ordered. We are going to put all these captions in a list corresponding to each image with their position in the list representing the order in which they originally appear. Let\\'s load the annotations (the image path and corresponding captions) in a list with each element of the list being a tuple consisting of image name, caption number and caption itself.</p> <pre><code>with open(captions, \"r\") as fl:\n    annotations = fl.readlines()\n\n# Converts the annotations where each element of this list is a tuple consisting of image file name, caption number and caption itself\nannotations = list(map(lambda x: tuple([*x.split('\\t')[0].split('#'), x.split('\\t')[1]]), annotations))\n</code></pre> <p>Now, for all captions of the same image, we will put them in a list sorted by their ordering.</p> <pre><code>captions = []\nimage_ids = set(ann[0] for ann in annotations)\nfor img_id in tqdm(image_ids):\n    current_img_captions = []\n    for ann_img_id, num, caption in annotations:\n        if img_id == ann_img_id:\n            current_img_captions.append((num, caption))\n\n    # Sort by the annotation number\n    current_img_captions.sort(key=lambda x: x[0])\n    captions.append((img_id, tuple([x[1] for x in current_img_captions])))\n</code></pre>"},{"location":"examples/flickr8k_dataset_creation/#converting-to-a-lance-dataset","title":"Converting to a Lance Dataset","text":"<p>Now that our captions list is in a proper format, we will write a <code>process()</code> function that will take the said captions as argument and yield a Pyarrow record batch consisting of the <code>image_id</code>, <code>image</code>{.interpreted-text role=\"meth\"} and <code>captions</code>. The image in this record batch will be in binary format and all the captions for an image will be in a list with their ordering preserved.</p> <pre><code>def process(captions):\n    for img_id, img_captions in tqdm(captions):\n        try:\n            with open(os.path.join(image_folder, img_id), 'rb') as im:\n                binary_im = im.read()\n\n        except FileNotFoundError:\n            print(f\"img_id '{img_id}' not found in the folder, skipping.\")\n            continue\n\n        img_id = pa.array([img_id], type=pa.string())\n        img = pa.array([binary_im], type=pa.binary())\n        capt = pa.array([img_captions], pa.list_(pa.string(), -1))\n\n        yield pa.RecordBatch.from_arrays(\n            [img_id, img, capt], \n            [\"image_id\", \"image\", \"captions\"]\n        )\n</code></pre> <p>Let\\'s also define the same schema to tell Pyarrow the type of data it should be expecting in the table.</p> <pre><code>schema = pa.schema([\n    pa.field(\"image_id\", pa.string()),\n    pa.field(\"image\", pa.binary()),\n    pa.field(\"captions\", pa.list_(pa.string(), -1)),\n])\n</code></pre> <p>We are including the <code>image_id</code> (which is the original image name) so it can be easier to reference and debug in the future.</p> <p>Finally, we define a reader to iteratively read those record batches and then write them to a lance dataset on the disk.</p> <pre><code>reader = pa.RecordBatchReader.from_batches(schema, process(captions))\nlance.write_dataset(reader, \"flickr8k.lance\", schema)\n</code></pre> <p>And that\\'s basically it! If you want to execute this in a notebook form, you can check out this example in our deeplearning-recipes repository here.</p> <p>For more Deep learning related examples using Lance dataset, be sure to check out the lance-deeplearning-recipes repository!</p>"},{"location":"examples/llm_dataset_creation/","title":"Creating text dataset for LLM training using Lance","text":"<p>Lance can be used for creating and caching a text (or code) dataset for pre-training / fine-tuning of Large Language Models. The need for this arises when one needs to train a model on a subset of data or process the data in chunks without downloading all of it on the disk at once. This becomes a considerable problem when you just want a subset of a Terabyte or Petabyte-scale dataset.</p> <p>In this example, we will be bypassing this problem by downloading a text dataset in parts, tokenizing it and saving it as a Lance dataset. This can be done for as many or as few data samples as you wish with average memory consumption approximately 3-4 GBs!</p> <p>For this example, we are working with the wikitext dataset, which is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia.</p>"},{"location":"examples/llm_dataset_creation/#preparing-and-pre-processing-the-raw-dataset","title":"Preparing and pre-processing the raw dataset","text":"<p>Let\\'s first define the dataset and the tokenizer</p> <pre><code>import lance\nimport pyarrow as pa\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom tqdm.auto import tqdm  # optional for progress tracking\n\ntokenizer = AutoTokenizer.from_pretrained('gpt2')\n\ndataset = load_dataset('wikitext', 'wikitext-103-raw-v1', streaming=True)['train']\ndataset = dataset.shuffle(seed=1337)\n</code></pre> <p>The [streaming]{.title-ref} argument in [load_dataset]{.title-ref} is especially important because if you run it without setting it to [True]{.title-ref}, the datasets library will download the entire dataset first, even though you only wish to use a subset of it. With [streaming]{.title-ref} set to [True]{.title-ref}, the samples will be downloaded as they are needed.</p> <p>Now we will define a function to help us with tokenizing our samples, one-by-one.</p> <pre><code>def tokenize(sample, field='text'):\n    return tokenizer(sample[field])['input_ids']\n</code></pre> <p>This function will receive a sample from a huggingface dataset and tokenize the values in the [field]{.title-ref} column. This is the main text you want to tokenize.</p>"},{"location":"examples/llm_dataset_creation/#creating-a-lance-dataset","title":"Creating a Lance dataset","text":"<p>Now that we have set up our raw dataset and pre-processing code, let\\'s define the main function that takes in the dataset, number of samples and field, and returns a pyarrow batch that will later be written into a lance dataset.</p> <pre><code>def process_samples(dataset, num_samples=100_000, field='text'):\n    current_sample = 0\n    for sample in tqdm(dataset, total=num_samples):\n        # If we have added all 5M samples, stop\n        if current_sample == num_samples:\n            break\n        if not sample[field]:\n            continue\n        # Tokenize the current sample\n        tokenized_sample = tokenize(sample, field)\n        # Increement the counter\n        current_sample += 1\n        # Yield a PyArrow RecordBatch\n        yield pa.RecordBatch.from_arrays(\n            [tokenized_sample], \n            names=[\"input_ids\"]\n        )\n</code></pre> <p>This function will be iterating over the huggingface dataset, one sample at a time, tokenizing the sample and yielding a pyarrow [RecordBatch]{.title-ref} with all the tokens. We will do this until we have reached the [num_samples]{.title-ref} number of samples or the end of the dataset, whichever comes first.</p> <p>Please note that by \\'sample\\', we mean one example (row) in the original dataset. What one example exactly means will depend on the dataset itself as it could be one line or an entire file of text. In this example, it varies in length between a line and a paragraph of text.</p> <p>We also need to define a schema to tell Lance what type of data we are expecting in our table. Since our dataset consists only of tokens which are long integers, [int64]{.title-ref} is the suitable datatype.</p> <pre><code>schema = pa.schema([\n    pa.field(\"input_ids\", pa.int64())\n])\n</code></pre> <p>Finally, we need to define a [reader]{.title-ref} that will be reading a stream of record batches from our <code>process_samples</code>{.interpreted-text role=\"meth\"} function that yields said record batches consisting of individual tokenized samples.</p> <pre><code>reader = pa.RecordBatchReader.from_batches(\n    schema, \n    process_samples(dataset, num_samples=500_000, field='text') # For 500K samples\n)\n</code></pre> <p>And finally we use the <code>lance.write_dataset</code>{.interpreted-text role=\"meth\"} which will write the dataset to the disk.</p> <pre><code># Write the dataset to disk\nlance.write_dataset(\n    reader, \n    \"wikitext_500K.lance\",\n    schema\n)\n</code></pre> <p>If you want to apply some other pre-processing to the tokens before saving it to the disk (like masking, etc), you may add it in the [process_samples]{.title-ref} function.</p> <p>And that\\'s it! Your dataset has been tokenized and saved to the disk!</p>"},{"location":"examples/llm_training/","title":"Training LLMs using a Lance text dataset","text":"<p>Using a Lance text dataset for pre-training / fine-tuning a Large Language model is straightforward and memory-efficient. This example follows up on the Creating text dataset for LLM training using Lance example. Check it out if you haven\\'t already.</p> <p>In this example, we will be training an LLM using \ud83e\udd17 transformers on the tokenized \\\"wikitext_500K\\\" lance dataset we created in the aforementioned example.</p>"},{"location":"examples/llm_training/#imports-and-setup","title":"Imports and Setup","text":"<p>Let\\'s setup our environment by doing all the necessary imports and defining a few basic things.</p> <pre><code>import numpy as np\nimport lance\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, Sampler\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# We'll be training the pre-trained GPT2 model in this example\nmodel_name = 'gpt2'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Also define some hyperparameters\nlr = 3e-4\nnb_epochs = 10\nblock_size = 1024\nbatch_size = 8\ndevice = 'cuda:0'\ndataset_path = 'wikitext_500K.lance'\n</code></pre> <p>Now that the basic setup is out of the way, let\\'s define our custom Dataset and a Sampler for streaming the tokens from our Lance dataset.</p>"},{"location":"examples/llm_training/#data-loading-setup","title":"Data-loading Setup","text":"<p>We start by defining a utility function that will help us load any number of tokens from our lance dataset in a \\'chunk\\'.</p> <pre><code>def from_indices(dataset, indices):\n    \"\"\"Load the elements on given indices from the dataset\"\"\"\n    chunk = dataset.take(indices).to_pylist()\n    chunk = list(map(lambda x: x['input_ids'], chunk))\n    return chunk\n</code></pre> <p>Now let\\'s define our custom dataset and sampler for loading the tokens.</p> <pre><code>class LanceDataset(Dataset):\n    def __init__(\n        self,\n        dataset_path,\n        block_size,\n    ):\n        # Load the lance dataset from the saved path\n        self.ds = lance.dataset(dataset_path)\n        self.block_size = block_size\n\n        # Doing this so the sampler never asks for an index at the end of text\n        self.length = self.ds.count_rows() - block_size\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Generate a window of indices starting from the current idx to idx+block_size\n        and return the tokens at those indices\n        \"\"\"\n        window = np.arange(idx, idx + self.block_size)\n        sample = from_indices(self.ds, window)\n\n        return {\"input_ids\": torch.tensor(sample), \"labels\": torch.tensor(sample)}\n</code></pre> <p>When given a random index by the sampler, the dataset will load the next <code>block_size</code> number of tokens starting from current index. This would in-essence form a sample as the loaded tokens would be causal.</p> <p>However we also need to make sure that the tokens we get from the dataset aren\\'t overlapping. Let\\'s understand this from an example:</p> <p>Let\\'s say, for some arbitrary block size, during the training loop the dataset return the following tokens:</p> <p>[\\\"Vienna is the capital of Austria\\\"]{.title-ref} at index = 12 for sample #1, and,</p> <p>[\\\"is the capital of Austria and\\\"]{.title-ref} at index = 13 for sample</p>"},{"location":"examples/llm_training/#2-and-so-on","title":"2, and so on","text":"<p>The problem here is that if we allow the dataloader to fetch the \\'samples\\' for any arbitrary number of indices, they may overlap (as we see above). This is not good for the model as it may start to overfit after seeing sufficient overlapping tokens.</p> <p>To solve this problem, we define a custom Sampler that only returns the indices that are \\'block_size\\' apart from each other, ensuring that we don\\'t see any overlapping samples.</p> <pre><code>class LanceSampler(Sampler):\n    r\"\"\"Samples tokens randomly but `block_size` indices apart.\n\n    Args:\n        data_source (Dataset): dataset to sample from\n        block_size (int): minimum index distance between each random sample\n    \"\"\"\n\n    def __init__(self, data_source, block_size=512):\n        self.data_source = data_source\n        self.num_samples = len(self.data_source)\n        self.available_indices = list(range(0, self.num_samples, block_size))\n        np.random.shuffle(self.available_indices)\n\n    def __iter__(self):\n        yield from self.available_indices\n\n    def __len__(self) -&gt; int:\n        return len(self.available_indices)\n</code></pre> <p>Now when we fetch the tokens from our dataset with sampler being the <code>LanceSampler</code>, all samples in all the batches that our model sees during the training are guaranteed to be non-overlapping.</p> <p>This is done by generating a list of indices starting from 0 to the end of the dataset (which if you remember is lance dataset length - block size) with each index \\'block_size\\' apart from the other. We then shuffle this list and yield indices from it.</p> <p>And that\\'s basically it for the Dataloading! Now all we are left is to train the model!</p>"},{"location":"examples/llm_training/#model-training","title":"Model Training","text":"<p>Now you train the model just like you would with any other dataset!</p> <pre><code># Define the dataset, sampler and dataloader\ndataset = LanceDataset(dataset_path, block_size)\nsampler = LanceSampler(dataset, block_size)\ndataloader = DataLoader(\n    dataset,\n    shuffle=False,\n    batch_size=batch_size,\n    sampler=sampler,\n    pin_memory=True\n)\n\n# Define the optimizer, training loop and train the model!\nmodel = model.to(device)\nmodel.train()\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n\nfor epoch in range(nb_epochs):\n    print(f\"========= Epoch: {epoch+1} / {nb_epochs} =========\")\n    epoch_loss = []\n    prog_bar = tqdm(dataloader, total=len(dataloader))\n    for batch in prog_bar:\n        optimizer.zero_grad(set_to_none=True)\n\n        # Put both input_ids and labels to the device\n        for k, v in batch.items():\n            batch[k] = v.to(device)\n\n        # Perform one forward pass and get the loss\n        outputs = model(**batch)\n        loss = outputs.loss\n\n        # Perform backward pass\n        loss.backward()\n        optimizer.step()\n\n        prog_bar.set_description(f\"loss: {loss.item():.4f}\")\n\n        epoch_loss.append(loss.item())\n\n    # Calculate training perplexity for this epoch\n    try:\n        perplexity = np.exp(np.mean(epoch_loss))\n    except OverflowError:\n        perplexity = float(\"-inf\")\n\n    print(f\"train_perplexity: {perplexity}\")\n</code></pre> <p>One tip: If your lance dataset is huge (like the wikitext_500K is), and you want to debug the model to look out for errors, you may want to wrap the dataloader in an <code>iter()</code> function and only run it for a couple batches.</p> <p>And that\\'s basically it!</p> <p>The best part about using Lance, the custom Dataset and Sampler is that you get a whooping 95% average GPU utilisation and minimal CPU overhead thanks to the lightning fast random access that Lance provides \ud83d\ude80</p>"},{"location":"examples/spark_datasource_example/","title":"Writing and Reading a Dataset Using Spark","text":"<p>Attention</p> <p>The Spark connector is currently an experimental feature undergoing rapid iteration.</p> <p>In this example, we will read a local <code>iris.csv</code> file and write it as a Lance dataset using Apache Spark, then demonstrate how to query the dataset.</p>"},{"location":"examples/spark_datasource_example/#preparing-the-environment-and-raw-dataset","title":"Preparing the Environment and Raw Dataset","text":"<p>Download the Spark binary package from the official website. We recommend downloading Spark 3.5+ for Scala 2.12 (as the Spark connector currently only supports Scala 2.12).</p> <p>You can directly download Spark 3.5.1 using this link.</p> <p>Prepare the dataset by downloading iris.csv to your local machine.</p> <p>Create a Scala file named <code>iris_to_lance_via_spark_shell.scala</code> and open it.</p>"},{"location":"examples/spark_datasource_example/#reading-the-raw-dataset-and-writing-to-a-lance-dataset","title":"Reading the Raw Dataset and Writing to a Lance Dataset","text":"<p>Add necessary imports and create a Spark session:</p> <pre><code>import org.apache.spark.sql.types.{StructType, StructField, DoubleType, StringType}\nimport org.apache.spark.sql.{SparkSession, DataFrame}\nimport com.lancedb.lance.spark.{LanceConfig, LanceDataSource}\n\nval spark = SparkSession.builder()\n  .appName(\"Iris CSV to Lance Converter\")\n  .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceCatalog\")\n  .getOrCreate()\n</code></pre> <p>Specifying your input and output path:</p> <pre><code>val irisPath = \"/path/to/your/input/iris.csv\"\nval outputPath = \"/path/to/your/output/iris.lance\"\n</code></pre> <p>Reading the <code>iris.csv</code> via the following snippet:</p> <pre><code>val rawDF = spark.read\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .csv(irisPath)\n\nrawDF.printSchema()\n</code></pre> <p>Preparing the lance schema and write a lance dataset:</p> <pre><code>val lanceSchema = new StructType()\n  .add(StructField(\"sepal_length\", DoubleType))\n  .add(StructField(\"sepal_width\", DoubleType))\n  .add(StructField(\"petal_length\", DoubleType))\n  .add(StructField(\"petal_width\", DoubleType))\n  .add(StructField(\"species\", StringType))\n\nval lanceDF = spark.createDataFrame(rawDF.rdd, lanceSchema)\n\nlanceDF.write\n  .format(LanceDataSource.name)\n  .option(LanceConfig.CONFIG_DATASET_URI, outputPath)\n  .save()\n</code></pre>"},{"location":"examples/spark_datasource_example/#reading-a-lance-dataset","title":"Reading a Lance dataset","text":"<p>After writing the dataset, we can read it back and examine its properties:</p> <pre><code>val lanceDF = spark.read\n  .format(\"lance\")\n  .option(LanceConfig.CONFIG_DATASET_URI, outputPath)\n  .load()\n\nprintln(s\"The total count: ${lanceDF.count()}\")\nlanceDF.printSchema()\nprintln(\"\\n The top 5 data:\")\nlanceDF.show(5, truncate = false)\n\nprintln(\"\\n Species distribution statistics:\")\nlanceDF.groupBy(\"species\").count().show()\n</code></pre> <p>First, we open the dataset and count the total rows. Then we print the dataset schema. Finally, we analyze the species distribution statistics.</p>"},{"location":"examples/spark_datasource_example/#running-the-spark-application","title":"Running the Spark Application","text":"<p>To execute the application, download these dependencies:</p> <ul> <li>lance-core JAR: Core Rust Spark binding exposing Lance features to   Java (available   here)</li> <li>lance-spark JAR: Spark connector for reading/writing Lance format   (available   here)</li> <li>jar-jni JAR: Load JNI dependencies embedded within a JAR file   (available   here)</li> <li>arrow-c-data JAR: Java implementation of C Data Interface (available   here)</li> <li>arrow-dataset JAR: Java implementation of Arrow Dataset API/Framework   (available   here)</li> </ul> <p>Place these JARs in the <code>${SPARK_HOME}/jars</code> directory, then run:</p> <pre><code>./bin/spark-shell --jars ./jars/lance-core-0.23.0.jar,./jars/lance-spark-0.23.0.jar,./jars/jar-jni-1.1.1.jar,./jars/arrow-c-data-12.0.1.jar,./jars/arrow-dataset-12.0.1.jar -i ./iris_to_lance_via_spark_shell.scala\n</code></pre> <p>It should be work! Have fun!</p>"},{"location":"examples/write_read_dataset/","title":"Writing and reading a dataset using Lance","text":"<p>In this example, we will write a simple lance dataset to disk. Then we will read it and print out some basic properties like the schema and sizes for each record batch in the dataset. The example uses only one record batch, however it should work for larger datasets (multiple record batches) as well.</p>"},{"location":"examples/write_read_dataset/#writing-the-raw-dataset","title":"Writing the raw dataset","text":"<p>See the complete example in write_read_ds.rs on GitHub.</p> <p>First we define a schema for our dataset, and create a record batch from that schema. Next we iterate over the record batches (only one in this case) and write them to disk. We also define the write parameters (set to overwrite) and then write the dataset to disk.</p>"},{"location":"examples/write_read_dataset/#reading-a-lance-dataset","title":"Reading a Lance dataset","text":"<p>Now that we have written the dataset to a new directory, we can read it back and print out some basic properties.</p> <p>See the complete example in write_read_ds.rs on GitHub.</p> <p>First we open the dataset, and create a scanner object. We use it to create a [batch_stream]{.title-ref} that will let us access each record batch in the dataset. Then we iterate over the record batches and print out the size and schema of each one.</p>"},{"location":"integrations/huggingface/","title":"Lance \u2764\ufe0f HuggingFace","text":"<p>The HuggingFace Hub has become the go to place for ML practitioners to find pre-trained models and useful datasets.</p> <p>HuggingFace datasets can be written directly into Lance format by using the <code>lance.write_dataset</code> method. You can write the entire dataset or a particular split. For example:</p> <pre><code># Huggingface datasets\nimport datasets\nimport lance\n\nlance.write_dataset(datasets.load_dataset(\n    \"poloclub/diffusiondb\", split=\"train[:10]\",\n), \"diffusiondb_train.lance\")\n</code></pre>"},{"location":"integrations/pytorch/","title":"PyTorch Integration","text":"<p>Machine learning users can use <code>~lance.torch.data.LanceDataset</code>, a subclass of <code>torch.utils.data.IterableDataset</code>{.interpreted-text role=\"class\"}, that to use Lance data directly PyTorch training and inference loops.</p> <p>It starts with creating a ML dataset for training. With the <code>./huggingface</code>, it takes just one line of Python to convert a HuggingFace dataset to a Lance dataset.</p> <pre><code># Huggingface datasets\nimport datasets\nimport lance\n\nhf_ds = datasets.load_dataset(\n    \"poloclub/diffusiondb\",\n    split=\"train\",\n    # name=\"2m_first_1k\",  # for a smaller subset of the dataset\n)\nlance.write_dataset(hf_ds, \"diffusiondb_train.lance\")\n</code></pre> <p>Then, you can use the Lance dataset in PyTorch training and inference loops.</p> <p>Note:</p> <ol> <li>the PyTorch dataset automatically convert data into     <code>torch.Tensor</code></li> </ol> <p>2. lance is not fork-safe. If you are using multiprocessing, use spawn instead. The safe dataloader uses the spawning method.</p> <p>* UnSafe Dataloader .. code-block:: python</p> <p>import torch import lance.torch.data</p> <p># Load lance dataset into a PyTorch IterableDataset. # with only columns \\\"image\\\" and \\\"prompt\\\". dataset = lance.torch.data.LanceDataset( \\\"diffusiondb_train.lance\\\", columns=[\\\"image\\\", \\\"prompt\\\"], batch_size=128, batch_readahead=8, # Control multi-threading reads. )</p> <p># Create a PyTorch DataLoader dataloader = torch.utils.data.DataLoader(dataset)</p> <p># Inference loop for batch in dataloader: inputs, targets = batch[\\\"prompt\\\"], batch[\\\"image\\\"] outputs = model(inputs) ...</p> <p>* Safe Dataloader .. code-block:: python from lance.torch.data import SafeLanceDataset, get_safe_loader dataset = SafeLanceDataset(temp_lance_dataset) loader = get_safe_loader( dataset, num_workers=2, batch_size=16, drop_last=False, )</p> <p>total_samples = 0 for batch in loader: total_samples += batch[\\\"id\\\"].shape[0]</p> <p><code>~lance.torch.data.LanceDataset</code> can composite with the <code>~lance.sampler.Sampler</code>{.interpreted-text role=\"class\"} classes to control the sampling strategy. For example, you can use <code>~lance.sampler.ShardedFragmentSampler</code>{.interpreted-text role=\"class\"} to use it in a distributed training environment. If not specified, it is a full scan.</p> <pre><code>from lance.sampler import ShardedFragmentSampler\nfrom lance.torch.data import LanceDataset\n\n# Load lance dataset into a PyTorch IterableDataset.\n# with only columns \"image\" and \"prompt\".\ndataset = LanceDataset(\n    \"diffusiondb_train.lance\",\n    columns=[\"image\", \"prompt\"],\n    batch_size=128,\n    batch_readahead=8,  # Control multi-threading reads.\n    sampler=ShardedFragmentSampler(\n        rank=1,  # Rank of the current process\n        world_size=8,  # Total number of processes\n    ),\n)\n</code></pre> <p>Available samplers:</p> <ul> <li><code>lance.sampler.ShardedFragmentSampler</code></li> <li><code>lance.sampler.ShardedBatchSampler</code></li> </ul> <p>Warning</p> <p>For multiprocessing you should probably not use fork as lance is multi-threaded internally and fork and multi-thread do not work well. Refer to this discussion.</p>"},{"location":"integrations/ray/","title":"Lance \u2764\ufe0f Ray","text":"<p>Ray effortlessly scale up ML workload to large distributed compute environment.</p> <p>Lance format is one of the official Ray data sources:</p> <ul> <li>Lance Data Source <code>ray.data.read_lance</code>{.interpreted-text   role=\"py:meth\"}</li> <li>Lance Data Sink <code>ray.data.Dataste.write_lance</code>{.interpreted-text   role=\"py:meth\"}</li> </ul> <pre><code>import ray import pandas as pd\n\nray.init()\n\ndata = \\[\n\n:   {\\\"id\\\": 1, \\\"name\\\": \\\"alice\\\"}, {\\\"id\\\": 2, \\\"name\\\": \\\"bob\\\"},\n    {\\\"id\\\": 3, \\\"name\\\": \\\"charlie\\\"}\n\n\\]\nray.data.from_items(data).write_lance(\\\"./alice_bob_and_charlie.lance\\\")\n\n\\# It can be read via lance directly df = ( lance.\ndataset(\\\"./alice_bob_and_charlie.lance\\\") .to_table() .to_pandas()\n.sort_values(by=\\[\\\"id\\\"\\]) .reset_index(drop=True) ) assert\ndf.equals(pd.DataFrame(data)), \\\"{} != {}\\\".format( df,\npd.DataFrame(data) )\n\n\\# Or via Ray.data.read_lance ray_df = (\nray.data.read_lance(\\\"./alice_bob_and_charlie.lance\\\") .to_pandas()\n.sort_values(by=\\[\\\"id\\\"\\]) .reset_index(drop=True) ) assert\ndf.equals(ray_df)\n</code></pre>"},{"location":"integrations/spark/","title":"Lance \u2764\ufe0f Spark","text":"<p>Lance can be used as a third party datasource of https://spark.apache.org/docs/latest/sql-data-sources.html</p> <p>Warning</p> <p>This feature is experimental and the APIs may change in the future.</p>"},{"location":"integrations/spark/#build-from-source-code","title":"Build from source code","text":"<pre><code>git clone https://github.com/lancedb/lance.git\ncd lance/java\nmvn clean package -DskipTests -Drust.release.build=true\n</code></pre> <p>After building the code, the spark related jars are under path <code>lance/java/spark/target/jars/</code></p> <pre><code>arrow-c-data-15.0.0.jar\narrow-dataset-15.0.0.jar\njar-jni-1.1.1.jar\nlance-core-0.25.0-SNAPSHOT.jar\nlance-spark-0.25.0-SNAPSHOT.jar\n</code></pre>"},{"location":"integrations/spark/#download-the-pre-build-jars","title":"Download the pre-build jars","text":"<p>If you did not want to get jars from source, you can download these five jars from maven repo.</p> <pre><code>wget https://repo1.maven.org/maven2/com/lancedb/lance-core/0.23.0/lance-core-0.23.0.jar\nwget https://repo1.maven.org/maven2/com/lancedb/lance-spark/0.23.0/lance-spark-0.23.0.jar\nwget https://repo1.maven.org/maven2/org/questdb/jar-jni/1.1.1/jar-jni-1.1.1.jar\nwget https://repo1.maven.org/maven2/org/apache/arrow/arrow-c-data/12.0.1/arrow-c-data-12.0.1.jar\nwget https://repo1.maven.org/maven2/org/apache/arrow/arrow-dataset/12.0.1/arrow-dataset-12.0.1.jar\n</code></pre>"},{"location":"integrations/spark/#configurations-for-lance-spark-connector","title":"Configurations for Lance Spark Connector","text":"<p>There are some configurations you have to set in <code>spark-defaults.conf</code> to enable lance datasource.</p> <pre><code>spark.sql.catalog.lance com.lancedb.lance.spark.LanceCatalog\n</code></pre> <p>This config define the [LanceCatalog]{.title-ref} and then the spark will treat lance as a datasource.</p> <p>If dealing with lance dataset stored in object store, these configurations should be set:</p> <pre><code>spark.sql.catalog.lance.access_key_id {your object store ak}\nspark.sql.catalog.lance.secret_access_key {your object store sk}\nspark.sql.catalog.lance.aws_region {your object store region(optional)}\nspark.sql.catalog.lance.aws_endpoint {your object store aws_endpoint which should be in virtual host style}\nspark.sql.catalog.lance.virtual_hosted_style_request true\n</code></pre>"},{"location":"integrations/spark/#startup-the-spark-shell","title":"Startup the Spark Shell","text":"<pre><code>bin/spark-shell --master \"local[56]\"  --jars \"/path_of_code/lance/java/spark/target/jars/*.jar\"\n</code></pre> <p>Use <code>--jars</code> to involve the related jars we build or downloaded.</p> <p>Note</p> <p>Spark shell console use <code>scala</code> language not <code>python</code></p>"},{"location":"integrations/spark/#using-spark-shell-to-manipulate-lance-dataset","title":"Using Spark Shell to manipulate lance dataset","text":"<ul> <li>Write a new dataset named <code>test.lance</code></li> </ul> <pre><code>val df = Seq(\n  (\"Alice\", 1),\n  (\"Bob\", 2)\n).toDF(\"name\", \"id\")\ndf.write.format(\"lance\").option(\"path\",\"./test.lance\").save()\n</code></pre> <ul> <li>Overwrite the <code>test.lance</code> dataset</li> </ul> <pre><code>val df = Seq(\n  (\"Alice\", 3),\n  (\"Bob\", 4)\n).toDF(\"name\", \"id\")\ndf.write.format(\"lance\").option(\"path\",\"./test.lance\").mode(\"overwrite\").save()\n</code></pre> <ul> <li>Append Data into the <code>test.lance</code>   dataset</li> </ul> <pre><code>val df = Seq(\n  (\"Chris\", 5),\n  (\"Derek\", 6)\n).toDF(\"name\", \"id\")\ndf.write.format(\"lance\").option(\"path\",\"./test.lance\").mode(\"append\").save()\n</code></pre> <ul> <li>Use spark data frame to read the <code>test.lance</code>{.interpreted-text   role=\"class\"} dataset</li> </ul> <pre><code>val data = spark.read.format(\"lance\").option(\"path\", \"./test.lance\").load();\ndata.show()\n</code></pre> <ul> <li>Register data frame as table and use sql to query   <code>test.lance</code> dataset</li> </ul> <pre><code>data.createOrReplaceTempView(\"lance_table\")\nspark.sql(\"select id, count(*) from lance_table group by id order by id\").show()\n</code></pre>"},{"location":"integrations/tensorflow/","title":"Tensorflow Integration","text":"<p>Lance can be used as a regular tf.data.Dataset in Tensorflow.</p> <p>Warning</p> <p>This feature is experimental and the APIs may change in the future.</p>"},{"location":"integrations/tensorflow/#reading-from-lance","title":"Reading from Lance","text":"<p>Using <code>lance.tf.data.from_lance</code>, you can create an [tf.data.Dataset]{.title-ref} easily.</p> <pre><code>import tensorflow as tf\nimport lance\n\n# Create tf dataset\nds = lance.tf.data.from_lance(\"s3://my-bucket/my-dataset\")\n\n# Chain tf dataset with other tf primitives\n\nfor batch in ds.shuffling(32).map(lambda x: tf.io.decode_png(x[\"image\"])):\n    print(batch)\n</code></pre> <p>Backed by the Lance columnar format, using <code>lance.tf.data.from_lance</code> supports efficient column selection, filtering, and more.</p> <pre><code>ds = lance.tf.data.from_lance(\n    \"s3://my-bucket/my-dataset\",\n    columns=[\"image\", \"label\"],\n    filter=\"split = 'train' AND collected_time &gt; timestamp '2020-01-01'\",\n    batch_size=256)\n</code></pre> <p>By default, Lance will infer the Tensor spec from the projected columns. You can also specify <code>tf.TensorSpec</code> manually.</p> <pre><code>batch_size = 256\nds = lance.tf.data.from_lance(\n    \"s3://my-bucket/my-dataset\",\n    columns=[\"image\", \"labels\"],\n    batch_size=batch_size,\n    output_signature={\n        \"image\": tf.TensorSpec(shape=(), dtype=tf.string),\n        \"labels\": tf.RaggedTensorSpec(\n            dtype=tf.int32, shape=(batch_size, None), ragged_rank=1),\n    },\n</code></pre>"},{"location":"integrations/tensorflow/#distributed-training-and-shuffling","title":"Distributed Training and Shuffling","text":"<p>Since a Lance Dataset is a set of Fragments, we can distribute and shuffle Fragments to different workers.</p> <pre><code>import tensorflow as tf\nfrom lance.tf.data import from_lance, lance_fragments\n\nworld_size = 32\nrank = 10\nseed = 123  #\nepoch = 100\n\ndataset_uri = \"s3://my-bucket/my-dataset\"\n\n# Shuffle fragments distributedly.\nfragments =\n    lance_fragments(\"s3://my-bucket/my-dataset\")\n    .shuffling(32, seed=seed)\n    .repeat(epoch)\n    .enumerate()\n    .filter(lambda i, _: i % world_size == rank)\n    .map(lambda _, fid: fid)\n\nds = from_lance(\n    uri,\n    columns=[\"image\", \"label\"],\n    fragments=fragments,\n    batch_size=32\n    )\nfor batch in ds:\n    print(batch)\n</code></pre> <p>Warning</p> <p>For multiprocessing you should probably not use fork as lance is multi-threaded internally and fork and multi-thread do not work well. Refer to this discussion.</p>"},{"location":"introduction/read_and_write/","title":"Read and Write Data","text":""},{"location":"introduction/read_and_write/#writing-lance-dataset","title":"Writing Lance Dataset","text":"<p>If you\\'re familiar with Apache PyArrow, you\\'ll find that creating a Lance dataset is straightforward. Begin by writing a <code>pyarrow.Table</code> using the <code>lance.write_dataset</code> function.</p> <p>If the dataset is too large to fully load into memory, you can stream data using <code>lance.write_dataset</code> also supports <code>~typing.Iterator</code> of <code>pyarrow.RecordBatch</code> es. You will need to provide a <code>pyarrow.Schema</code> for the dataset in this case.</p> <p><code>lance.write_dataset</code> supports writing <code>pyarrow.Table</code>, <code>pandas.DataFrame</code>, <code>pyarrow.dataset.Dataset</code>, and <code>Iterator[pyarrow.RecordBatch]</code>.</p>"},{"location":"introduction/read_and_write/#adding-rows","title":"Adding Rows","text":"<p>To insert data into your dataset, you can use either <code>LanceDataset.insert &lt;lance.LanceDataset.insert&gt;</code>{.interpreted-text role=\"py:meth\"} or <code>~lance.write_dataset</code>{.interpreted-text role=\"py:meth\"} with <code>mode=append</code>.</p>"},{"location":"introduction/read_and_write/#deleting-rows","title":"Deleting rows","text":"<p>Lance supports deleting rows from a dataset using a SQL filter, as described in <code>filter-push-down</code>. For example, to delete Bob\\'s row from the dataset above, one could use:</p> <p>Note</p> <p>The Lance Format is immutable. Each write operation creates a new version of the dataset, so users must reopen the dataset to see the changes. Likewise, rows are removed by marking them as deleted in a separate deletion index, rather than rewriting the files. This approach is faster and avoids invalidating any indices that reference the files, ensuring that subsequent queries do not return the deleted rows.</p>"},{"location":"introduction/read_and_write/#updating-rows","title":"Updating rows","text":"<p>Lance supports updating rows based on SQL expressions with the <code>lance.LanceDataset.update</code> method. For example, if we notice that Bob\\'s name in our dataset has been sometimes written as <code>Blob</code>, we can fix that with:</p> <pre><code>import lance\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\ndataset.update({\"name\": \"'Bob'\"}, where=\"name = 'Blob'\")\n</code></pre> <p>The update values are SQL expressions, which is why <code>'Bob'</code> is wrapped in single quotes. This means we can use complex expressions that reference existing columns if we wish. For example, if two years have passed and we wish to update the ages of Alice and Bob in the same example, we could write:</p> <pre><code>import lance\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\ndataset.update({\"age\": \"age + 2\"})\n</code></pre> <p>If you are trying to update a set of individual rows with new values then it is often more efficient to use the merge insert operation described below.</p> <pre><code>import lance\n\n# Change the ages of both Alice and Bob\nnew_table = pa.Table.from_pylist([{\"name\": \"Alice\", \"age\": 30},\n                                  {\"name\": \"Bob\", \"age\": 20}])\n\n# This works, but is inefficient, see below for a better approach\ndataset = lance.dataset(\"./alice_and_bob.lance\")\nfor idx in range(new_table.num_rows):\n  name = new_table[0][idx].as_py()\n  new_age = new_table[1][idx].as_py()\n  dataset.update({\"age\": new_age}, where=f\"name='{name}'\")\n</code></pre>"},{"location":"introduction/read_and_write/#merge-insert","title":"Merge Insert","text":"<p>Lance supports a merge insert operation. This can be used to add new data in bulk while also (potentially) matching against existing data. This operation can be used for a number of different use cases.</p>"},{"location":"introduction/read_and_write/#bulk-update","title":"Bulk Update","text":"<p>The <code>lance.LanceDataset.update</code> method is useful for updating rows based on a filter. However, if we want to replace existing rows with new rows then a <code>lance.LanceDataset.merge_insert</code> operation would be more efficient:</p> <p>Note that, similar to the update operation, rows that are modified will be removed and inserted back into the table, changing their position to the end. Also, the relative order of these rows could change because we are using a hash-join operation internally.</p>"},{"location":"introduction/read_and_write/#insert-if-not-exists","title":"Insert if not Exists","text":"<p>Sometimes we only want to insert data if we haven\\'t already inserted it before. This can happen, for example, when we have a batch of data but we don\\'t know which rows we\\'ve added previously and we don\\'t want to create duplicate rows. We can use the merge insert operation to achieve this:</p>"},{"location":"introduction/read_and_write/#update-or-insert-upsert","title":"Update or Insert (Upsert)","text":"<p>Sometimes we want to combine both of the above behaviors. If a row already exists we want to update it. If the row does not exist we want to add it. This operation is sometimes called \\\"upsert\\\". We can use the merge insert operation to do this as well:</p>"},{"location":"introduction/read_and_write/#replace-a-portion-of-data","title":"Replace a Portion of Data","text":"<p>A less common, but still useful, behavior can be to replace some region of existing rows (defined by a filter) with new data. This is similar to performing both a delete and an insert in a single transaction. For example:</p>"},{"location":"introduction/read_and_write/#reading-lance-dataset","title":"Reading Lance Dataset","text":"<p>To open a Lance dataset, use the <code>lance.dataset</code>{.interpreted-text role=\"py:meth\"} function:</p> <pre><code>import lance\nds = lance.dataset(\"s3://bucket/path/imagenet.lance\")\n# Or local path\nds = lance.dataset(\"./imagenet.lance\")\n</code></pre> <p>Note</p> <p>Lance supports local file system, AWS <code>s3</code> and Google Cloud Storage(<code>gs</code>) as storage backends at the moment. Read more in Object Store Configuration.</p> <p>The most straightforward approach for reading a Lance dataset is to utilize the <code>lance.LanceDataset.to_table</code>{.interpreted-text role=\"py:meth\"} method in order to load the entire dataset into memory.</p> <pre><code>table = ds.to_table()\n</code></pre> <p>Due to Lance being a high-performance columnar format, it enables efficient reading of subsets of the dataset by utilizing Column (projection) push-down and filter (predicates) push-downs.</p> <pre><code>table = ds.to_table(\n    columns=[\"image\", \"label\"],\n    filter=\"label = 2 AND text IS NOT NULL\",\n    limit=1000,\n    offset=3000)\n</code></pre> <p>Lance understands the cost of reading heavy columns such as <code>image</code>. Consequently, it employs an optimized query plan to execute the operation efficiently.</p>"},{"location":"introduction/read_and_write/#iterative-read","title":"Iterative Read","text":"<p>If the dataset is too large to fit in memory, you can read it in batches using the <code>lance.LanceDataset.to_batches</code>{.interpreted-text role=\"py:meth\"} method:</p> <pre><code>for batch in ds.to_batches(columns=[\"image\"], filter=\"label = 10\"):\n    # do something with batch\n    compute_on_batch(batch)\n</code></pre> <p>Unsurprisingly, <code>~lance.LanceDataset.to_batches</code>{.interpreted-text role=\"py:meth\"} takes the same parameters as <code>~lance.LanceDataset.to_table</code> function.</p>"},{"location":"introduction/read_and_write/#filter-push-down","title":"Filter push-down","text":"<p>Lance embraces the utilization of standard SQL expressions as predicates for dataset filtering. By pushing down the SQL predicates directly to the storage system, the overall I/O load during a scan is significantly reduced.</p> <p>Currently, Lance supports a growing list of expressions.</p> <ul> <li><code>&gt;</code>, <code>&gt;=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>=</code></li> <li><code>AND</code>, <code>OR</code>, <code>NOT</code></li> <li><code>IS NULL</code>, <code>IS NOT NULL</code></li> <li><code>IS TRUE</code>, <code>IS NOT TRUE</code>, <code>IS FALSE</code>, <code>IS NOT FALSE</code></li> <li><code>IN</code></li> <li><code>LIKE</code>, <code>NOT LIKE</code></li> <li><code>regexp_match(column, pattern)</code></li> <li><code>CAST</code></li> </ul> <p>For example, the following filter string is acceptable:</p> <pre><code>((label IN [10, 20]) AND (note['email'] IS NOT NULL))\n    OR NOT note['created']\n</code></pre> <p>Nested fields can be accessed using the subscripts. Struct fields can be subscripted using field names, while list fields can be subscripted using indices.</p> <p>If your column name contains special characters or is a SQL Keyword, you can use backtick (<code>`\\</code>) to escape it. For nested fields, each segment of the path must be wrapped in backticks.</p> <pre><code>`CUBE` = 10 AND `column name with space` IS NOT NULL\n  AND `nested with space`.`inner with space` &lt; 2\n</code></pre> <p>Warning</p> <p>Field names containing periods (<code>.</code>) are not supported.</p> <p>Literals for dates, timestamps, and decimals can be written by writing the string value after the type name. For example</p> <pre><code>date_col = date '2021-01-01'\nand timestamp_col = timestamp '2021-01-01 00:00:00'\nand decimal_col = decimal(8,3) '1.000'\n</code></pre> <p>For timestamp columns, the precision can be specified as a number in the type parameter. Microsecond precision (6) is the default.</p> <p>SQL                            Time unit</p> <p><code>timestamp(0)</code>                 Seconds</p> <p><code>timestamp(3)</code>                 Milliseconds</p> <p><code>timestamp(6)</code>                 Microseconds</p> <p><code>timestamp(9)</code>                 Nanoseconds</p> <p>Lance internally stores data in Arrow format. The mapping from SQL types to Arrow is:</p> <p>SQL type                       Arrow type</p> <p><code>boolean</code> <code>Boolean</code></p> <p><code>tinyint</code> / <code>tinyint unsigned</code> <code>Int8</code> / <code>UInt8</code></p> <p><code>smallint</code> /                   <code>Int16</code> / <code>UInt16</code> <code>smallint unsigned</code> </p> <p><code>int</code> or <code>integer</code> /           <code>Int32</code> / <code>UInt32</code> <code>int unsigned</code> or             <code>integer unsigned</code> </p> <p><code>bigint</code> / <code>bigint unsigned</code> <code>Int64</code> / <code>UInt64</code></p> <p><code>float</code> <code>Float32</code></p> <p><code>double</code> <code>Float64</code></p> <p><code>decimal(precision, scale)</code> <code>Decimal128</code></p> <p><code>date</code> <code>Date32</code></p> <p><code>timestamp</code> <code>Timestamp</code> (1)</p> <p><code>string</code> <code>Utf8</code></p> <p><code>binary</code> <code>Binary</code></p> <p>(1) See precision mapping in previous table.</p>"},{"location":"introduction/read_and_write/#random-read","title":"Random read","text":"<p>One district feature of Lance, as columnar format, is that it allows you to read random samples quickly.</p> <pre><code># Access the 2nd, 101th and 501th rows\ndata = ds.take([1, 100, 500], columns=[\"image\", \"label\"])\n</code></pre> <p>The ability to achieve fast random access to individual rows plays a crucial role in facilitating various workflows such as random sampling and shuffling in ML training. Additionally, it empowers users to construct secondary indices, enabling swift execution of queries for enhanced performance.</p>"},{"location":"introduction/read_and_write/#table-maintenance","title":"Table Maintenance","text":"<p>Some operations over time will cause a Lance dataset to have a poor layout. For example, many small appends will lead to a large number of small fragments. Or deleting many rows will lead to slower queries due to the need to filter out deleted rows.</p> <p>To address this, Lance provides methods for optimizing dataset layout.</p>"},{"location":"introduction/read_and_write/#compact-data-files","title":"Compact data files","text":"<p>Data files can be rewritten so there are fewer files. When passing a <code>target_rows_per_fragment</code> to <code>lance.dataset.DatasetOptimizer.compact_files</code>{.interpreted-text role=\"py:meth\"}, Lance will skip any fragments that are already above that row count, and rewrite others. Fragments will be merged according to their fragment ids, so the inherent ordering of the data will be preserved.</p> <p>Note</p> <p>Compaction creates a new version of the table. It does not delete the old version of the table and the files referenced by it.</p> <pre><code>import lance\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\ndataset.optimize.compact_files(target_rows_per_fragment=1024 * 1024)\n</code></pre> <p>During compaction, Lance can also remove deleted rows. Rewritten fragments will not have deletion files. This can improve scan performance since the soft deleted rows don\\'t have to be skipped during the scan.</p> <p>When files are rewritten, the original row addresses are invalidated. This means the affected files are no longer part of any ANN index if they were before. Because of this, it\\'s recommended to rewrite files before re-building indices.</p>"},{"location":"introduction/schema_evolution/","title":"Schema Evolution","text":"<p>Lance supports schema evolution: adding, removing, and altering columns in a dataset. Most of these operations can be performed without rewriting the data files in the dataset, making them very efficient operations.</p> <p>In general, schema changes will conflict with most other concurrent write operations. For example, if you change the schema of the dataset while someone else is appending data to it, either your schema change or the append will fail, depending on the order of the operations. Thus, it\\'s recommended to perform schema changes when no other writes are happening.</p>"},{"location":"introduction/schema_evolution/#renaming-columns","title":"Renaming columns","text":"<p>Columns can be renamed using the <code>lance.LanceDataset.alter_columns</code> method.</p> <pre><code>table = pa.table({\\\"id\\\": pa.array(\\[1, 2, 3\\])}) dataset =\nlance.write_dataset(table, \\\"ids\\\") dataset.alter_columns({\\\"path\\\":\n\\\"id\\\", \\\"name\\\": \\\"new_id\\\"}) print(dataset.to_table().to_pandas())\n</code></pre> <pre><code>new_id 0 1 1 2 2 3\n</code></pre> <p>This works for nested columns as well. To address a nested column, use a dot (<code>.</code>) to separate the levels of nesting. For example:</p> <pre><code>data = \\[\n\n:   {\\\"meta\\\": {\\\"id\\\": 1, \\\"name\\\": \\\"Alice\\\"}}, {\\\"meta\\\": {\\\"id\\\": 2,\n    \\\"name\\\": \\\"Bob\\\"}},\n\n\\] schema = pa.schema(\\[ (\\\"meta\\\", pa.struct(\\[ (\\\"id\\\", pa.int32()),\n(\\\"name\\\", pa.string()), \\])) \\]) dataset = lance.write_dataset(data,\n\\\"nested_rename\\\") dataset.alter_columns({\\\"path\\\": \\\"meta.id\\\",\n\\\"name\\\": \\\"new_id\\\"}) print(dataset.to_table().to_pandas())\n</code></pre> <pre><code>meta 0 {\\'new_id\\': 1, \\'name\\': \\'Alice\\'} 1 {\\'new_id\\': 2, \\'name\\':\n\\'Bob\\'}\n</code></pre>"},{"location":"introduction/schema_evolution/#casting-column-data-types","title":"Casting column data types","text":"<p>In addition to changing column names, you can also change the data type of a column using the <code>lance.LanceDataset.alter_columns</code> method. This requires rewriting that column to new data files, but does not require rewriting the other columns.</p> <p>Note</p> <p>If the column has an index, the index will be dropped if the column type is changed.</p> <p>This method can be used to change the vector type of a column. For example, we can change a float32 embedding column into a float16 column to save disk space at the cost of lower precision:</p> <pre><code>table = pa.table({\n\n:   \\\"id\\\": pa.array(\\[1, 2, 3\\]), \\\"embedding\\\":\n    pa.FixedShapeTensorArray.from_numpy_ndarray( np.random.rand(3,\n    128).astype(\\\"float32\\\"))\n\n}) dataset = lance.write_dataset(table, \\\"embeddings\\\")\ndataset.alter_columns({\\\"path\\\": \\\"embedding\\\", \\\"data_type\\\":\n[pa.list]()(pa.float16(), 128)}) print(dataset.schema)\n</code></pre> <pre><code>id: int64 embedding: fixed_size_list\\&lt;item: halffloat\\&gt;\\[128\\] child 0,\nitem: halffloat\n</code></pre>"},{"location":"introduction/schema_evolution/#adding-new-columns","title":"Adding new columns","text":"<p>New columns can be added and populated within a single operation using the <code>lance.LanceDataset.add_columns</code> method. There are two ways to specify how to populate the new columns: first, by providing a SQL expression for each new column, or second, by providing a function to generate the new column data.</p> <p>SQL expressions can either be independent expressions or reference existing columns. SQL literal values can be used to set a single value for all existing rows.</p> <pre><code>table = pa.table({\\\"name\\\": pa.array(\\[\\\"Alice\\\", \\\"Bob\\\",\n\\\"Carla\\\"\\])}) dataset = lance.write_dataset(table, \\\"names\\\")\ndataset.add_columns({ \\\"hash\\\": \\\"sha256(name)\\\", \\\"status\\\":\n\\\"\\'active\\'\\\", }) print(dataset.to_table().to_pandas())\n</code></pre> <pre><code>name hash status 0 Alice b\\';xc5x10bx97\\&lt;Ex8dZo-x8ddxa0#\\$cTxad\\~x0\\...\nactive 1 Bob b\\'xcdx9fxb1xe1Hxccxd8D.Zxa7Ix04xccsx\\... active 2 Carla\nb\\'xadx8dx83xffxd8+Zx8exd4)xe8Y+\\\\xb3\\... active\n</code></pre> <p>You can also provide a Python function to generate the new column data. This can be used, for example, to compute a new embedding column. This function should take a PyArrow RecordBatch and return either a PyArrow RecordBatch or a Pandas DataFrame. The function will be called once for each batch in the dataset.</p> <p>If the function is expensive to compute and can fail, it is recommended to set a checkpoint file in the UDF. This checkpoint file saves the state of the UDF after each invocation, so that if the UDF fails, it can be restarted from the last checkpoint. Note that this file can get quite large, since it needs to store unsaved results for up to an entire data file.</p> <pre><code>import lance\nimport pyarrow as pa\nimport numpy as np\n\ntable = pa.table({\"id\": pa.array([1, 2, 3])})\ndataset = lance.write_dataset(table, \"ids\")\n\n@lance.batch_udf(checkpoint_file=\"embedding_checkpoint.sqlite\")\ndef add_random_vector(batch):\n    embeddings = np.random.rand(batch.num_rows, 128).astype(\"float32\")\n    return pd.DataFrame({\"embedding\": embeddings})\ndataset.add_columns(add_random_vector)\n</code></pre>"},{"location":"introduction/schema_evolution/#adding-new-columns-with-schema-only","title":"Adding new columns with Schema only","text":"<p>A common use case we\\'ve seen in production is to add a new column to a dataset without populating it. This is useful to later run a large distributed job to populate the column lazily. To do this, you can use the <code>lance.LanceDataset.add_columns</code> method to add columns with <code>pyarrow.Field</code>{.interpreted-text role=\"py:class\"} or <code>pyarrow.Schema</code>.</p> <pre><code>table = pa.table({\\\"id\\\": pa.array(\\[1, 2, 3\\])}) dataset =\nlance.write_dataset(table, \\\"null_columns\\\")\n\n\\# With pyarrow Field dataset.add_columns(pa.field(\\\"embedding\\\",\n[pa.list]()(pa.float32(), 128))) assert dataset.schema == pa.schema(\\[\n(\\\"id\\\", pa.int64()), (\\\"embedding\\\", [pa.list]()(pa.float32(), 128)),\n\\])\n\n\\# With pyarrow Schema dataset.add_columns(pa.schema(\\[ (\\\"label\\\",\npa.string()), (\\\"score\\\", pa.float32()), \\])) assert dataset.schema ==\npa.schema(\\[ (\\\"id\\\", pa.int64()), (\\\"embedding\\\",\n[pa.list]()(pa.float32(), 128)), (\\\"label\\\", pa.string()), (\\\"score\\\",\npa.float32()), \\])\n</code></pre> <p>This operation is very fast, as it only updates the metadata of the dataset.</p>"},{"location":"introduction/schema_evolution/#adding-new-columns-using-merge","title":"Adding new columns using merge","text":"<p>If you have pre-computed one or more new columns, you can add them to an existing dataset using the <code>lance.LanceDataset.merge</code>{.interpreted-text role=\"py:meth\"} method. This allows filling in additional columns without having to rewrite the whole dataset.</p> <p>To use the <code>merge</code> method, provide a new dataset that includes the columns you want to add, and a column name to use for joining the new data to the existing dataset.</p> <p>For example, imagine we have a dataset of embeddings and ids:</p> <pre><code>table = pa.table({\n\n:   \\\"id\\\": pa.array(\\[1, 2, 3\\]), \\\"embedding\\\":\n    pa.array(\\[np.array(\\[1, 2, 3\\]), np.array(\\[4, 5, 6\\]),\n    np.array(\\[7, 8, 9\\])\\])\n\n}) dataset = lance.write_dataset(table, \\\"embeddings\\\",\nmode=\\\"overwrite\\\")\n</code></pre> <p>Now if we want to add a column of labels we have generated, we can do so by merging a new table:</p> <pre><code>new_data = pa.table({\n\n:   \\\"id\\\": pa.array(\\[1, 2, 3\\]), \\\"label\\\": pa.array(\\[\\\"horse\\\",\n    \\\"rabbit\\\", \\\"cat\\\"\\])\n\n}) dataset.merge(new_data, \\\"id\\\") print(dataset.to_table().to_pandas())\n</code></pre> <pre><code>id embedding label 0 1 \\[1, 2, 3\\] horse 1 2 \\[4, 5, 6\\] rabbit 2 3 \\[7,\n8, 9\\] cat\n</code></pre>"},{"location":"introduction/schema_evolution/#dropping-columns","title":"Dropping columns","text":"<p>Finally, you can drop columns from a dataset using the <code>lance.LanceDataset.drop_columns</code> method. This is a metadata-only operation and does not delete the data on disk. This makes it very quick.</p> <p>To actually remove the data from disk, the files must be rewritten to remove the columns and then the old files must be deleted. This can be done using <code>lance.dataset.DatasetOptimizer.compact_files()</code>{.interpreted-text role=\"py:meth\"} followed by <code>lance.LanceDataset.cleanup_old_versions()</code>{.interpreted-text role=\"py:meth\"}.</p>"},{"location":"notebooks/quickstart/","title":"Quickstart","text":"<pre><code>import shutil\n\nimport lance\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\n</code></pre>"},{"location":"notebooks/quickstart/#creating-datasets","title":"Creating datasets","text":"<p>Via pyarrow it\\'s really easy to create lance datasets</p> <p>Create a dataframe</p> <pre><code>df = pd.DataFrame({\"a\": [5]})\ndf\n</code></pre> a 0 5 <p>Write it to lance</p> <pre><code>shutil.rmtree(\"/tmp/test.lance\", ignore_errors=True)\n\ndataset = lance.write_dataset(df, \"/tmp/test.lance\")\ndataset.to_table().to_pandas()\n</code></pre> a 0 5"},{"location":"notebooks/quickstart/#converting-from-parquet","title":"Converting from parquet","text":"<pre><code>shutil.rmtree(\"/tmp/test.parquet\", ignore_errors=True)\nshutil.rmtree(\"/tmp/test.lance\", ignore_errors=True)\n\ntbl = pa.Table.from_pandas(df)\npa.dataset.write_dataset(tbl, \"/tmp/test.parquet\", format='parquet')\n\nparquet = pa.dataset.dataset(\"/tmp/test.parquet\")\nparquet.to_table().to_pandas()\n</code></pre> a 0 5 <p>Write to lance in 1 line</p> <pre><code>dataset = lance.write_dataset(parquet, \"/tmp/test.lance\")\n</code></pre> <pre><code># make sure it's the same\ndataset.to_table().to_pandas()\n</code></pre> a 0 5"},{"location":"notebooks/quickstart/#versioning","title":"Versioning","text":"<p>We can append rows</p> <pre><code>df = pd.DataFrame({\"a\": [10]})\ntbl = pa.Table.from_pandas(df)\ndataset = lance.write_dataset(tbl, \"/tmp/test.lance\", mode=\"append\")\n\ndataset.to_table().to_pandas()\n</code></pre> a 0 5 1 10 <p>We can overwrite the data and create a new version</p> <pre><code>df = pd.DataFrame({\"a\": [50, 100]})\ntbl = pa.Table.from_pandas(df)\ndataset = lance.write_dataset(tbl, \"/tmp/test.lance\", mode=\"overwrite\")\n</code></pre> <pre><code>dataset.to_table().to_pandas()\n</code></pre> a 0 50 1 100 <p>The old version is still there</p> <pre><code>dataset.versions()\n</code></pre> <pre><code>[{'version': 1,\n  'timestamp': datetime.datetime(2024, 8, 15, 21, 22, 31, 453453),\n  'metadata': {}},\n {'version': 2,\n  'timestamp': datetime.datetime(2024, 8, 15, 21, 22, 35, 475152),\n  'metadata': {}},\n {'version': 3,\n  'timestamp': datetime.datetime(2024, 8, 15, 21, 22, 45, 32922),\n  'metadata': {}}]\n</code></pre> <pre><code>lance.dataset('/tmp/test.lance', version=1).to_table().to_pandas()\n</code></pre> a 0 5 <pre><code>lance.dataset('/tmp/test.lance', version=2).to_table().to_pandas()\n</code></pre> a 0 5 1 10 <p>We can create tags</p> <pre><code>dataset.tags.create(\"stable\", 2)\ndataset.tags.create(\"nightly\", 3)\ndataset.tags.list()\n</code></pre> <pre><code>{'nightly': {'version': 3, 'manifest_size': 628},\n 'stable': {'version': 2, 'manifest_size': 684}}\n</code></pre> <p>which can be checked out</p> <pre><code>lance.dataset('/tmp/test.lance', version=\"stable\").to_table().to_pandas()\n</code></pre> a 0 5 1 10"},{"location":"notebooks/quickstart/#vectors","title":"Vectors","text":""},{"location":"notebooks/quickstart/#data-preparation","title":"Data preparation","text":"<p>For this tutorial let\\'s use the Sift 1M dataset:</p> <ul> <li>Download <code>ANN_SIFT1M</code> from: http://corpus-texmex.irisa.fr/</li> <li>Direct link should be   <code>ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz</code></li> <li>Download and then unzip the tarball</li> </ul> <pre><code>!rm -rf sift* vec_data.lance\n!wget ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz\n!tar -xzf sift.tar.gz\n</code></pre> <pre><code>--2023-02-13 16:54:50--  ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz\n           =&gt; \u2018sift.tar.gz\u2019\nResolving ftp.irisa.fr (ftp.irisa.fr)... 131.254.254.45\nConnecting to ftp.irisa.fr (ftp.irisa.fr)|131.254.254.45|:21... connected.\nLogging in as anonymous ... Logged in!\n==&gt; SYST ... done.    ==&gt; PWD ... done.\n==&gt; TYPE I ... done.  ==&gt; CWD (1) /local/texmex/corpus ... done.\n==&gt; SIZE sift.tar.gz ... 168280445\n==&gt; PASV ... done.    ==&gt; RETR sift.tar.gz ... done.\nLength: 168280445 (160M) (unauthoritative)\n\nsift.tar.gz         100%[===================&gt;] 160.48M  6.85MB/s    in 36s\n\n2023-02-13 16:55:29 (4.43 MB/s) - \u2018sift.tar.gz\u2019 saved [168280445]\n</code></pre> <p>Convert it to Lance</p> <pre><code>from lance.vector import vec_to_table\nimport struct\n\nuri = \"vec_data.lance\"\n\nwith open(\"sift/sift_base.fvecs\", mode=\"rb\") as fobj:\n    buf = fobj.read()\n    data = np.array(struct.unpack(\"&lt;128000000f\", buf[4 : 4 + 4 * 1000000 * 128])).reshape((1000000, 128))\n    dd = dict(zip(range(1000000), data))\n\ntable = vec_to_table(dd)\nlance.write_dataset(table, uri, max_rows_per_group=8192, max_rows_per_file=1024*1024)\n</code></pre> <pre><code>&lt;lance.dataset.LanceDataset at 0x13859fe20&gt;\n</code></pre> <pre><code>uri = \"vec_data.lance\"\nsift1m = lance.dataset(uri)\n</code></pre>"},{"location":"notebooks/quickstart/#knn-no-index","title":"KNN (no index)","text":"<p>Sample 100 vectors as query vectors</p> <pre><code>import duckdb\n# if this segfaults make sure duckdb v0.7+ is installed\nsamples = duckdb.query(\"SELECT vector FROM sift1m USING SAMPLE 100\").to_df().vector\nsamples\n</code></pre> <pre><code>0     [29.0, 10.0, 1.0, 50.0, 7.0, 89.0, 95.0, 51.0,...\n1     [7.0, 5.0, 39.0, 49.0, 17.0, 12.0, 83.0, 117.0...\n2     [0.0, 0.0, 0.0, 10.0, 12.0, 31.0, 6.0, 0.0, 0....\n3     [0.0, 2.0, 9.0, 1.793662034335766e-43, 30.0, 1...\n4     [54.0, 112.0, 16.0, 0.0, 0.0, 7.0, 112.0, 44.0...\n                            ...                        \n95    [1.793662034335766e-43, 33.0, 47.0, 28.0, 0.0,...\n96    [1.0, 4.0, 2.0, 32.0, 3.0, 7.0, 119.0, 116.0, ...\n97    [17.0, 46.0, 12.0, 0.0, 0.0, 3.0, 23.0, 58.0, ...\n98    [0.0, 11.0, 30.0, 14.0, 34.0, 7.0, 0.0, 0.0, 1...\n99    [20.0, 8.0, 121.0, 98.0, 37.0, 77.0, 9.0, 18.0...\nName: vector, Length: 100, dtype: object\n</code></pre> <p>Call nearest neighbors (no ANN index here)</p> <pre><code>import time\n\nstart = time.time()\ntbl = sift1m.to_table(columns=[\"id\"], nearest={\"column\": \"vector\", \"q\": samples[0], \"k\": 10})\nend = time.time()\n\nprint(f\"Time(sec): {end-start}\")\nprint(tbl.to_pandas())\n</code></pre> <pre><code>Time(sec): 0.10735273361206055\n       id                                             vector    score\n0  144678  [29.0, 10.0, 1.0, 50.0, 7.0, 89.0, 95.0, 51.0,...      0.0\n1  575538  [2.0, 0.0, 1.0, 42.0, 3.0, 38.0, 152.0, 27.0, ...  76908.0\n2  241428  [11.0, 0.0, 2.0, 118.0, 11.0, 108.0, 116.0, 21...  92877.0\n3  220788  [0.0, 0.0, 0.0, 95.0, 0.0, 8.0, 133.0, 67.0, 1...  93305.0\n4  833796  [1.0, 1.0, 0.0, 23.0, 11.0, 26.0, 140.0, 115.0...  95721.0\n5  919065  [1.0, 1.0, 1.0, 42.0, 96.0, 42.0, 126.0, 83.0,...  96632.0\n6  741948  [36.0, 9.0, 15.0, 108.0, 17.0, 23.0, 25.0, 55....  96927.0\n7  225303  [0.0, 0.0, 3.0, 41.0, 0.0, 2.0, 36.0, 84.0, 68...  97055.0\n8  787098  [4.0, 5.0, 7.0, 29.0, 7.0, 1.0, 9.0, 91.0, 33....  97950.0\n9  113073  [0.0, 0.0, 0.0, 64.0, 65.0, 30.0, 12.0, 33.0, ...  99572.0\n</code></pre> <p>Without the index this is scanning through the whole dataset to compute the distance. <code>&lt;br/&gt;</code></p> <p>For real-time serving we can do much better with an ANN index</p>"},{"location":"notebooks/quickstart/#build-index","title":"Build index","text":"<p>Now let\\'s build an index. Lance now supports IVF_PQ, IVF_HNSW_PQ and IVF_HNSW_SQ indexes</p> <p>NOTE If you\\'d rather not wait for index build, you can download a version with the index pre-built from here and skip the next cell</p> <pre><code>%%time\n\nsift1m.create_index(\n    \"vector\",\n    index_type=\"IVF_PQ\", # IVF_PQ, IVF_HNSW_PQ and IVF_HNSW_SQ are supported\n    num_partitions=256,  # IVF\n    num_sub_vectors=16,  # PQ\n)\n</code></pre> <pre><code>Building vector index: IVF256,PQ16\nCPU times: user 2min 23s, sys: 2.77 s, total: 2min 26s\nWall time: 22.7 s\nSample 65536 out of 1000000 to train kmeans of 128 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\n</code></pre> <p>NOTE If you\\'re trying this on your own data, make sure your vector (dimensions / num_sub_vectors) % 8 == 0, or else index creation will take much longer than expected due to SIMD misalignment</p>"},{"location":"notebooks/quickstart/#try-nearest-neighbors-again-with-ann-index","title":"Try nearest neighbors again with ANN index","text":"<p>Let\\'s look for nearest neighbors again</p> <pre><code>sift1m = lance.dataset(uri)\n</code></pre> <pre><code>import time\n\ntot = 0\nfor q in samples:\n    start = time.time()\n    tbl = sift1m.to_table(nearest={\"column\": \"vector\", \"q\": q, \"k\": 10})\n    end = time.time()\n    tot += (end - start)\n\nprint(f\"Avg(sec): {tot / len(samples)}\")\nprint(tbl.to_pandas())\n</code></pre> <pre><code>Avg(sec): 0.0009334301948547364\n       id                                             vector         score\n0  378825  [20.0, 8.0, 121.0, 98.0, 37.0, 77.0, 9.0, 18.0...  16560.197266\n1  143787  [11.0, 24.0, 122.0, 122.0, 53.0, 4.0, 0.0, 3.0...  61714.941406\n2  356895  [0.0, 14.0, 67.0, 122.0, 83.0, 23.0, 1.0, 0.0,...  64147.218750\n3  535431  [9.0, 22.0, 118.0, 118.0, 4.0, 5.0, 4.0, 4.0, ...  69092.593750\n4  308778  [1.0, 7.0, 48.0, 123.0, 73.0, 36.0, 8.0, 4.0, ...  69131.812500\n5  222477  [14.0, 73.0, 39.0, 4.0, 16.0, 94.0, 19.0, 8.0,...  69244.195312\n6  672558  [2.0, 1.0, 0.0, 11.0, 36.0, 23.0, 7.0, 10.0, 0...  70264.828125\n7  365538  [54.0, 43.0, 97.0, 59.0, 34.0, 17.0, 10.0, 15....  70273.710938\n8  659787  [10.0, 9.0, 23.0, 121.0, 38.0, 26.0, 38.0, 9.0...  70374.703125\n9  603930  [32.0, 32.0, 122.0, 122.0, 70.0, 4.0, 15.0, 12...  70583.375000\n</code></pre> <p>NOTE on performance, your actual numbers will vary by your storage. These numbers are run on local disk on an M2 Macbook Air. If you\\'re querying S3 directly, HDD, or network drives, performance will be slower.</p> <p>The latency vs recall is tunable via:</p> <ul> <li>nprobes: how many IVF partitions to search</li> <li>refine_factor: determines how many vectors are retrieved during   re-ranking</li> </ul> <pre><code>%%time\n\nsift1m.to_table(\n    nearest={\n        \"column\": \"vector\",\n        \"q\": samples[0],\n        \"k\": 10,\n        \"nprobes\": 10,\n        \"refine_factor\": 5,\n    }\n).to_pandas()\n</code></pre> <pre><code>CPU times: user 2.53 ms, sys: 3.31 ms, total: 5.84 ms\nWall time: 4.18 ms\n</code></pre> id vector score 0 144678 [29.0, 10.0, 1.0, 50.0, 7.0, 89.0, 95.0, 51.0,... 0.0 1 575538 [2.0, 0.0, 1.0, 42.0, 3.0, 38.0, 152.0, 27.0, ... 76908.0 2 241428 [11.0, 0.0, 2.0, 118.0, 11.0, 108.0, 116.0, 21... 92877.0 3 220788 [0.0, 0.0, 0.0, 95.0, 0.0, 8.0, 133.0, 67.0, 1... 93305.0 4 833796 [1.0, 1.0, 0.0, 23.0, 11.0, 26.0, 140.0, 115.0... 95721.0 5 919065 [1.0, 1.0, 1.0, 42.0, 96.0, 42.0, 126.0, 83.0,... 96632.0 6 741948 [36.0, 9.0, 15.0, 108.0, 17.0, 23.0, 25.0, 55.... 96927.0 7 225303 [0.0, 0.0, 3.0, 41.0, 0.0, 2.0, 36.0, 84.0, 68... 97055.0 8 787098 [4.0, 5.0, 7.0, 29.0, 7.0, 1.0, 9.0, 91.0, 33.... 97950.0 9 113073 [0.0, 0.0, 0.0, 64.0, 65.0, 30.0, 12.0, 33.0, ... 99572.0 <p>q =&gt; sample vector</p> <p>k =&gt; how many neighbors to return</p> <p>nprobes =&gt; how many partitions (in the coarse quantizer) to probe</p> <p>refine_factor =&gt; controls \\\"re-ranking\\\". If k=10 and refine_factor=5 then retrieve 50 nearest neighbors by ANN and re-sort using actual distances then return top 10. This improves recall without sacrificing performance too much</p> <p>NOTE the latencies above include file io as lance currently doesn\\'t hold anything in memory. Along with index building speed, creating a purely in memory version of the dataset would make the biggest impact on performance.</p>"},{"location":"notebooks/quickstart/#features-and-vector-can-be-retrieved-together","title":"Features and vector can be retrieved together","text":"<p>Usually we have other feature or metadata columns that need to be stored and fetched together. If you\\'re managing data and the index separately, you have to do a bunch of annoying plumbing to put stuff together. With Lance it\\'s a single call</p> <pre><code>tbl = sift1m.to_table()\ntbl = tbl.append_column(\"item_id\", pa.array(range(len(tbl))))\ntbl = tbl.append_column(\"revenue\", pa.array((np.random.randn(len(tbl))+5)*1000))\ntbl.to_pandas()\n</code></pre> id vector item_id revenue 0 0 [0.0, 16.0, 35.0, 5.0, 32.0, 31.0, 14.0, 10.0,... 0 5950.436925 1 1 [1.8e-43, 14.0, 35.0, 19.0, 20.0, 3.0, 1.0, 13... 1 4680.298627 2 2 [33.0, 1.8e-43, 0.0, 1.0, 5.0, 3.0, 44.0, 40.0... 2 5342.593212 3 3 [23.0, 10.0, 1.8e-43, 12.0, 47.0, 14.0, 25.0, ... 3 5080.994002 4 4 [27.0, 29.0, 21.0, 1.8e-43, 1.0, 1.0, 0.0, 0.0... 4 4977.299308 ... ... ... ... ... 999995 999995 [8.0, 9.0, 5.0, 0.0, 10.0, 39.0, 72.0, 68.0, 3... 999995 4928.768010 999996 999996 [3.0, 28.0, 55.0, 29.0, 35.0, 12.0, 1.0, 2.0, ... 999996 5056.264199 999997 999997 [0.0, 13.0, 41.0, 72.0, 40.0, 9.0, 0.0, 0.0, 0... 999997 5930.547635 999998 999998 [41.0, 121.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 24... 999998 5985.139759 999999 999999 [2.0, 4.0, 8.0, 8.0, 26.0, 72.0, 63.0, 0.0, 0.... 999999 5008.962686 <p>1000000 rows \u00d7 4 columns</p> <pre><code>sift1m = lance.write_dataset(tbl, uri, mode=\"overwrite\")\n</code></pre> <pre><code>sift1m.to_table(columns=[\"revenue\"], nearest={\"column\": \"vector\", \"q\": samples[0], \"k\": 10}).to_pandas()\n</code></pre> revenue vector score 0 2994.968781 [29.0, 10.0, 1.0, 50.0, 7.0, 89.0, 95.0, 51.0,... 0.0 1 4231.026305 [2.0, 0.0, 1.0, 42.0, 3.0, 38.0, 152.0, 27.0, ... 76908.0 2 3340.900287 [11.0, 0.0, 2.0, 118.0, 11.0, 108.0, 116.0, 21... 92877.0 3 4339.588996 [0.0, 0.0, 0.0, 95.0, 0.0, 8.0, 133.0, 67.0, 1... 93305.0 4 5141.730799 [1.0, 1.0, 0.0, 23.0, 11.0, 26.0, 140.0, 115.0... 95721.0 5 4518.194820 [1.0, 1.0, 1.0, 42.0, 96.0, 42.0, 126.0, 83.0,... 96632.0 6 3383.586889 [36.0, 9.0, 15.0, 108.0, 17.0, 23.0, 25.0, 55.... 96927.0 7 5496.905675 [0.0, 0.0, 3.0, 41.0, 0.0, 2.0, 36.0, 84.0, 68... 97055.0 8 5298.669719 [4.0, 5.0, 7.0, 29.0, 7.0, 1.0, 9.0, 91.0, 33.... 97950.0 9 6742.810395 [0.0, 0.0, 0.0, 64.0, 65.0, 30.0, 12.0, 33.0, ... 99572.0"},{"location":"notebooks/youtube_transcript_search/","title":"We\\'re going to build question and answer bot","text":"<p>That allow you to search through youtube transcripts using natural language</p> <pre><code>pip install --quiet openai tqdm ratelimiter retry datasets pylance\n</code></pre> <pre><code>[notice] A new release of pip is available: 23.0 -&gt; 23.0.1\n[notice] To update, run: pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\n</code></pre>"},{"location":"notebooks/youtube_transcript_search/#download-the-data","title":"Download the data","text":"<p>700 videos and 208619 sentences</p> <pre><code>from datasets import load_dataset\n\ndata = load_dataset('jamescalam/youtube-transcriptions', split='train')\ndata\n</code></pre> <pre><code>Found cached dataset json (/Users/changshe/.cache/huggingface/datasets/jamescalam___json/jamescalam--youtube-transcriptions-08d889f6a5386b9b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n\nDataset({\n    features: ['title', 'published', 'url', 'video_id', 'channel_id', 'id', 'text', 'start', 'end'],\n    num_rows: 208619\n})\n</code></pre> <pre><code>data.to_pandas().title.nunique()\n</code></pre> <pre><code>700\n</code></pre>"},{"location":"notebooks/youtube_transcript_search/#prepare-context","title":"Prepare context","text":"<p>Create context of 20 sentences</p> <pre><code>import numpy as np\nimport pandas as pd\n\nwindow = 20\nstride = 4\n</code></pre> <pre><code>def contextualize(raw_df, window, stride):\n    def process_video(vid):\n        # For each video, create the text rolling window\n        text = vid.text.values\n        time_end = vid[\"end\"].values\n        contexts = vid.iloc[:-window:stride, :].copy()\n        contexts[\"text\"] = [' '.join(text[start_i:start_i+window])\n                            for start_i in range(0, len(vid)-window, stride)]\n        contexts[\"end\"] = [time_end[start_i+window-1]\n                            for start_i in range(0, len(vid)-window, stride)]        \n        return contexts\n    # concat result from all videos\n    return pd.concat([process_video(vid) for _, vid in raw_df.groupby(\"title\")])\n\ndf = contextualize(data.to_pandas(), 20, 4)\n</code></pre> <pre><code>len(df)\n</code></pre> <pre><code>48935\n</code></pre>"},{"location":"notebooks/youtube_transcript_search/#create-embedding-function","title":"Create embedding function","text":"<p>We\\'ll call the OpenAI embeddings API to get embeddings</p> <pre><code>import functools\nimport openai\nimport ratelimiter\nfrom retry import retry\n\nembed_model = \"text-embedding-ada-002\"\n\n# API limit at 60/min == 1/sec\nlimiter = ratelimiter.RateLimiter(max_calls=0.9, period=1.0)\n\n# Get the embedding with retry\n@retry(tries=10, delay=1, max_delay=30, backoff=3, jitter=1)\ndef embed_func(c):    \n    rs = openai.Embedding.create(input=c, engine=embed_model)\n    return [record[\"embedding\"] for record in rs[\"data\"]]\n\nrate_limited = limiter(embed_func)\n</code></pre> <pre><code>from tqdm.auto import tqdm\nimport math\n\nopenai.api_key = \"sk-...\"\n\n# We request in batches rather than 1 embedding at a time\ndef to_batches(arr, batch_size):\n    length = len(arr)\n    def _chunker(arr):\n        for start_i in range(0, len(arr), batch_size):\n            yield arr[start_i:start_i+batch_size]\n    # add progress meter\n    yield from tqdm(_chunker(arr), total=math.ceil(length / batch_size))\n\nbatch_size = 1000\nbatches = to_batches(df.text.values.tolist(), batch_size)\nembeds = [emb for c in batches for emb in rate_limited(c)]\n</code></pre> <pre><code>{\"model_id\":\"84194f89598140b2bebbfa55c90cdcc9\",\"version_major\":2,\"version_minor\":0}\n</code></pre>"},{"location":"notebooks/youtube_transcript_search/#create-lance-dataset-to-support-vector-search","title":"Create Lance dataset to support vector search","text":"<pre><code>import lance\nimport pyarrow as pa\nfrom lance.vector import vec_to_table\n\ntable = vec_to_table(np.array(embeds))\ncombined = pa.Table.from_pandas(df).append_column(\"vector\", table[\"vector\"])\nds = lance.write_dataset(combined, \"chatbot.lance\")\n</code></pre> <pre><code>ds = ds.create_index(\"vector\",\n                     index_type=\"IVF_PQ\", \n                     num_partitions=64,  # IVF\n                     num_sub_vectors=96)  # PQ\n</code></pre> <pre><code>Building vector index: IVF64,OPQ96, metric=l2\nSample 16384 out of 48935 to train kmeans of 1536 dim, 64 clusters\n</code></pre>"},{"location":"notebooks/youtube_transcript_search/#create-and-answer-the-prompt","title":"Create and answer the prompt","text":"<pre><code>def create_prompt(query, context):\n    limit = 3750\n\n    prompt_start = (\n        \"Answer the question based on the context below.\\n\\n\"+\n        \"Context:\\n\"\n    )\n    prompt_end = (\n        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n    )\n    # append contexts until hitting limit\n    for i in range(1, len(context)):\n        if len(\"\\n\\n---\\n\\n\".join(context.text[:i])) &gt;= limit:\n            prompt = (\n                prompt_start +\n                \"\\n\\n---\\n\\n\".join(context.text[:i-1]) +\n                prompt_end\n            )\n            break\n        elif i == len(context)-1:\n            prompt = (\n                prompt_start +\n                \"\\n\\n---\\n\\n\".join(context.text) +\n                prompt_end\n            )    \n    return prompt\n</code></pre> <pre><code>def complete(prompt):\n    # query text-davinci-003\n    res = openai.Completion.create(\n        engine='text-davinci-003',\n        prompt=prompt,\n        temperature=0,\n        max_tokens=400,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0,\n        stop=None\n    )\n    return res['choices'][0]['text'].strip()\n\n# check that it works\nquery = \"who was the 12th person on the moon and when did they land?\"\ncomplete(query)\n</code></pre> <pre><code>'The 12th person on the moon was Harrison Schmitt, and he landed on December 11, 1972.'\n</code></pre> <pre><code>def answer(question):\n    emb = embed_func(query)[0]\n    context = ds.to_table(\n        nearest={\n            \"column\": \"vector\",\n            \"k\": 3,\n            \"q\": emb,\n            \"nprobes\": 20,\n            \"refine_factor\": 100\n        }).to_pandas()\n    prompt = create_prompt(question, context)\n    return complete(prompt), context.reset_index()\n</code></pre> <pre><code>from IPython.display import YouTubeVideo\n</code></pre>"},{"location":"notebooks/youtube_transcript_search/#show-the-answer-and-show-the-video-at-the-right-place","title":"Show the answer and show the video at the right place","text":"<pre><code>query = (\"Which training method should I use for sentence transformers \"\n         \"when I only have pairs of related sentences?\")\ncompletion, context = answer(query)\n\nprint(completion)\ntop_match = context.iloc[0]\nYouTubeVideo(top_match[\"url\"].split(\"/\")[-1], start=top_match[\"start\"])\n</code></pre> <pre><code>NLI with multiple negative ranking loss.\n</code></pre> <p>YouTube video thumbnail would be displayed here when running the notebook</p> <pre><code>\n</code></pre>"}]}