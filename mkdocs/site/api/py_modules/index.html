
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Modern columnar format for ML workloads">
      
      
      
        <link rel="canonical" href="https://lancedb.github.io/lance/api/py_modules/">
      
      
        <link rel="prev" href="../python/">
      
      
        <link rel="next" href="../../contributing/">
      
      
      <link rel="icon" href="../../assets/favicon_64x64.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Python Modules - Lance</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../assets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#python-api-reference" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Lance" class="md-header__button md-logo" aria-label="Lance" data-md-component="logo">
      
  <img src="../../assets/high-res-icon.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Lance
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Python Modules
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/lancedb/lance" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    lancedb/lance
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Lance" class="md-nav__button md-logo" aria-label="Lance" data-md-component="logo">
      
  <img src="../../assets/high-res-icon.png" alt="logo">

    </a>
    Lance
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/lancedb/lance" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    lancedb/lance
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Introduction
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/quickstart/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quickstart
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/youtube_transcript_search/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    YouTube Transcript Search
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../introduction/read_and_write/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Read and Write
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../introduction/schema_evolution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Schema Evolution
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Advanced Usage
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Advanced Usage
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../format/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Lance Format Spec
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../blob/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Blob API
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tags/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tags
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../object_store/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Object Store Configuration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../distributed_write/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Distributed Write
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../performance/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Performance Guide
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenizer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tokenizer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../arrays/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Extension Arrays
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Integrations
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Integrations
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../integrations/huggingface/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Huggingface
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../integrations/tensorflow/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tensorflow
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../integrations/pytorch/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PyTorch
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../integrations/ray/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Ray
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../integrations/spark/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Spark
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Examples
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/examples/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/write_read_dataset/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Write/Read Dataset
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/llm_dataset_creation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLM Dataset Creation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/llm_training/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLM Training
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/flickr8k_dataset_creation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Flickr8k Dataset Creation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/clip_training/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CLIP Training
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/artefact_management/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Artefact Management
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/spark_datasource_example/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Spark DataSource
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" checked>
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    API References
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            API References
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../python/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Python API
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Python Modules
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Python Modules
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lance_1" class="md-nav__link">
    <span class="md-ellipsis">
      lance
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lance" class="md-nav__link">
    <span class="md-ellipsis">
      lance
    </span>
  </a>
  
    <nav class="md-nav" aria-label="lance">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.__version__" class="md-nav__link">
    <span class="md-ellipsis">
      __version__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.BlobColumn" class="md-nav__link">
    <span class="md-ellipsis">
      BlobColumn
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.BlobFile" class="md-nav__link">
    <span class="md-ellipsis">
      BlobFile
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BlobFile">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.BlobFile.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.BlobFile.size" class="md-nav__link">
    <span class="md-ellipsis">
      size
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.DataStatistics" class="md-nav__link">
    <span class="md-ellipsis">
      DataStatistics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.FieldStatistics" class="md-nav__link">
    <span class="md-ellipsis">
      FieldStatistics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.FragmentMetadata" class="md-nav__link">
    <span class="md-ellipsis">
      FragmentMetadata
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FragmentMetadata">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.FragmentMetadata--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.FragmentMetadata.num_deletions" class="md-nav__link">
    <span class="md-ellipsis">
      num_deletions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.FragmentMetadata.num_rows" class="md-nav__link">
    <span class="md-ellipsis">
      num_rows
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.FragmentMetadata.to_json" class="md-nav__link">
    <span class="md-ellipsis">
      to_json
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset" class="md-nav__link">
    <span class="md-ellipsis">
      LanceDataset
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LanceDataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.data_storage_version" class="md-nav__link">
    <span class="md-ellipsis">
      data_storage_version
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.lance_schema" class="md-nav__link">
    <span class="md-ellipsis">
      lance_schema
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.latest_version" class="md-nav__link">
    <span class="md-ellipsis">
      latest_version
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.max_field_id" class="md-nav__link">
    <span class="md-ellipsis">
      max_field_id
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.partition_expression" class="md-nav__link">
    <span class="md-ellipsis">
      partition_expression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.schema" class="md-nav__link">
    <span class="md-ellipsis">
      schema
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.stats" class="md-nav__link">
    <span class="md-ellipsis">
      stats
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.tags" class="md-nav__link">
    <span class="md-ellipsis">
      tags
    </span>
  </a>
  
    <nav class="md-nav" aria-label="tags">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.tags--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.uri" class="md-nav__link">
    <span class="md-ellipsis">
      uri
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.version" class="md-nav__link">
    <span class="md-ellipsis">
      version
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.add_columns" class="md-nav__link">
    <span class="md-ellipsis">
      add_columns
    </span>
  </a>
  
    <nav class="md-nav" aria-label="add_columns">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.add_columns--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.add_columns--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.add_columns--see-also" class="md-nav__link">
    <span class="md-ellipsis">
      See Also
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.alter_columns" class="md-nav__link">
    <span class="md-ellipsis">
      alter_columns
    </span>
  </a>
  
    <nav class="md-nav" aria-label="alter_columns">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.alter_columns--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.alter_columns--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.checkout_version" class="md-nav__link">
    <span class="md-ellipsis">
      checkout_version
    </span>
  </a>
  
    <nav class="md-nav" aria-label="checkout_version">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.checkout_version--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.checkout_version--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.cleanup_old_versions" class="md-nav__link">
    <span class="md-ellipsis">
      cleanup_old_versions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="cleanup_old_versions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.cleanup_old_versions--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.commit" class="md-nav__link">
    <span class="md-ellipsis">
      commit
    </span>
  </a>
  
    <nav class="md-nav" aria-label="commit">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.commit--warnings" class="md-nav__link">
    <span class="md-ellipsis">
      Warnings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.commit--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.commit--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.commit--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.commit_batch" class="md-nav__link">
    <span class="md-ellipsis">
      commit_batch
    </span>
  </a>
  
    <nav class="md-nav" aria-label="commit_batch">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.commit_batch--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.commit_batch--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.count_rows" class="md-nav__link">
    <span class="md-ellipsis">
      count_rows
    </span>
  </a>
  
    <nav class="md-nav" aria-label="count_rows">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.count_rows--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.count_rows--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.create_index" class="md-nav__link">
    <span class="md-ellipsis">
      create_index
    </span>
  </a>
  
    <nav class="md-nav" aria-label="create_index">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.create_index--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.create_index--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.create_index--references" class="md-nav__link">
    <span class="md-ellipsis">
      References
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.create_scalar_index" class="md-nav__link">
    <span class="md-ellipsis">
      create_scalar_index
    </span>
  </a>
  
    <nav class="md-nav" aria-label="create_scalar_index">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.create_scalar_index--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.create_scalar_index--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.delete" class="md-nav__link">
    <span class="md-ellipsis">
      delete
    </span>
  </a>
  
    <nav class="md-nav" aria-label="delete">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.delete--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.delete--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.drop_columns" class="md-nav__link">
    <span class="md-ellipsis">
      drop_columns
    </span>
  </a>
  
    <nav class="md-nav" aria-label="drop_columns">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.drop_columns--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.drop_columns--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.drop_index" class="md-nav__link">
    <span class="md-ellipsis">
      drop_index
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.get_fragment" class="md-nav__link">
    <span class="md-ellipsis">
      get_fragment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.get_fragments" class="md-nav__link">
    <span class="md-ellipsis">
      get_fragments
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.head" class="md-nav__link">
    <span class="md-ellipsis">
      head
    </span>
  </a>
  
    <nav class="md-nav" aria-label="head">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.head--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.head--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.insert" class="md-nav__link">
    <span class="md-ellipsis">
      insert
    </span>
  </a>
  
    <nav class="md-nav" aria-label="insert">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.insert--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.join" class="md-nav__link">
    <span class="md-ellipsis">
      join
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.merge" class="md-nav__link">
    <span class="md-ellipsis">
      merge
    </span>
  </a>
  
    <nav class="md-nav" aria-label="merge">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.merge--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.merge--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.merge--see-also" class="md-nav__link">
    <span class="md-ellipsis">
      See Also
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.merge_insert" class="md-nav__link">
    <span class="md-ellipsis">
      merge_insert
    </span>
  </a>
  
    <nav class="md-nav" aria-label="merge_insert">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.merge_insert--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.merge_insert--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.merge_insert--perform-a-upsert-operation" class="md-nav__link">
    <span class="md-ellipsis">
      Perform a "upsert" operation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.merge_insert--perform-an-insert-if-not-exists-operation" class="md-nav__link">
    <span class="md-ellipsis">
      Perform an "insert if not exists" operation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.merge_insert--perform-an-upsert-operation-only-updating-column-a" class="md-nav__link">
    <span class="md-ellipsis">
      Perform an "upsert" operation, only updating column "a"
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.migrate_manifest_paths_v2" class="md-nav__link">
    <span class="md-ellipsis">
      migrate_manifest_paths_v2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.prewarm_index" class="md-nav__link">
    <span class="md-ellipsis">
      prewarm_index
    </span>
  </a>
  
    <nav class="md-nav" aria-label="prewarm_index">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.prewarm_index--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.replace_field_metadata" class="md-nav__link">
    <span class="md-ellipsis">
      replace_field_metadata
    </span>
  </a>
  
    <nav class="md-nav" aria-label="replace_field_metadata">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.replace_field_metadata--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.replace_schema" class="md-nav__link">
    <span class="md-ellipsis">
      replace_schema
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.replace_schema_metadata" class="md-nav__link">
    <span class="md-ellipsis">
      replace_schema_metadata
    </span>
  </a>
  
    <nav class="md-nav" aria-label="replace_schema_metadata">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.replace_schema_metadata--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.restore" class="md-nav__link">
    <span class="md-ellipsis">
      restore
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.sample" class="md-nav__link">
    <span class="md-ellipsis">
      sample
    </span>
  </a>
  
    <nav class="md-nav" aria-label="sample">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.sample--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.sample--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.scanner" class="md-nav__link">
    <span class="md-ellipsis">
      scanner
    </span>
  </a>
  
    <nav class="md-nav" aria-label="scanner">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.scanner--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.session" class="md-nav__link">
    <span class="md-ellipsis">
      session
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.take" class="md-nav__link">
    <span class="md-ellipsis">
      take
    </span>
  </a>
  
    <nav class="md-nav" aria-label="take">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.take--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.take--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.take_blobs" class="md-nav__link">
    <span class="md-ellipsis">
      take_blobs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="take_blobs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.take_blobs--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.to_batches" class="md-nav__link">
    <span class="md-ellipsis">
      to_batches
    </span>
  </a>
  
    <nav class="md-nav" aria-label="to_batches">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.to_batches--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.to_batches--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.to_table" class="md-nav__link">
    <span class="md-ellipsis">
      to_table
    </span>
  </a>
  
    <nav class="md-nav" aria-label="to_table">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.to_table--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.to_table--notes" class="md-nav__link">
    <span class="md-ellipsis">
      Notes
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.update" class="md-nav__link">
    <span class="md-ellipsis">
      update
    </span>
  </a>
  
    <nav class="md-nav" aria-label="update">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.update--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.update--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.update--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.validate" class="md-nav__link">
    <span class="md-ellipsis">
      validate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.versions" class="md-nav__link">
    <span class="md-ellipsis">
      versions
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment" class="md-nav__link">
    <span class="md-ellipsis">
      LanceFragment
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LanceFragment">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.metadata" class="md-nav__link">
    <span class="md-ellipsis">
      metadata
    </span>
  </a>
  
    <nav class="md-nav" aria-label="metadata">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.metadata--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.num_deletions" class="md-nav__link">
    <span class="md-ellipsis">
      num_deletions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.physical_rows" class="md-nav__link">
    <span class="md-ellipsis">
      physical_rows
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.schema" class="md-nav__link">
    <span class="md-ellipsis">
      schema
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.create" class="md-nav__link">
    <span class="md-ellipsis">
      create
    </span>
  </a>
  
    <nav class="md-nav" aria-label="create">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.create--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.create--see-also" class="md-nav__link">
    <span class="md-ellipsis">
      See Also
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.create--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.create_from_file" class="md-nav__link">
    <span class="md-ellipsis">
      create_from_file
    </span>
  </a>
  
    <nav class="md-nav" aria-label="create_from_file">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.create_from_file--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.data_files" class="md-nav__link">
    <span class="md-ellipsis">
      data_files
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.delete" class="md-nav__link">
    <span class="md-ellipsis">
      delete
    </span>
  </a>
  
    <nav class="md-nav" aria-label="delete">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.delete--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.delete--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.delete--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.delete--see-also" class="md-nav__link">
    <span class="md-ellipsis">
      See Also
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.deletion_file" class="md-nav__link">
    <span class="md-ellipsis">
      deletion_file
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.merge" class="md-nav__link">
    <span class="md-ellipsis">
      merge
    </span>
  </a>
  
    <nav class="md-nav" aria-label="merge">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.merge--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.merge--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.merge--see-also" class="md-nav__link">
    <span class="md-ellipsis">
      See Also
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.merge--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.merge_columns" class="md-nav__link">
    <span class="md-ellipsis">
      merge_columns
    </span>
  </a>
  
    <nav class="md-nav" aria-label="merge_columns">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.merge_columns--see-also" class="md-nav__link">
    <span class="md-ellipsis">
      See Also
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.merge_columns--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.scanner" class="md-nav__link">
    <span class="md-ellipsis">
      scanner
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation" class="md-nav__link">
    <span class="md-ellipsis">
      LanceOperation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LanceOperation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Append" class="md-nav__link">
    <span class="md-ellipsis">
      Append
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Append">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Append--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Append--warning" class="md-nav__link">
    <span class="md-ellipsis">
      Warning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Append--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.BaseOperation" class="md-nav__link">
    <span class="md-ellipsis">
      BaseOperation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.CreateIndex" class="md-nav__link">
    <span class="md-ellipsis">
      CreateIndex
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.DataReplacement" class="md-nav__link">
    <span class="md-ellipsis">
      DataReplacement
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.DataReplacementGroup" class="md-nav__link">
    <span class="md-ellipsis">
      DataReplacementGroup
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Delete" class="md-nav__link">
    <span class="md-ellipsis">
      Delete
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Delete">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Delete--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Delete--warning" class="md-nav__link">
    <span class="md-ellipsis">
      Warning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Delete--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Merge" class="md-nav__link">
    <span class="md-ellipsis">
      Merge
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Merge">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Merge--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Merge--warning" class="md-nav__link">
    <span class="md-ellipsis">
      Warning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Merge--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Overwrite" class="md-nav__link">
    <span class="md-ellipsis">
      Overwrite
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Overwrite">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Overwrite--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Overwrite--warning" class="md-nav__link">
    <span class="md-ellipsis">
      Warning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Overwrite--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Project" class="md-nav__link">
    <span class="md-ellipsis">
      Project
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Project">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Project--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Project--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Project--rename-column-b-into-b0-and-rename-b1-into-b" class="md-nav__link">
    <span class="md-ellipsis">
      rename column lance &para; __version__ = &#39;0.27.3&#39; module &para; str(object='') -&gt; str str(bytes_or_buffer[, encoding[, errors]]) -&gt; str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object.str() (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. BlobColumn &para; A utility to wrap a Pyarrow binary column and iterate over the rows as file-like objects. This can be useful for working with medium-to-small binary objects that need to interface with APIs that expect file-like objects. For very large binary objects (4-8MB or more per value) you might be better off creating a blob column and using meth:lance.Dataset.take_blobs to access the blob data. BlobFile &para; Bases: RawIOBase Represents a blob in a Lance dataset as a file-like object. __init__(inner) &para; Internal only: To obtain a BlobFile use meth:lance.dataset.Dataset.take_blobs. size() &para; Returns the size of the blob in bytes. DataStatistics dataclass &para; Statistics about the data in the dataset FieldStatistics dataclass &para; Statistics about a field in the dataset FragmentMetadata dataclass &para; Metadata for a fragment. Attributes&para; id : int The ID of the fragment. files : List[DataFile] The data files of the fragment. Each data file must have the same number of rows. Each file stores a different subset of the columns. physical_rows : int The number of rows originally in this fragment. This is the number of rows in the data files before deletions. deletion_file : Optional[DeletionFile] The deletion file, if any. row_id_meta : Optional[RowIdMeta] The row id metadata, if any. num_deletions property &para; The number of rows that have been deleted from this fragment. num_rows property &para; The number of rows in this fragment after deletions. to_json() &para; Get this as a simple JSON-serializable dictionary. LanceDataset &para; Bases: Dataset A Lance Dataset in Lance format where the data is stored at the given uri. data_storage_version property &para; The version of the data storage format this dataset is using lance_schema property &para; The LanceSchema for this dataset latest_version property &para; Returns the latest version of the dataset. max_field_id property &para; The max_field_id in manifest partition_expression property &para; Not implemented (just override pyarrow dataset to prevent segfault) schema property &para; The pyarrow Schema for this dataset stats property &para; Experimental API tags property &para; Tag management for the dataset. Similar to Git, tags are a way to add metadata to a specific version of the dataset. .. warning:: Tagged versions are exempted from the :py:meth:`cleanup_old_versions()` process. To remove a version that has been tagged, you must first :py:meth:`~Tags.delete` the associated tag. Examples&para; .. code-block:: python ds = lance.open(&quot;dataset.lance&quot;) ds.tags.create(&quot;v2-prod-20250203&quot;, 10) tags = ds.tags.list() uri property &para; The location of the data version property &para; Returns the currently checked out version of the dataset add_columns(transforms, read_columns=None, reader_schema=None, batch_size=None) &para; Add new columns with defined values. There are several ways to specify the new columns. First, you can provide SQL expressions for each new column. Second you can provide a UDF that takes a batch of existing data and returns a new batch with the new columns. These new columns will be appended to the dataset. You can also provide a RecordBatchReader which will read the new column values from some external source. This is often useful when the new column values have already been staged to files (often by some distributed process) See the :func:lance.add_columns_udf decorator for more information on writing UDFs. Parameters&para; transforms : dict or AddColumnsUDF or ReaderLike If this is a dictionary, then the keys are the names of the new columns and the values are SQL expression strings. These strings can reference existing columns in the dataset. If this is a AddColumnsUDF, then it is a UDF that takes a batch of existing data and returns a new batch with the new columns. If this is :class:pyarrow.Field or :class:pyarrow.Schema, it adds all NULL columns with the given schema, in a metadata-only operation. read_columns : list of str, optional The names of the columns that the UDF will read. If None, then the UDF will read all columns. This is only used when transforms is a UDF. Otherwise, the read columns are inferred from the SQL expressions. reader_schema: pa.Schema, optional Only valid if transforms is a ReaderLike object. This will be used to determine the schema of the reader. batch_size: int, optional The number of rows to read at a time from the source dataset when applying the transform. This is ignored if the dataset is a v1 dataset. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3]}) dataset = lance.write_dataset(table, "my_dataset") @lance.batch_udf() ... def double_a(batch): ... df = batch.to_pandas() ... return pd.DataFrame({'double_a': 2 * df['a']}) dataset.add_columns(double_a) dataset.to_table().to_pandas() a double_a 0 1 2 1 2 4 2 3 6 dataset.add_columns({"triple_a": "a * 3"}) dataset.to_table().to_pandas() a double_a triple_a 0 1 2 3 1 2 4 6 2 3 6 9 See Also&para; LanceDataset.merge : Merge a pre-computed set of columns into the dataset. alter_columns(*alterations) &para; Alter column name, data type, and nullability. Columns that are renamed can keep any indices that are on them. If a column has an IVF_PQ index, it can be kept if the column is casted to another type. However, other index types don't support casting at this time. Column types can be upcasted (such as int32 to int64) or downcasted (such as int64 to int32). However, downcasting will fail if there are any values that cannot be represented in the new type. In general, columns can be casted to same general type: integers to integers, floats to floats, and strings to strings. However, strings, binary, and list columns can be casted between their size variants. For example, string to large string, binary to large binary, and list to large list. Columns that are renamed can keep any indices that are on them. However, if the column is casted to a different type, its indices will be dropped. Parameters&para; alterations : Iterable[Dict[str, Any]] A sequence of dictionaries, each with the following keys: - &quot;path&quot;: str The column path to alter. For a top-level column, this is the name. For a nested column, this is the dot-separated path, e.g. &quot;a.b.c&quot;. - &quot;name&quot;: str, optional The new name of the column. If not specified, the column name is not changed. - &quot;nullable&quot;: bool, optional Whether the column should be nullable. If not specified, the column nullability is not changed. Only non-nullable columns can be changed to nullable. Currently, you cannot change a nullable column to non-nullable. - &quot;data_type&quot;: pyarrow.DataType, optional The new data type to cast the column to. If not specified, the column data type is not changed. Examples&para; import lance import pyarrow as pa schema = pa.schema([pa.field('a', pa.int64()), ... pa.field('b', pa.string(), nullable=False)]) table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.alter_columns({"path": "a", "name": "x"}, ... {"path": "b", "nullable": True}) dataset.to_table().to_pandas() x b 0 1 a 1 2 b 2 3 c dataset.alter_columns({"path": "x", "data_type": pa.int32()}) dataset.schema x: int32 b: string checkout_version(version) &para; Load the given version of the dataset. Unlike the :func:dataset constructor, this will re-use the current cache. This is a no-op if the dataset is already at the given version. Parameters&para; version: int | str, The version to check out. A version number (int) or a tag (str) can be provided. Returns&para; LanceDataset cleanup_old_versions(older_than=None, *, delete_unverified=False, error_if_tagged_old_versions=True) &para; Cleans up old versions of the dataset. Some dataset changes, such as overwriting, leave behind data that is not referenced by the latest dataset version. The old data is left in place to allow the dataset to be restored back to an older version. This method will remove older versions and any data files they reference. Once this cleanup task has run you will not be able to checkout or restore these older versions. Parameters&para; timedelta, optional Only versions older than this will be removed. If not specified, this will default to two weeks. bool, default False Files leftover from a failed transaction may appear to be part of an in-progress operation (e.g. appending new data) and these files will not be deleted unless they are at least 7 days old. If delete_unverified is True then these files will be deleted regardless of their age. This should only be set to True if you can guarantee that no other process is currently working on this dataset. Otherwise the dataset could be put into a corrupted state. bool, default True Some versions may have tags associated with them. Tagged versions will not be cleaned up, regardless of how old they are. If this argument is set to True (the default), an exception will be raised if any tagged versions match the parameters. Otherwise, tagged versions will be ignored without any error and only untagged versions will be cleaned up. commit(base_uri, operation, blobs_op=None, read_version=None, commit_lock=None, storage_options=None, enable_v2_manifest_paths=None, detached=False, max_retries=20) staticmethod &para; Create a new version of dataset This method is an advanced method which allows users to describe a change that has been made to the data files. This method is not needed when using Lance to apply changes (e.g. when using class:LanceDataset or func:write_dataset.) It's current purpose is to allow for changes being made in a distributed environment where no single process is doing all of the work. For example, a distributed bulk update or a distributed bulk modify operation. Once all of the changes have been made, this method can be called to make the changes visible by updating the dataset manifest. Warnings&para; This is an advanced API and doesn't provide the same level of validation as the other APIs. For example, it's the responsibility of the caller to ensure that the fragments are valid for the schema. Parameters&para; base_uri: str, Path, or LanceDataset The base uri of the dataset, or the dataset object itself. Using the dataset object can be more efficient because it can re-use the file metadata cache. operation: BaseOperation The operation to apply to the dataset. This describes what changes have been made. See available operations under :class:LanceOperation. read_version: int, optional The version of the dataset that was used as the base for the changes. This is not needed for overwrite or restore operations. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:migrate_manifest_paths_v2 method. Default is False. WARNING: turning this on will make the dataset unreadable for older versions of Lance (prior to 0.17.0). detached : bool, optional If True, then the commit will not be part of the dataset lineage. It will never show up as the latest dataset and the only way to check it out in the future will be to specifically check it out by version. The version will be a random version that is only unique amongst detached commits. The caller should store this somewhere as there will be no other way to obtain it in the future. max_retries : int The maximum number of retries to perform when committing the dataset. Returns&para; LanceDataset A new version of Lance Dataset. Examples&para; Creating a new dataset with the :class:LanceOperation.Overwrite operation: import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment1 = lance.fragment.LanceFragment.create("example", tab1) fragment2 = lance.fragment.LanceFragment.create("example", tab2) fragments = [fragment1, fragment2] operation = lance.LanceOperation.Overwrite(tab1.schema, fragments) dataset = lance.LanceDataset.commit("example", operation) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d commit_batch(dest, transactions, commit_lock=None, storage_options=None, enable_v2_manifest_paths=None, detached=False, max_retries=20) staticmethod &para; Create a new version of dataset with multiple transactions. This method is an advanced method which allows users to describe a change that has been made to the data files. This method is not needed when using Lance to apply changes (e.g. when using class:LanceDataset or func:write_dataset.) Parameters&para; dest: str, Path, or LanceDataset The base uri of the dataset, or the dataset object itself. Using the dataset object can be more efficient because it can re-use the file metadata cache. transactions: Iterable[Transaction] The transactions to apply to the dataset. These will be merged into a single transaction and applied to the dataset. Note: Only append transactions are currently supported. Other transaction types will be supported in the future. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:migrate_manifest_paths_v2 method. Default is False. WARNING: turning this on will make the dataset unreadable for older versions of Lance (prior to 0.17.0). detached : bool, optional If True, then the commit will not be part of the dataset lineage. It will never show up as the latest dataset and the only way to check it out in the future will be to specifically check it out by version. The version will be a random version that is only unique amongst detached commits. The caller should store this somewhere as there will be no other way to obtain it in the future. max_retries : int The maximum number of retries to perform when committing the dataset. Returns&para; dict with keys: dataset: LanceDataset A new version of Lance Dataset. merged: Transaction The merged transaction that was applied to the dataset. count_rows(filter=None, **kwargs) &para; Count rows matching the scanner filter. Parameters&para; **kwargs : dict, optional See py:method:scanner method for full parameter description. Returns&para; count : int The total number of rows in the dataset. create_index(column, index_type, name=None, metric=&#39;L2&#39;, replace=False, num_partitions=None, ivf_centroids=None, pq_codebook=None, num_sub_vectors=None, accelerator=None, index_cache_size=None, shuffle_partition_batches=None, shuffle_partition_concurrency=None, ivf_centroids_file=None, precomputed_partition_dataset=None, storage_options=None, filter_nan=True, one_pass_ivfpq=False, **kwargs) &para; Create index on column. Experimental API Parameters&para; column : str The column to be indexed. index_type : str The type of the index. "IVF_PQ, IVF_HNSW_PQ and IVF_HNSW_SQ" are supported now. name : str, optional The index name. If not provided, it will be generated from the column name. metric : str The distance metric type, i.e., "L2" (alias to "euclidean"), "cosine" or "dot" (dot product). Default is "L2". replace : bool Replace the existing index if it exists. num_partitions : int, optional The number of partitions of IVF (Inverted File Index). ivf_centroids : optional It can be either class:np.ndarray, class:pyarrow.FixedSizeListArray or class:pyarrow.FixedShapeTensorArray. A num_partitions x dimension array of existing K-mean centroids for IVF clustering. If not provided, a new KMeans model will be trained. pq_codebook : optional, It can be class:np.ndarray, class:pyarrow.FixedSizeListArray, or class:pyarrow.FixedShapeTensorArray. A num_sub_vectors x (2 ^ nbits * dimensions // num_sub_vectors) array of K-mean centroids for PQ codebook. Note: ``nbits`` is always 8 for now. If not provided, a new PQ model will be trained. num_sub_vectors : int, optional The number of sub-vectors for PQ (Product Quantization). accelerator : str or torch.Device, optional If set, use an accelerator to speed up the training process. Accepted accelerator: "cuda" (Nvidia GPU) and "mps" (Apple Silicon GPU). If not set, use the CPU. index_cache_size : int, optional The size of the index cache in number of entries. Default value is 256. shuffle_partition_batches : int, optional The number of batches, using the row group size of the dataset, to include in each shuffle partition. Default value is 10240. Assuming the row group size is 1024, each shuffle partition will hold 10240 * 1024 = 10,485,760 rows. By making this value smaller, this shuffle will consume less memory but will take longer to complete, and vice versa. shuffle_partition_concurrency : int, optional The number of shuffle partitions to process concurrently. Default value is 2 By making this value smaller, this shuffle will consume less memory but will take longer to complete, and vice versa. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. filter_nan: bool Defaults to True. False is UNSAFE, and will cause a crash if any null/nan values are present (and otherwise will not). Disables the null filter used for nullable columns. Obtains a small speed boost. one_pass_ivfpq: bool Defaults to False. If enabled, index type must be "IVF_PQ". Reduces disk IO. kwargs : Parameters passed to the index building process. The SQ (Scalar Quantization) is available for only IVF_HNSW_SQ index type, this quantization method is used to reduce the memory usage of the index, it maps the float vectors to integer vectors, each integer is of num_bits, now only 8 bits are supported. If index_type is "IVF_*", then the following parameters are required: num_partitions If index_type is with "PQ", then the following parameters are required: num_sub_vectors Optional parameters for IVF_PQ: - ivf_centroids Existing K-mean centroids for IVF clustering. - num_bits The number of bits for PQ (Product Quantization). Default is 8. Only 4, 8 are supported. Optional parameters for IVF_HNSW_*: max_level Int, the maximum number of levels in the graph. m Int, the number of edges per node in the graph. ef_construction Int, the number of nodes to examine during the construction. Examples&para; .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_PQ&quot;, num_partitions=256, num_sub_vectors=16 ) .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_HNSW_SQ&quot;, num_partitions=256, ) Experimental Accelerator (GPU) support: accelerate: use GPU to train IVF partitions. Only supports CUDA (Nvidia) or MPS (Apple) currently. Requires PyTorch being installed. .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_PQ&quot;, num_partitions=256, num_sub_vectors=16, accelerator=&quot;cuda&quot; ) References&para; Faiss Index &lt;https://github.com/facebookresearch/faiss/wiki/Faiss-indexes&gt;_ IVF introduced in Video Google: a text retrieval approach to object matching in videos &lt;https://ieeexplore.ieee.org/abstract/document/1238663&gt;_ Product quantization for nearest neighbor search &lt;https://hal.inria.fr/inria-00514462v2/document&gt;_ create_scalar_index(column, index_type, name=None, *, replace=True, **kwargs) &para; Create a scalar index on a column. Scalar indices, like vector indices, can be used to speed up scans. A scalar index can speed up scans that contain filter expressions on the indexed column. For example, the following scan will be faster if the column my_col has a scalar index: .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) my_table = dataset.scanner(filter=&quot;my_col != 7&quot;).to_table() Vector search with pre-filers can also benefit from scalar indices. For example, .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) my_table = dataset.scanner( nearest=dict( column=&quot;vector&quot;, q=[1, 2, 3, 4], k=10, ) filter=&quot;my_col != 7&quot;, prefilter=True ) There are 5 types of scalar indices available today. BTREE. The most common type is BTREE. This index is inspired by the btree data structure although only the first few layers of the btree are cached in memory. It will perform well on columns with a large number of unique values and few rows per value. BITMAP. This index stores a bitmap for each unique value in the column. This index is useful for columns with a small number of unique values and many rows per value. LABEL_LIST. A special index that is used to index list columns whose values have small cardinality. For example, a column that contains lists of tags (e.g. ["tag1", "tag2", "tag3"]) can be indexed with a LABEL_LIST index. This index can only speedup queries with array_has_any or array_has_all filters. NGRAM. A special index that is used to index string columns. This index creates a bitmap for each ngram in the string. By default we use trigrams. This index can currently speed up queries using the contains function in filters. FTS/INVERTED. It is used to index document columns. This index can conduct full-text searches. For example, a column that contains any word of query string "hello world". The results will be ranked by BM25. Note that the LANCE_BYPASS_SPILLING environment variable can be used to bypass spilling to disk. Setting this to true can avoid memory exhaustion issues (see https://github.com/apache/datafusion/issues/10073 for more info). Experimental API Parameters&para; column : str The column to be indexed. Must be a boolean, integer, float, or string column. index_type : str The type of the index. One of "BTREE", "BITMAP", "LABEL_LIST", "NGRAM", "FTS" or "INVERTED". name : str, optional The index name. If not provided, it will be generated from the column name. replace : bool, default True Replace the existing index if it exists. bool, default True This is for the INVERTED index. If True, the index will store the positions of the words in the document, so that you can conduct phrase query. This will significantly increase the index size. It won't impact the performance of non-phrase queries even if it is set to True. base_tokenizer: str, default "simple" This is for the INVERTED index. The base tokenizer to use. The value can be: * "simple": splits tokens on whitespace and punctuation. * "whitespace": splits tokens on whitespace. * "raw": no tokenization. language: str, default "English" This is for the INVERTED index. The language for stemming and stop words. This is only used when stem or remove_stop_words is true max_token_length: Optional[int], default 40 This is for the INVERTED index. The maximum token length. Any token longer than this will be removed. lower_case: bool, default True This is for the INVERTED index. If True, the index will convert all text to lowercase. stem: bool, default False This is for the INVERTED index. If True, the index will stem the tokens. remove_stop_words: bool, default False This is for the INVERTED index. If True, the index will remove stop words. ascii_folding: bool, default False This is for the INVERTED index. If True, the index will convert non-ascii characters to ascii characters if possible. This would remove accents like "é" -&gt; "e". Examples&para; .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) dataset.create_index( &quot;category&quot;, &quot;BTREE&quot;, ) Scalar indices can only speed up scans for basic filters using equality, comparison, range (e.g. my_col BETWEEN 0 AND 100), and set membership (e.g. my_col IN (0, 1, 2)) Scalar indices can be used if the filter contains multiple indexed columns and the filter criteria are AND'd or OR'd together (e.g. my_col &lt; 0 AND other_col&gt; 100) Scalar indices may be used if the filter contains non-indexed columns but, depending on the structure of the filter, they may not be usable. For example, if the column not_indexed does not have a scalar index then the filter my_col = 0 OR not_indexed = 1 will not be able to use any scalar index on my_col. To determine if a scan is making use of a scalar index you can use explain_plan to look at the query plan that lance has created. Queries that use scalar indices will either have a ScalarIndexQuery relation or a MaterializeIndex operator. delete(predicate) &para; Delete rows from the dataset. This marks rows as deleted, but does not physically remove them from the files. This keeps the existing indexes still valid. Parameters&para; predicate : str or pa.compute.Expression The predicate to use to select rows to delete. May either be a SQL string or a pyarrow Expression. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.delete("a = 1 or b in ('a', 'b')") dataset.to_table() pyarrow.Table a: int64 b: string a: [[3]] b: [["c"]] drop_columns(columns) &para; Drop one or more columns from the dataset This is a metadata-only operation and does not remove the data from the underlying storage. In order to remove the data, you must subsequently call compact_files to rewrite the data without the removed columns and then call cleanup_old_versions to remove the old files. Parameters&para; columns : list of str The names of the columns to drop. These can be nested column references (e.g. "a.b.c") or top-level column names (e.g. "a"). Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.drop_columns(["a"]) dataset.to_table().to_pandas() b 0 a 1 b 2 c drop_index(name) &para; Drops an index from the dataset Note: Indices are dropped by "index name". This is not the same as the field name. If you did not specify a name when you created the index then a name was generated for you. You can use the list_indices method to get the names of the indices. get_fragment(fragment_id) &para; Get the fragment with fragment id. get_fragments(filter=None) &para; Get all fragments from the dataset. Note: filter is not supported yet. head(num_rows, **kwargs) &para; Load the first N rows of the dataset. Parameters&para; num_rows : int The number of rows to load. **kwargs : dict, optional See scanner() method for full parameter description. Returns&para; table : Table insert(data, *, mode=&#39;append&#39;, **kwargs) &para; Insert data into the dataset. Parameters&para; data_obj: Reader-like The data to be written. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader - Huggingface dataset mode: str, default 'append' The mode to use when writing the data. Options are: create - create a new dataset (raises if uri already exists). overwrite - create a new snapshot version append - create a new version that is the concat of the input the latest version (raises if uri does not exist) **kwargs : dict, optional Additional keyword arguments to pass to :func:write_dataset. join(right_dataset, keys, right_keys=None, join_type=&#39;left outer&#39;, left_suffix=None, right_suffix=None, coalesce_keys=True, use_threads=True) &para; Not implemented (just override pyarrow dataset to prevent segfault) merge(data_obj, left_on, right_on=None, schema=None) &para; Merge another dataset into this one. Performs a left join, where the dataset is the left side and data_obj is the right side. Rows existing in the dataset but not on the left will be filled with null values, unless Lance doesn't support null values for some types, in which case an error will be raised. Parameters&para; data_obj: Reader-like The data to be merged. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, Iterator[RecordBatch], or RecordBatchReader left_on: str The name of the column in the dataset to join on. right_on: str or None The name of the column in data_obj to join on. If None, defaults to left_on. Examples&para; import lance import pyarrow as pa df = pa.table({'x': [1, 2, 3], 'y': ['a', 'b', 'c']}) dataset = lance.write_dataset(df, "dataset") dataset.to_table().to_pandas() x y 0 1 a 1 2 b 2 3 c new_df = pa.table({'x': [1, 2, 3], 'z': ['d', 'e', 'f']}) dataset.merge(new_df, 'x') dataset.to_table().to_pandas() x y z 0 1 a d 1 2 b e 2 3 c f See Also&para; LanceDataset.add_columns : Add new columns by computing batch-by-batch. merge_insert(on) &para; Returns a builder that can be used to create a "merge insert" operation This operation can add rows, update rows, and remove rows in a single transaction. It is a very generic tool that can be used to create behaviors like "insert if not exists", "update or insert (i.e. upsert)", or even replace a portion of existing data with new data (e.g. replace all data where month="january") The merge insert operation works by combining new data from a source table with existing data in a target table by using a join. There are three categories of records. "Matched" records are records that exist in both the source table and the target table. "Not matched" records exist only in the source table (e.g. these are new data). "Not matched by source" records exist only in the target table (this is old data). The builder returned by this method can be used to customize what should happen for each category of data. Please note that the data will be reordered as part of this operation. This is because updated rows will be deleted from the dataset and then reinserted at the end with the new values. The order of the newly inserted rows may fluctuate randomly because a hash-join operation is used internally. Parameters&para; Union[str, Iterable[str]] A column (or columns) to join on. This is how records from the source table and target table are matched. Typically this is some kind of key or id column. Examples&para; Use when_matched_update_all() and when_not_matched_insert_all() to perform an "upsert" operation. This will update rows that already exist in the dataset and insert rows that do not exist. import lance import pyarrow as pa table = pa.table({"a": [2, 1, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform a "upsert" operation&para; dataset.merge_insert("a") \ ... .when_matched_update_all() \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b 0 1 b 1 2 x 2 3 y 3 4 z Use when_not_matched_insert_all() to perform an "insert if not exists" operation. This will only insert rows that do not already exist in the dataset. import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example2") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform an "insert if not exists" operation&para; dataset.merge_insert("a") \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 0, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 z You are not required to provide all the columns. If you only want to update a subset of columns, you can omit columns you don't want to update. Omitted columns will keep their existing values if they are updated, or will be null if they are inserted. import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"], \ ... "c": ["x", "y", "z"]}) dataset = lance.write_dataset(table, "example3") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform an "upsert" operation, only updating column "a"&para; dataset.merge_insert("a") \ ... .when_matched_update_all() \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b c 0 1 a x 1 2 x y 2 3 y z 3 4 z None migrate_manifest_paths_v2() &para; Migrate the manifest paths to the new format. This will update the manifest to use the new v2 format for paths. This function is idempotent, and can be run multiple times without changing the state of the object store. DANGER: this should not be run while other concurrent operations are happening. And it should also run until completion before resuming other operations. prewarm_index(name) &para; Prewarm an index This will load the entire index into memory. This can help avoid cold start issues with index queries. If the index does not fit in the index cache, then this will result in wasted I/O. Parameters&para; name: str The name of the index to prewarm. replace_field_metadata(field_name, new_metadata) &para; Replace the metadata of a field in the schema Parameters&para; field_name: str The name of the field to replace the metadata for new_metadata: dict The new metadata to set replace_schema(schema) &para; Not implemented (just override pyarrow dataset to prevent segfault) See method:replace_schema_metadata or method:replace_field_metadata replace_schema_metadata(new_metadata) &para; Replace the schema metadata of the dataset Parameters&para; new_metadata: dict The new metadata to set restore() &para; Restore the currently checked out version as the latest version of the dataset. This creates a new commit. sample(num_rows, columns=None, randomize_order=True, **kwargs) &para; Select a random sample of data Parameters&para; num_rows: int number of rows to retrieve columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. **kwargs : dict, optional see scanner() method for full parameter description. Returns&para; table : Table scanner(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, fragments=None, full_text_query=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, fast_search=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, include_deleted_rows=None, scan_stats_callback=None, strict_batch_size=None) &para; Return a Scanner that can support various pushdowns. Parameters&para; columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. filter: pa.compute.Expression or str Expression or str that is a valid SQL where clause. See Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;_ for valid SQL expressions. limit: int, default None Fetch up to this many rows. All rows if None or unspecified. offset: int, default None Fetch starting with this row. 0 if None or unspecified. nearest: dict, default None Get the rows corresponding to the K most similar vectors. Example: .. code-block:: python { &quot;column&quot;: &lt;embedding col name&gt;, &quot;q&quot;: &lt;query vector as pa.Float32Array&gt;, &quot;k&quot;: 10, &quot;nprobes&quot;: 1, &quot;refine_factor&quot;: 1 } int, default None The target size of batches returned. In some cases batches can be up to twice this size (but never larger than this). In some cases batches can be smaller than this size. io_buffer_size: int, default None The size of the IO buffer. See ScannerBuilder.io_buffer_size for more information. batch_readahead: int, optional The number of batches to read ahead. fragment_readahead: int, optional The number of fragments to read ahead. scan_in_order: bool, default True Whether to read the fragments and batches in order. If false, throughput may be higher, but batches will be returned out of order and memory use might increase. fragments: iterable of LanceFragment, default None If specified, only scan these fragments. If scan_in_order is True, then the fragments will be scanned in the order given. prefilter: bool, default False If True then the filter will be applied before the vector query is run. This will generate more correct results but it may be a more costly query. It's generally good when the filter is highly selective. If False then the filter will be applied after the vector query is run. This will perform well but the results may have fewer than the requested number of rows (or be empty) if the rows closest to the query do not match the filter. It&#39;s generally good when the filter is not very selective. use_scalar_index: bool, default True Lance will automatically use scalar indices to optimize a query. In some corner cases this can make query performance worse and this parameter can be used to disable scalar indices in these cases. late_materialization: bool or List[str], default None Allows custom control over late materialization. Late materialization fetches non-query columns using a take operation after the filter. This is useful when there are few results or columns are very large. Early materialization can be better when there are many results or the columns are very narrow. If True, then all columns are late materialized. If False, then all columns are early materialized. If a list of strings, then only the columns in the list are late materialized. The default uses a heuristic that assumes filters will select about 0.1% of the rows. If your filter is more selective (e.g. find by id) you may want to set this to True. If your filter is not very selective (e.g. matches 20% of the rows) you may want to set this to False. full_text_query: str or dict, optional query string to search for, the results will be ranked by BM25. e.g. "hello world", would match documents containing "hello" or "world". or a dictionary with the following keys: - columns: list[str] The columns to search, currently only supports a single column in the columns list. - query: str The query string to search for. fast_search: bool, default False If True, then the search will only be performed on the indexed data, which yields faster search time. scan_stats_callback: Callable[[ScanStatistics], None], default None A callback function that will be called with the scan statistics after the scan is complete. Errors raised by the callback will be logged but not re-raised. include_deleted_rows: bool, default False If True, then rows that have been deleted, but are still present in the fragment, will be returned. These rows will have the _rowid column set to null. All other columns will reflect the value stored on disk and may not be null. Note: if this is a search operation, or a take operation (including scalar indexed scans) then deleted rows cannot be returned. .. note:: For now, if BOTH filter and nearest is specified, then: 1. nearest is executed first. 2. The results are filtered afterwards. For debugging ANN results, you can choose to not use the index even if present by specifying use_index=False. For example, the following will always return exact KNN results: .. code-block:: python dataset.to_table(nearest={ &quot;column&quot;: &quot;vector&quot;, &quot;k&quot;: 10, &quot;q&quot;: &lt;query vector&gt;, &quot;use_index&quot;: False } session() &para; Return the dataset session, which holds the dataset's state. take(indices, columns=None) &para; Select rows of data by index. Parameters&para; indices : Array or array-like indices of rows to select in the dataset. columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. Returns&para; table : pyarrow.Table take_blobs(blob_column, ids=None, addresses=None, indices=None) &para; Select blobs by row IDs. Instead of loading large binary blob data into memory before processing it, this API allows you to open binary blob data as a regular Python file-like object. For more details, see class:lance.BlobFile. Exactly one of ids, addresses, or indices must be specified. Parameters blob_column : str The name of the blob column to select. ids : Integer Array or array-like row IDs to select in the dataset. addresses: Integer Array or array-like The (unstable) row addresses to select in the dataset. indices : Integer Array or array-like The offset / indices of the row in the dataset. Returns&para; blob_files : List[BlobFile] to_batches(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, full_text_query=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, strict_batch_size=None, **kwargs) &para; Read the dataset as materialized record batches. Parameters&para; **kwargs : dict, optional Arguments for meth:~LanceDataset.scanner. Returns&para; record_batches : Iterator of class:~pyarrow.RecordBatch to_table(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, fast_search=None, full_text_query=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, include_deleted_rows=None) &para; Read the data into memory as a class:pyarrow.Table Parameters&para; columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. filter : pa.compute.Expression or str Expression or str that is a valid SQL where clause. See Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;_ for valid SQL expressions. limit: int, default None Fetch up to this many rows. All rows if None or unspecified. offset: int, default None Fetch starting with this row. 0 if None or unspecified. nearest: dict, default None Get the rows corresponding to the K most similar vectors. Example: .. code-block:: python { &quot;column&quot;: &lt;embedding col name&gt;, &quot;q&quot;: &lt;query vector as pa.Float32Array&gt;, &quot;k&quot;: 10, &quot;metric&quot;: &quot;cosine&quot;, &quot;nprobes&quot;: 1, &quot;refine_factor&quot;: 1 } int, optional The number of rows to read at a time. io_buffer_size: int, default None The size of the IO buffer. See ScannerBuilder.io_buffer_size for more information. batch_readahead: int, optional The number of batches to read ahead. fragment_readahead: int, optional The number of fragments to read ahead. scan_in_order: bool, optional, default True Whether to read the fragments and batches in order. If false, throughput may be higher, but batches will be returned out of order and memory use might increase. prefilter: bool, optional, default False Run filter before the vector search. late_materialization: bool or List[str], default None Allows custom control over late materialization. See ScannerBuilder.late_materialization for more information. use_scalar_index: bool, default True Allows custom control over scalar index usage. See ScannerBuilder.use_scalar_index for more information. with_row_id: bool, optional, default False Return row ID. with_row_address: bool, optional, default False Return row address use_stats: bool, optional, default True Use stats pushdown during filters. fast_search: bool, optional, default False full_text_query: str or dict, optional query string to search for, the results will be ranked by BM25. e.g. "hello world", would match documents contains "hello" or "world". or a dictionary with the following keys: - columns: list[str] The columns to search, currently only supports a single column in the columns list. - query: str The query string to search for. include_deleted_rows: bool, optional, default False If True, then rows that have been deleted, but are still present in the fragment, will be returned. These rows will have the _rowid column set to null. All other columns will reflect the value stored on disk and may not be null. Note: if this is a search operation, or a take operation (including scalar indexed scans) then deleted rows cannot be returned. Notes&para; If BOTH filter and nearest is specified, then: nearest is executed first. The results are filtered afterward, unless pre-filter sets to True. update(updates, where=None) &para; Update column values for rows matching where. Parameters&para; updates : dict of str to str A mapping of column names to a SQL expression. where : str, optional A SQL predicate indicating which rows should be updated. Returns&para; updates : dict A dictionary containing the number of rows updated. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") update_stats = dataset.update(dict(a = 'a + 2'), where="b != 'a'") update_stats["num_updated_rows"] = 2 dataset.to_table().to_pandas() a b 0 1 a 1 4 b 2 5 c validate() &para; Validate the dataset. This checks the integrity of the dataset and will raise an exception if the dataset is corrupted. versions() &para; Return all versions in this dataset. LanceFragment &para; Bases: Fragment metadata property &para; Return the metadata of this fragment. Returns&para; FragmentMetadata num_deletions property &para; Return the number of deleted rows in this fragment. physical_rows property &para; Return the number of rows originally in this fragment. To get the number of rows after deletions, use :meth:count_rows instead. schema property &para; Return the schema of this fragment. create(dataset_uri, data, fragment_id=None, schema=None, max_rows_per_group=1024, progress=None, mode=&#39;append&#39;, *, data_storage_version=None, use_legacy_format=None, storage_options=None) staticmethod &para; Create a :class:FragmentMetadata from the given data. This can be used if the dataset is not yet created. .. warning:: Internal API. This method is not intended to be used by end users. Parameters&para; dataset_uri: str The URI of the dataset. fragment_id: int The ID of the fragment. data: pa.Table or pa.RecordBatchReader The data to be written to the fragment. schema: pa.Schema, optional The schema of the data. If not specified, the schema will be inferred from the data. max_rows_per_group: int, default 1024 The maximum number of rows per group in the data file. progress: FragmentWriteProgress, optional Experimental API. Progress tracking for writing the fragment. Pass a custom class that defines hooks to be called when each fragment is starting to write and finishing writing. mode: str, default "append" The write mode. If "append" is specified, the data will be checked against the existing dataset's schema. Otherwise, pass "create" or "overwrite" to assign new field ids to the schema. data_storage_version: optional, str, default None The version of the data storage format to use. Newer versions are more efficient but require newer versions of lance to read. The default (None) will use the latest stable version. See the user guide for more details. use_legacy_format: bool, default None Deprecated parameter. Use data_storage_version instead. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. See Also&para; lance.dataset.LanceOperation.Overwrite : The operation used to create a new dataset or overwrite one using fragments created with this API. See the doc page for an example of using this API. lance.dataset.LanceOperation.Append : The operation used to append fragments created with this API to an existing dataset. See the doc page for an example of using this API. Returns&para; FragmentMetadata create_from_file(filename, dataset, fragment_id) staticmethod &para; Create a fragment from the given datafile uri. This can be used if the datafile is loss from dataset. .. warning:: Internal API. This method is not intended to be used by end users. Parameters&para; filename: str The filename of the datafile. dataset: LanceDataset The dataset that the fragment belongs to. fragment_id: int The ID of the fragment. data_files() &para; Return the data files of this fragment. delete(predicate) &para; Delete rows from this Fragment. This will add or update the deletion file of this fragment. It does not modify or delete the data files of this fragment. If no rows are left after the deletion, this method will return None. .. warning:: Internal API. This method is not intended to be used by end users. Parameters&para; predicate: str A SQL predicate that specifies the rows to delete. Returns&para; FragmentMetadata or None A new fragment containing the new deletion file, or None if no rows left. Examples&para; import lance import pyarrow as pa tab = pa.table({"a": [1, 2, 3], "b": [4, 5, 6]}) dataset = lance.write_dataset(tab, "dataset") frag = dataset.get_fragment(0) frag.delete("a &gt; 1") FragmentMetadata(id=0, files=[DataFile(path='...', fields=[0, 1], ...), ...) frag.delete("a &gt; 0") is None True See Also&para; lance.dataset.LanceOperation.Delete : The operation used to commit these changes to a dataset. See the doc page for an example of using this API. deletion_file() &para; Return the deletion file, if any merge(data_obj, left_on, right_on=None, schema=None) &para; Merge another dataset into this fragment. Performs a left join, where the fragment is the left side and data_obj is the right side. Rows existing in the dataset but not on the left will be filled with null values, unless Lance doesn't support null values for some types, in which case an error will be raised. Parameters&para; data_obj: Reader-like The data to be merged. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, Iterator[RecordBatch], or RecordBatchReader left_on: str The name of the column in the dataset to join on. right_on: str or None The name of the column in data_obj to join on. If None, defaults to left_on. Examples&para; import lance import pyarrow as pa df = pa.table({'x': [1, 2, 3], 'y': ['a', 'b', 'c']}) dataset = lance.write_dataset(df, "dataset") dataset.to_table().to_pandas() x y 0 1 a 1 2 b 2 3 c fragments = dataset.get_fragments() new_df = pa.table({'x': [1, 2, 3], 'z': ['d', 'e', 'f']}) merged = [] schema = None for f in fragments: ... f, schema = f.merge(new_df, 'x') ... merged.append(f) merge = lance.LanceOperation.Merge(merged, schema) dataset = lance.LanceDataset.commit("dataset", merge, read_version=1) dataset.to_table().to_pandas() x y z 0 1 a d 1 2 b e 2 3 c f See Also&para; LanceDataset.merge_columns : Add columns to this Fragment. Returns&para; Tuple[FragmentMetadata, LanceSchema] A new fragment with the merged column(s) and the final schema. merge_columns(value_func, columns=None, batch_size=None, reader_schema=None) &para; Add columns to this Fragment. .. warning:: Internal API. This method is not intended to be used by end users. The parameters and their interpretation are the same as in the :meth:lance.dataset.LanceDataset.add_columns operation. The only difference is that, instead of modifying the dataset, a new fragment is created. The new schema of the fragment is returned as well. These can be used in a later operation to commit the changes to the dataset. See Also&para; lance.dataset.LanceOperation.Merge : The operation used to commit these changes to the dataset. See the doc page for an example of using this API. Returns&para; Tuple[FragmentMetadata, LanceSchema] A new fragment with the added column(s) and the final schema. scanner(*, columns=None, batch_size=None, filter=None, limit=None, offset=None, with_row_id=False, with_row_address=False, batch_readahead=16) &para; See Dataset::scanner for details LanceOperation &para; Append dataclass &para; Bases: BaseOperation Append new rows to the dataset. Attributes&para; fragments: list[FragmentMetadata] The fragments that contain the new rows. Warning&para; This is an advanced API for distributed operations. To append to a dataset on a single machine, use :func:lance.write_dataset. Examples&para; To append new rows to a dataset, first use :meth:lance.fragment.LanceFragment.create to create fragments. Then collect the fragment metadata into a list and pass it to this class. Finally, pass the operation to the :meth:LanceDataset.commit method to create the new dataset. import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) dataset = lance.write_dataset(tab1, "example") tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment = lance.fragment.LanceFragment.create("example", tab2) operation = lance.LanceOperation.Append([fragment]) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d BaseOperation &para; Bases: ABC Base class for operations that can be applied to a dataset. See available operations under :class:LanceOperation. CreateIndex dataclass &para; Bases: BaseOperation Operation that creates an index on the dataset. DataReplacement dataclass &para; Bases: BaseOperation Operation that replaces existing datafiles in the dataset. DataReplacementGroup dataclass &para; Group of data replacements Delete dataclass &para; Bases: BaseOperation Remove fragments or rows from the dataset. Attributes&para; updated_fragments: list[FragmentMetadata] The fragments that have been updated with new deletion vectors. deleted_fragment_ids: list[int] The ids of the fragments that have been deleted entirely. These are the fragments where :meth:LanceFragment.delete() returned None. predicate: str The original SQL predicate used to select the rows to delete. Warning&para; This is an advanced API for distributed operations. To delete rows from dataset on a single machine, use :meth:lance.LanceDataset.delete. Examples&para; To delete rows from a dataset, call :meth:lance.fragment.LanceFragment.delete on each of the fragments. If that returns a new fragment, add that to the updated_fragments list. If it returns None, that means the whole fragment was deleted, so add the fragment id to the deleted_fragment_ids. Finally, pass the operation to the :meth:LanceDataset.commit method to complete the deletion operation. import lance import pyarrow as pa table = pa.table({"a": [1, 2], "b": ["a", "b"]}) dataset = lance.write_dataset(table, "example") table = pa.table({"a": [3, 4], "b": ["c", "d"]}) dataset = lance.write_dataset(table, "example", mode="append") dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d predicate = "a &gt;= 2" updated_fragments = [] deleted_fragment_ids = [] for fragment in dataset.get_fragments(): ... new_fragment = fragment.delete(predicate) ... if new_fragment is not None: ... updated_fragments.append(new_fragment) ... else: ... deleted_fragment_ids.append(fragment.fragment_id) operation = lance.LanceOperation.Delete(updated_fragments, ... deleted_fragment_ids, ... predicate) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b 0 1 a Merge dataclass &para; Bases: BaseOperation Operation that adds columns. Unlike Overwrite, this should not change the structure of the fragments, allowing existing indices to be kept. Attributes&para; fragments: iterable of FragmentMetadata The fragments that make up the new dataset. schema: LanceSchema or pyarrow.Schema The schema of the new dataset. Passing a LanceSchema is preferred, and passing a pyarrow.Schema is deprecated. Warning&para; This is an advanced API for distributed operations. To overwrite or create new dataset on a single machine, use :func:lance.write_dataset. Examples&para; To add new columns to a dataset, first define a method that will create the new columns based on the existing columns. Then use :meth:lance.fragment.LanceFragment.add_columns import lance import pyarrow as pa import pyarrow.compute as pc table = pa.table({"a": [1, 2, 3, 4], "b": ["a", "b", "c", "d"]}) dataset = lance.write_dataset(table, "example") dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d def double_a(batch: pa.RecordBatch) -&gt; pa.RecordBatch: ... doubled = pc.multiply(batch["a"], 2) ... return pa.record_batch([doubled], ["a_doubled"]) fragments = [] for fragment in dataset.get_fragments(): ... new_fragment, new_schema = fragment.merge_columns(double_a, ... columns=['a']) ... fragments.append(new_fragment) operation = lance.LanceOperation.Merge(fragments, new_schema) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b a_doubled 0 1 a 2 1 2 b 4 2 3 c 6 3 4 d 8 Overwrite dataclass &para; Bases: BaseOperation Overwrite or create a new dataset. Attributes&para; new_schema: pyarrow.Schema The schema of the new dataset. fragments: list[FragmentMetadata] The fragments that make up the new dataset. Warning&para; This is an advanced API for distributed operations. To overwrite or create new dataset on a single machine, use :func:lance.write_dataset. Examples&para; To create or overwrite a dataset, first use :meth:lance.fragment.LanceFragment.create to create fragments. Then collect the fragment metadata into a list and pass it along with the schema to this class. Finally, pass the operation to the :meth:LanceDataset.commit method to create the new dataset. import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment1 = lance.fragment.LanceFragment.create("example", tab1) fragment2 = lance.fragment.LanceFragment.create("example", tab2) fragments = [fragment1, fragment2] operation = lance.LanceOperation.Overwrite(tab1.schema, fragments) dataset = lance.LanceDataset.commit("example", operation) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d Project dataclass &para; Bases: BaseOperation Operation that project columns. Use this operator for drop column or rename/swap column. Attributes&para; schema: LanceSchema The lance schema of the new dataset. Examples&para; Use the projece operator to swap column: import lance import pyarrow as pa import pyarrow.compute as pc from lance.schema import LanceSchema table = pa.table({"a": [1, 2], "b": ["a", "b"], "b1": ["c", "d"]}) dataset = lance.write_dataset(table, "example") dataset.to_table().to_pandas() a b b1 0 1 a c 1 2 b d rename column b into b0 and rename b1 into b&para; table = pa.table({"a": [3, 4], "b0": ["a", "b"], "b": ["c", "d"]}) lance_schema = LanceSchema.from_pyarrow(table.schema) operation = lance.LanceOperation.Project(lance_schema) dataset = lance.LanceDataset.commit("example", operation, read_version=1) dataset.to_table().to_pandas() a b0 b 0 1 a c 1 2 b d Restore dataclass &para; Bases: BaseOperation Operation that restores a previous version of the dataset. Rewrite dataclass &para; Bases: BaseOperation Operation that rewrites one or more files and indices into one or more files and indices. Attributes&para; groups: list[RewriteGroup] Groups of files that have been rewritten. rewritten_indices: list[RewrittenIndex] Indices that have been rewritten. Warning&para; This is an advanced API not intended for general use. RewriteGroup dataclass &para; Collection of rewritten files RewrittenIndex dataclass &para; An index that has been rewritten Update dataclass &para; Bases: BaseOperation Operation that updates rows in the dataset. Attributes&para; removed_fragment_ids: list[int] The ids of the fragments that have been removed entirely. updated_fragments: list[FragmentMetadata] The fragments that have been updated with new deletion vectors. new_fragments: list[FragmentMetadata] The fragments that contain the new rows. LanceScanner &para; Bases: Scanner dataset_schema property &para; The schema with which batches will be read from fragments. analyze_plan() &para; Execute the plan for this scanner and display with runtime metrics. Parameters&para; verbose : bool, default False Use a verbose output format. Returns&para; plan : str count_rows() &para; Count rows matching the scanner filter. Returns&para; count : int explain_plan(verbose=False) &para; Return the execution plan for this scanner. Parameters&para; verbose : bool, default False Use a verbose output format. Returns&para; plan : str from_batches(*args, **kwargs) staticmethod &para; Not implemented from_dataset(*args, **kwargs) staticmethod &para; Not implemented from_fragment(*args, **kwargs) staticmethod &para; Not implemented head(num_rows) &para; Load the first N rows of the dataset. Parameters&para; num_rows : int The number of rows to load. Returns&para; Table scan_batches() &para; Consume a Scanner in record batches with corresponding fragments. Returns&para; record_batches : iterator of TaggedRecordBatch take(indices) &para; Not implemented to_table() &para; Read the data into memory and return a pyarrow Table. MergeInsertBuilder &para; Bases: _MergeInsertBuilder conflict_retries(max_retries) &para; Set number of times to retry the operation if there is contention. If this is set &gt; 0, then the operation will keep a copy of the input data either in memory or on disk (depending on the size of the data) and will retry the operation if there is contention. Default is 10. execute(data_obj, *, schema=None) &para; Executes the merge insert operation This function updates the original dataset and returns a dictionary with information about merge statistics - i.e. the number of inserted, updated, and deleted rows. Parameters&para; ReaderLike The new data to use as the source table for the operation. This parameter can be any source of data (e.g. table / dataset) that :func:~lance.write_dataset accepts. schema: Optional[pa.Schema] The schema of the data. This only needs to be supplied whenever the data source is some kind of generator. execute_uncommitted(data_obj, *, schema=None) &para; Executes the merge insert operation without committing This function updates the original dataset and returns a dictionary with information about merge statistics - i.e. the number of inserted, updated, and deleted rows. Parameters&para; ReaderLike The new data to use as the source table for the operation. This parameter can be any source of data (e.g. table / dataset) that :func:~lance.write_dataset accepts. schema: Optional[pa.Schema] The schema of the data. This only needs to be supplied whenever the data source is some kind of generator. retry_timeout(timeout) &para; Set the timeout used to limit retries. This is the maximum time to spend on the operation before giving up. At least one attempt will be made, regardless of how long it takes to complete. Subsequent attempts will be cancelled once this timeout is reached. If the timeout has been reached during the first attempt, the operation will be cancelled immediately before making a second attempt. The default is 30 seconds. when_matched_update_all(condition=None) &para; Configure the operation to update matched rows After this method is called, when the merge insert operation executes, any rows that match both the source table and the target table will be updated. The rows from the target table will be removed and the rows from the source table will be added. An optional condition may be specified. This should be an SQL filter and, if present, then only matched rows that also satisfy this filter will be updated. The SQL filter should use the prefix target. to refer to columns in the target table and the prefix source. to refer to columns in the source table. For example, source.last_update &lt; target.last_update. If a condition is specified and rows do not satisfy the condition then these rows will not be updated. Failure to satisfy the filter does not cause a "matched" row to become a "not matched" row. when_not_matched_by_source_delete(expr=None) &para; Configure the operation to delete source rows that do not match After this method is called, when the merge insert operation executes, any rows that exist only in the target table will be deleted. An optional filter can be specified to limit the scope of the delete operation. If given (as an SQL filter) then only rows which match the filter will be deleted. when_not_matched_insert_all() &para; Configure the operation to insert not matched rows After this method is called, when the merge insert operation executes, any rows that exist only in the source table will be inserted into the target table. batch_udf(output_schema=None, checkpoint_file=None) &para; Create a user defined function (UDF) that adds columns to a dataset. This function is used to add columns to a dataset. It takes a function that takes a single argument, a RecordBatch, and returns a RecordBatch. The function is called once for each batch in the dataset. The function should not modify the input batch, but instead create a new batch with the new columns added. Parameters&para; output_schema : Schema, optional The schema of the output RecordBatch. This is used to validate the output of the function. If not provided, the schema of the first output RecordBatch will be used. checkpoint_file : str or Path, optional If specified, this file will be used as a cache for unsaved results of this UDF. If the process fails, and you call add_columns again with this same file, it will resume from the last saved state. This is useful for long running processes that may fail and need to be resumed. This file may get very large. It will hold up to an entire data files' worth of results on disk, which can be multiple gigabytes of data. Returns&para; AddColumnsUDF json_to_schema(schema_json) &para; Converts a JSON string to a PyArrow schema. Parameters&para; schema_json: Dict[str, Any] The JSON payload to convert to a PyArrow Schema. schema_to_json(schema) &para; Converts a pyarrow schema to a JSON string. Parameters&para; write_dataset(data_obj, uri, schema=None, mode=&#39;create&#39;, *, max_rows_per_file=1024 * 1024, max_rows_per_group=1024, max_bytes_per_file=90 * 1024 * 1024 * 1024, commit_lock=None, progress=None, storage_options=None, data_storage_version=None, use_legacy_format=None, enable_v2_manifest_paths=False, enable_move_stable_row_ids=False) &para; Write a given data_obj to the given uri Parameters&para; data_obj: Reader-like The data to be written. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader - Huggingface dataset uri: str, Path, or LanceDataset Where to write the dataset to (directory). If a LanceDataset is passed, the session will be reused. schema: Schema, optional If specified and the input is a pandas DataFrame, use this schema instead of the default pandas to arrow table conversion. mode: str create - create a new dataset (raises if uri already exists). overwrite - create a new snapshot version append - create a new version that is the concat of the input the latest version (raises if uri does not exist) max_rows_per_file: int, default 1024 * 1024 The max number of rows to write before starting a new file max_rows_per_group: int, default 1024 The max number of rows before starting a new group (in the same file) max_bytes_per_file: int, default 90 * 1024 * 1024 * 1024 The max number of bytes to write before starting a new file. This is a soft limit. This limit is checked after each group is written, which means larger groups may cause this to be overshot meaningfully. This defaults to 90 GB, since we have a hard limit of 100 GB per file on object stores. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. progress: FragmentWriteProgress, optional Experimental API. Progress tracking for writing the fragment. Pass a custom class that defines hooks to be called when each fragment is starting to write and finishing writing. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. data_storage_version: optional, str, default None The version of the data storage format to use. Newer versions are more efficient but require newer versions of lance to read. The default (None) will use the latest stable version. See the user guide for more details. use_legacy_format : optional, bool, default None Deprecated method for setting the data storage version. Use the data_storage_version parameter instead. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:LanceDataset.migrate_manifest_paths_v2 method. Default is False. enable_move_stable_row_ids : bool, optional Experimental parameter: if set to true, the writer will use move-stable row ids. These row ids are stable after compaction operations, but not after updates. This makes compaction more efficient, since with stable row ids no secondary indices need to be updated to point to new row ids. into lance.dataset &para; DataStatistics dataclass &para; Statistics about the data in the dataset DatasetOptimizer &para; compact_files(*, target_rows_per_fragment=1024 * 1024, max_rows_per_group=1024, max_bytes_per_file=None, materialize_deletions=True, materialize_deletions_threshold=0.1, num_threads=None, batch_size=None) &para; Compacts small files in the dataset, reducing total number of files. This does a few things Removes deleted rows from fragments Removes dropped columns from fragments Merges small fragments into larger ones This method preserves the insertion order of the dataset. This may mean it leaves small fragments in the dataset if they are not adjacent to other fragments that need compaction. For example, if you have fragments with row counts 5 million, 100, and 5 million, the middle fragment will not be compacted because the fragments it is adjacent to do not need compaction. Parameters&para; target_rows_per_fragment: int, default 1024*1024 The target number of rows per fragment. This is the number of rows that will be in each fragment after compaction. max_rows_per_group: int, default 1024 Max number of rows per group. This does not affect which fragments need compaction, but does affect how they are re-written if selected. This setting only affects datasets using the legacy storage format. The newer format does not require row groups. max_bytes_per_file: Optional[int], default None Max number of bytes in a single file. This does not affect which fragments need compaction, but does affect how they are re-written if selected. If this value is too small you may end up with fragments that are smaller than target_rows_per_fragment. The default will use the default from ``write_dataset``. materialize_deletions: bool, default True Whether to compact fragments with soft deleted rows so they are no longer present in the file. materialize_deletions_threshold: float, default 0.1 The fraction of original rows that are soft deleted in a fragment before the fragment is a candidate for compaction. num_threads: int, optional The number of threads to use when performing compaction. If not specified, defaults to the number of cores on the machine. batch_size: int, optional The batch size to use when scanning input fragments. You may want to reduce this if you are running out of memory during compaction. The default will use the same default from ``scanner``. Returns&para; CompactionMetrics Metrics about the compaction process See Also&para; lance.optimize.Compaction optimize_indices(**kwargs) &para; Optimizes index performance. As new data arrives it is not added to existing indexes automatically. When searching we need to perform an indexed search of the old data plus an expensive unindexed search on the new data. As the amount of new unindexed data grows this can have an impact on search latency. This function will add the new data to existing indexes, restoring the performance. This function does not retrain the index, it only assigns the new data to existing partitions. This means an update is much quicker than retraining the entire index but may have less accuracy (especially if the new data exhibits new patterns, concepts, or trends) Parameters&para; num_indices_to_merge: int, default 1 The number of indices to merge. If set to 0, new delta index will be created. index_names: List[str], default None The names of the indices to optimize. If None, all indices will be optimized. retrain: bool, default False Whether to retrain the whole index. If true, the index will be retrained based on the current data, num_indices_to_merge will be ignored, and all indices will be merged into one. This is useful when the data distribution has changed significantly, and we want to retrain the index to improve the search quality. This would be faster than re-create the index from scratch. FieldStatistics dataclass &para; Statistics about a field in the dataset LanceDataset &para; Bases: Dataset A Lance Dataset in Lance format where the data is stored at the given uri. data_storage_version property &para; The version of the data storage format this dataset is using lance_schema property &para; The LanceSchema for this dataset latest_version property &para; Returns the latest version of the dataset. max_field_id property &para; The max_field_id in manifest partition_expression property &para; Not implemented (just override pyarrow dataset to prevent segfault) schema property &para; The pyarrow Schema for this dataset stats property &para; Experimental API tags property &para; Tag management for the dataset. Similar to Git, tags are a way to add metadata to a specific version of the dataset. .. warning:: Tagged versions are exempted from the :py:meth:`cleanup_old_versions()` process. To remove a version that has been tagged, you must first :py:meth:`~Tags.delete` the associated tag. Examples&para; .. code-block:: python ds = lance.open(&quot;dataset.lance&quot;) ds.tags.create(&quot;v2-prod-20250203&quot;, 10) tags = ds.tags.list() uri property &para; The location of the data version property &para; Returns the currently checked out version of the dataset add_columns(transforms, read_columns=None, reader_schema=None, batch_size=None) &para; Add new columns with defined values. There are several ways to specify the new columns. First, you can provide SQL expressions for each new column. Second you can provide a UDF that takes a batch of existing data and returns a new batch with the new columns. These new columns will be appended to the dataset. You can also provide a RecordBatchReader which will read the new column values from some external source. This is often useful when the new column values have already been staged to files (often by some distributed process) See the :func:lance.add_columns_udf decorator for more information on writing UDFs. Parameters&para; transforms : dict or AddColumnsUDF or ReaderLike If this is a dictionary, then the keys are the names of the new columns and the values are SQL expression strings. These strings can reference existing columns in the dataset. If this is a AddColumnsUDF, then it is a UDF that takes a batch of existing data and returns a new batch with the new columns. If this is :class:pyarrow.Field or :class:pyarrow.Schema, it adds all NULL columns with the given schema, in a metadata-only operation. read_columns : list of str, optional The names of the columns that the UDF will read. If None, then the UDF will read all columns. This is only used when transforms is a UDF. Otherwise, the read columns are inferred from the SQL expressions. reader_schema: pa.Schema, optional Only valid if transforms is a ReaderLike object. This will be used to determine the schema of the reader. batch_size: int, optional The number of rows to read at a time from the source dataset when applying the transform. This is ignored if the dataset is a v1 dataset. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3]}) dataset = lance.write_dataset(table, "my_dataset") @lance.batch_udf() ... def double_a(batch): ... df = batch.to_pandas() ... return pd.DataFrame({'double_a': 2 * df['a']}) dataset.add_columns(double_a) dataset.to_table().to_pandas() a double_a 0 1 2 1 2 4 2 3 6 dataset.add_columns({"triple_a": "a * 3"}) dataset.to_table().to_pandas() a double_a triple_a 0 1 2 3 1 2 4 6 2 3 6 9 See Also&para; LanceDataset.merge : Merge a pre-computed set of columns into the dataset. alter_columns(*alterations) &para; Alter column name, data type, and nullability. Columns that are renamed can keep any indices that are on them. If a column has an IVF_PQ index, it can be kept if the column is casted to another type. However, other index types don't support casting at this time. Column types can be upcasted (such as int32 to int64) or downcasted (such as int64 to int32). However, downcasting will fail if there are any values that cannot be represented in the new type. In general, columns can be casted to same general type: integers to integers, floats to floats, and strings to strings. However, strings, binary, and list columns can be casted between their size variants. For example, string to large string, binary to large binary, and list to large list. Columns that are renamed can keep any indices that are on them. However, if the column is casted to a different type, its indices will be dropped. Parameters&para; alterations : Iterable[Dict[str, Any]] A sequence of dictionaries, each with the following keys: - &quot;path&quot;: str The column path to alter. For a top-level column, this is the name. For a nested column, this is the dot-separated path, e.g. &quot;a.b.c&quot;. - &quot;name&quot;: str, optional The new name of the column. If not specified, the column name is not changed. - &quot;nullable&quot;: bool, optional Whether the column should be nullable. If not specified, the column nullability is not changed. Only non-nullable columns can be changed to nullable. Currently, you cannot change a nullable column to non-nullable. - &quot;data_type&quot;: pyarrow.DataType, optional The new data type to cast the column to. If not specified, the column data type is not changed. Examples&para; import lance import pyarrow as pa schema = pa.schema([pa.field('a', pa.int64()), ... pa.field('b', pa.string(), nullable=False)]) table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.alter_columns({"path": "a", "name": "x"}, ... {"path": "b", "nullable": True}) dataset.to_table().to_pandas() x b 0 1 a 1 2 b 2 3 c dataset.alter_columns({"path": "x", "data_type": pa.int32()}) dataset.schema x: int32 b: string checkout_version(version) &para; Load the given version of the dataset. Unlike the :func:dataset constructor, this will re-use the current cache. This is a no-op if the dataset is already at the given version. Parameters&para; version: int | str, The version to check out. A version number (int) or a tag (str) can be provided. Returns&para; LanceDataset cleanup_old_versions(older_than=None, *, delete_unverified=False, error_if_tagged_old_versions=True) &para; Cleans up old versions of the dataset. Some dataset changes, such as overwriting, leave behind data that is not referenced by the latest dataset version. The old data is left in place to allow the dataset to be restored back to an older version. This method will remove older versions and any data files they reference. Once this cleanup task has run you will not be able to checkout or restore these older versions. Parameters&para; timedelta, optional Only versions older than this will be removed. If not specified, this will default to two weeks. bool, default False Files leftover from a failed transaction may appear to be part of an in-progress operation (e.g. appending new data) and these files will not be deleted unless they are at least 7 days old. If delete_unverified is True then these files will be deleted regardless of their age. This should only be set to True if you can guarantee that no other process is currently working on this dataset. Otherwise the dataset could be put into a corrupted state. bool, default True Some versions may have tags associated with them. Tagged versions will not be cleaned up, regardless of how old they are. If this argument is set to True (the default), an exception will be raised if any tagged versions match the parameters. Otherwise, tagged versions will be ignored without any error and only untagged versions will be cleaned up. commit(base_uri, operation, blobs_op=None, read_version=None, commit_lock=None, storage_options=None, enable_v2_manifest_paths=None, detached=False, max_retries=20) staticmethod &para; Create a new version of dataset This method is an advanced method which allows users to describe a change that has been made to the data files. This method is not needed when using Lance to apply changes (e.g. when using class:LanceDataset or func:write_dataset.) It's current purpose is to allow for changes being made in a distributed environment where no single process is doing all of the work. For example, a distributed bulk update or a distributed bulk modify operation. Once all of the changes have been made, this method can be called to make the changes visible by updating the dataset manifest. Warnings&para; This is an advanced API and doesn't provide the same level of validation as the other APIs. For example, it's the responsibility of the caller to ensure that the fragments are valid for the schema. Parameters&para; base_uri: str, Path, or LanceDataset The base uri of the dataset, or the dataset object itself. Using the dataset object can be more efficient because it can re-use the file metadata cache. operation: BaseOperation The operation to apply to the dataset. This describes what changes have been made. See available operations under :class:LanceOperation. read_version: int, optional The version of the dataset that was used as the base for the changes. This is not needed for overwrite or restore operations. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:migrate_manifest_paths_v2 method. Default is False. WARNING: turning this on will make the dataset unreadable for older versions of Lance (prior to 0.17.0). detached : bool, optional If True, then the commit will not be part of the dataset lineage. It will never show up as the latest dataset and the only way to check it out in the future will be to specifically check it out by version. The version will be a random version that is only unique amongst detached commits. The caller should store this somewhere as there will be no other way to obtain it in the future. max_retries : int The maximum number of retries to perform when committing the dataset. Returns&para; LanceDataset A new version of Lance Dataset. Examples&para; Creating a new dataset with the :class:LanceOperation.Overwrite operation: import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment1 = lance.fragment.LanceFragment.create("example", tab1) fragment2 = lance.fragment.LanceFragment.create("example", tab2) fragments = [fragment1, fragment2] operation = lance.LanceOperation.Overwrite(tab1.schema, fragments) dataset = lance.LanceDataset.commit("example", operation) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d commit_batch(dest, transactions, commit_lock=None, storage_options=None, enable_v2_manifest_paths=None, detached=False, max_retries=20) staticmethod &para; Create a new version of dataset with multiple transactions. This method is an advanced method which allows users to describe a change that has been made to the data files. This method is not needed when using Lance to apply changes (e.g. when using class:LanceDataset or func:write_dataset.) Parameters&para; dest: str, Path, or LanceDataset The base uri of the dataset, or the dataset object itself. Using the dataset object can be more efficient because it can re-use the file metadata cache. transactions: Iterable[Transaction] The transactions to apply to the dataset. These will be merged into a single transaction and applied to the dataset. Note: Only append transactions are currently supported. Other transaction types will be supported in the future. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:migrate_manifest_paths_v2 method. Default is False. WARNING: turning this on will make the dataset unreadable for older versions of Lance (prior to 0.17.0). detached : bool, optional If True, then the commit will not be part of the dataset lineage. It will never show up as the latest dataset and the only way to check it out in the future will be to specifically check it out by version. The version will be a random version that is only unique amongst detached commits. The caller should store this somewhere as there will be no other way to obtain it in the future. max_retries : int The maximum number of retries to perform when committing the dataset. Returns&para; dict with keys: dataset: LanceDataset A new version of Lance Dataset. merged: Transaction The merged transaction that was applied to the dataset. count_rows(filter=None, **kwargs) &para; Count rows matching the scanner filter. Parameters&para; **kwargs : dict, optional See py:method:scanner method for full parameter description. Returns&para; count : int The total number of rows in the dataset. create_index(column, index_type, name=None, metric=&#39;L2&#39;, replace=False, num_partitions=None, ivf_centroids=None, pq_codebook=None, num_sub_vectors=None, accelerator=None, index_cache_size=None, shuffle_partition_batches=None, shuffle_partition_concurrency=None, ivf_centroids_file=None, precomputed_partition_dataset=None, storage_options=None, filter_nan=True, one_pass_ivfpq=False, **kwargs) &para; Create index on column. Experimental API Parameters&para; column : str The column to be indexed. index_type : str The type of the index. "IVF_PQ, IVF_HNSW_PQ and IVF_HNSW_SQ" are supported now. name : str, optional The index name. If not provided, it will be generated from the column name. metric : str The distance metric type, i.e., "L2" (alias to "euclidean"), "cosine" or "dot" (dot product). Default is "L2". replace : bool Replace the existing index if it exists. num_partitions : int, optional The number of partitions of IVF (Inverted File Index). ivf_centroids : optional It can be either class:np.ndarray, class:pyarrow.FixedSizeListArray or class:pyarrow.FixedShapeTensorArray. A num_partitions x dimension array of existing K-mean centroids for IVF clustering. If not provided, a new KMeans model will be trained. pq_codebook : optional, It can be class:np.ndarray, class:pyarrow.FixedSizeListArray, or class:pyarrow.FixedShapeTensorArray. A num_sub_vectors x (2 ^ nbits * dimensions // num_sub_vectors) array of K-mean centroids for PQ codebook. Note: ``nbits`` is always 8 for now. If not provided, a new PQ model will be trained. num_sub_vectors : int, optional The number of sub-vectors for PQ (Product Quantization). accelerator : str or torch.Device, optional If set, use an accelerator to speed up the training process. Accepted accelerator: "cuda" (Nvidia GPU) and "mps" (Apple Silicon GPU). If not set, use the CPU. index_cache_size : int, optional The size of the index cache in number of entries. Default value is 256. shuffle_partition_batches : int, optional The number of batches, using the row group size of the dataset, to include in each shuffle partition. Default value is 10240. Assuming the row group size is 1024, each shuffle partition will hold 10240 * 1024 = 10,485,760 rows. By making this value smaller, this shuffle will consume less memory but will take longer to complete, and vice versa. shuffle_partition_concurrency : int, optional The number of shuffle partitions to process concurrently. Default value is 2 By making this value smaller, this shuffle will consume less memory but will take longer to complete, and vice versa. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. filter_nan: bool Defaults to True. False is UNSAFE, and will cause a crash if any null/nan values are present (and otherwise will not). Disables the null filter used for nullable columns. Obtains a small speed boost. one_pass_ivfpq: bool Defaults to False. If enabled, index type must be "IVF_PQ". Reduces disk IO. kwargs : Parameters passed to the index building process. The SQ (Scalar Quantization) is available for only IVF_HNSW_SQ index type, this quantization method is used to reduce the memory usage of the index, it maps the float vectors to integer vectors, each integer is of num_bits, now only 8 bits are supported. If index_type is "IVF_*", then the following parameters are required: num_partitions If index_type is with "PQ", then the following parameters are required: num_sub_vectors Optional parameters for IVF_PQ: - ivf_centroids Existing K-mean centroids for IVF clustering. - num_bits The number of bits for PQ (Product Quantization). Default is 8. Only 4, 8 are supported. Optional parameters for IVF_HNSW_*: max_level Int, the maximum number of levels in the graph. m Int, the number of edges per node in the graph. ef_construction Int, the number of nodes to examine during the construction. Examples&para; .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_PQ&quot;, num_partitions=256, num_sub_vectors=16 ) .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_HNSW_SQ&quot;, num_partitions=256, ) Experimental Accelerator (GPU) support: accelerate: use GPU to train IVF partitions. Only supports CUDA (Nvidia) or MPS (Apple) currently. Requires PyTorch being installed. .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_PQ&quot;, num_partitions=256, num_sub_vectors=16, accelerator=&quot;cuda&quot; ) References&para; Faiss Index &lt;https://github.com/facebookresearch/faiss/wiki/Faiss-indexes&gt;_ IVF introduced in Video Google: a text retrieval approach to object matching in videos &lt;https://ieeexplore.ieee.org/abstract/document/1238663&gt;_ Product quantization for nearest neighbor search &lt;https://hal.inria.fr/inria-00514462v2/document&gt;_ create_scalar_index(column, index_type, name=None, *, replace=True, **kwargs) &para; Create a scalar index on a column. Scalar indices, like vector indices, can be used to speed up scans. A scalar index can speed up scans that contain filter expressions on the indexed column. For example, the following scan will be faster if the column my_col has a scalar index: .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) my_table = dataset.scanner(filter=&quot;my_col != 7&quot;).to_table() Vector search with pre-filers can also benefit from scalar indices. For example, .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) my_table = dataset.scanner( nearest=dict( column=&quot;vector&quot;, q=[1, 2, 3, 4], k=10, ) filter=&quot;my_col != 7&quot;, prefilter=True ) There are 5 types of scalar indices available today. BTREE. The most common type is BTREE. This index is inspired by the btree data structure although only the first few layers of the btree are cached in memory. It will perform well on columns with a large number of unique values and few rows per value. BITMAP. This index stores a bitmap for each unique value in the column. This index is useful for columns with a small number of unique values and many rows per value. LABEL_LIST. A special index that is used to index list columns whose values have small cardinality. For example, a column that contains lists of tags (e.g. ["tag1", "tag2", "tag3"]) can be indexed with a LABEL_LIST index. This index can only speedup queries with array_has_any or array_has_all filters. NGRAM. A special index that is used to index string columns. This index creates a bitmap for each ngram in the string. By default we use trigrams. This index can currently speed up queries using the contains function in filters. FTS/INVERTED. It is used to index document columns. This index can conduct full-text searches. For example, a column that contains any word of query string "hello world". The results will be ranked by BM25. Note that the LANCE_BYPASS_SPILLING environment variable can be used to bypass spilling to disk. Setting this to true can avoid memory exhaustion issues (see https://github.com/apache/datafusion/issues/10073 for more info). Experimental API Parameters&para; column : str The column to be indexed. Must be a boolean, integer, float, or string column. index_type : str The type of the index. One of "BTREE", "BITMAP", "LABEL_LIST", "NGRAM", "FTS" or "INVERTED". name : str, optional The index name. If not provided, it will be generated from the column name. replace : bool, default True Replace the existing index if it exists. bool, default True This is for the INVERTED index. If True, the index will store the positions of the words in the document, so that you can conduct phrase query. This will significantly increase the index size. It won't impact the performance of non-phrase queries even if it is set to True. base_tokenizer: str, default "simple" This is for the INVERTED index. The base tokenizer to use. The value can be: * "simple": splits tokens on whitespace and punctuation. * "whitespace": splits tokens on whitespace. * "raw": no tokenization. language: str, default "English" This is for the INVERTED index. The language for stemming and stop words. This is only used when stem or remove_stop_words is true max_token_length: Optional[int], default 40 This is for the INVERTED index. The maximum token length. Any token longer than this will be removed. lower_case: bool, default True This is for the INVERTED index. If True, the index will convert all text to lowercase. stem: bool, default False This is for the INVERTED index. If True, the index will stem the tokens. remove_stop_words: bool, default False This is for the INVERTED index. If True, the index will remove stop words. ascii_folding: bool, default False This is for the INVERTED index. If True, the index will convert non-ascii characters to ascii characters if possible. This would remove accents like "é" -&gt; "e". Examples&para; .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) dataset.create_index( &quot;category&quot;, &quot;BTREE&quot;, ) Scalar indices can only speed up scans for basic filters using equality, comparison, range (e.g. my_col BETWEEN 0 AND 100), and set membership (e.g. my_col IN (0, 1, 2)) Scalar indices can be used if the filter contains multiple indexed columns and the filter criteria are AND'd or OR'd together (e.g. my_col &lt; 0 AND other_col&gt; 100) Scalar indices may be used if the filter contains non-indexed columns but, depending on the structure of the filter, they may not be usable. For example, if the column not_indexed does not have a scalar index then the filter my_col = 0 OR not_indexed = 1 will not be able to use any scalar index on my_col. To determine if a scan is making use of a scalar index you can use explain_plan to look at the query plan that lance has created. Queries that use scalar indices will either have a ScalarIndexQuery relation or a MaterializeIndex operator. delete(predicate) &para; Delete rows from the dataset. This marks rows as deleted, but does not physically remove them from the files. This keeps the existing indexes still valid. Parameters&para; predicate : str or pa.compute.Expression The predicate to use to select rows to delete. May either be a SQL string or a pyarrow Expression. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.delete("a = 1 or b in ('a', 'b')") dataset.to_table() pyarrow.Table a: int64 b: string a: [[3]] b: [["c"]] drop_columns(columns) &para; Drop one or more columns from the dataset This is a metadata-only operation and does not remove the data from the underlying storage. In order to remove the data, you must subsequently call compact_files to rewrite the data without the removed columns and then call cleanup_old_versions to remove the old files. Parameters&para; columns : list of str The names of the columns to drop. These can be nested column references (e.g. "a.b.c") or top-level column names (e.g. "a"). Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.drop_columns(["a"]) dataset.to_table().to_pandas() b 0 a 1 b 2 c drop_index(name) &para; Drops an index from the dataset Note: Indices are dropped by "index name". This is not the same as the field name. If you did not specify a name when you created the index then a name was generated for you. You can use the list_indices method to get the names of the indices. get_fragment(fragment_id) &para; Get the fragment with fragment id. get_fragments(filter=None) &para; Get all fragments from the dataset. Note: filter is not supported yet. head(num_rows, **kwargs) &para; Load the first N rows of the dataset. Parameters&para; num_rows : int The number of rows to load. **kwargs : dict, optional See scanner() method for full parameter description. Returns&para; table : Table insert(data, *, mode=&#39;append&#39;, **kwargs) &para; Insert data into the dataset. Parameters&para; data_obj: Reader-like The data to be written. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader - Huggingface dataset mode: str, default 'append' The mode to use when writing the data. Options are: create - create a new dataset (raises if uri already exists). overwrite - create a new snapshot version append - create a new version that is the concat of the input the latest version (raises if uri does not exist) **kwargs : dict, optional Additional keyword arguments to pass to :func:write_dataset. join(right_dataset, keys, right_keys=None, join_type=&#39;left outer&#39;, left_suffix=None, right_suffix=None, coalesce_keys=True, use_threads=True) &para; Not implemented (just override pyarrow dataset to prevent segfault) merge(data_obj, left_on, right_on=None, schema=None) &para; Merge another dataset into this one. Performs a left join, where the dataset is the left side and data_obj is the right side. Rows existing in the dataset but not on the left will be filled with null values, unless Lance doesn't support null values for some types, in which case an error will be raised. Parameters&para; data_obj: Reader-like The data to be merged. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, Iterator[RecordBatch], or RecordBatchReader left_on: str The name of the column in the dataset to join on. right_on: str or None The name of the column in data_obj to join on. If None, defaults to left_on. Examples&para; import lance import pyarrow as pa df = pa.table({'x': [1, 2, 3], 'y': ['a', 'b', 'c']}) dataset = lance.write_dataset(df, "dataset") dataset.to_table().to_pandas() x y 0 1 a 1 2 b 2 3 c new_df = pa.table({'x': [1, 2, 3], 'z': ['d', 'e', 'f']}) dataset.merge(new_df, 'x') dataset.to_table().to_pandas() x y z 0 1 a d 1 2 b e 2 3 c f See Also&para; LanceDataset.add_columns : Add new columns by computing batch-by-batch. merge_insert(on) &para; Returns a builder that can be used to create a "merge insert" operation This operation can add rows, update rows, and remove rows in a single transaction. It is a very generic tool that can be used to create behaviors like "insert if not exists", "update or insert (i.e. upsert)", or even replace a portion of existing data with new data (e.g. replace all data where month="january") The merge insert operation works by combining new data from a source table with existing data in a target table by using a join. There are three categories of records. "Matched" records are records that exist in both the source table and the target table. "Not matched" records exist only in the source table (e.g. these are new data). "Not matched by source" records exist only in the target table (this is old data). The builder returned by this method can be used to customize what should happen for each category of data. Please note that the data will be reordered as part of this operation. This is because updated rows will be deleted from the dataset and then reinserted at the end with the new values. The order of the newly inserted rows may fluctuate randomly because a hash-join operation is used internally. Parameters&para; Union[str, Iterable[str]] A column (or columns) to join on. This is how records from the source table and target table are matched. Typically this is some kind of key or id column. Examples&para; Use when_matched_update_all() and when_not_matched_insert_all() to perform an "upsert" operation. This will update rows that already exist in the dataset and insert rows that do not exist. import lance import pyarrow as pa table = pa.table({"a": [2, 1, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform a "upsert" operation&para; dataset.merge_insert("a") \ ... .when_matched_update_all() \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b 0 1 b 1 2 x 2 3 y 3 4 z Use when_not_matched_insert_all() to perform an "insert if not exists" operation. This will only insert rows that do not already exist in the dataset. import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example2") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform an "insert if not exists" operation&para; dataset.merge_insert("a") \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 0, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 z You are not required to provide all the columns. If you only want to update a subset of columns, you can omit columns you don't want to update. Omitted columns will keep their existing values if they are updated, or will be null if they are inserted. import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"], \ ... "c": ["x", "y", "z"]}) dataset = lance.write_dataset(table, "example3") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform an "upsert" operation, only updating column "a"&para; dataset.merge_insert("a") \ ... .when_matched_update_all() \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b c 0 1 a x 1 2 x y 2 3 y z 3 4 z None migrate_manifest_paths_v2() &para; Migrate the manifest paths to the new format. This will update the manifest to use the new v2 format for paths. This function is idempotent, and can be run multiple times without changing the state of the object store. DANGER: this should not be run while other concurrent operations are happening. And it should also run until completion before resuming other operations. prewarm_index(name) &para; Prewarm an index This will load the entire index into memory. This can help avoid cold start issues with index queries. If the index does not fit in the index cache, then this will result in wasted I/O. Parameters&para; name: str The name of the index to prewarm. replace_field_metadata(field_name, new_metadata) &para; Replace the metadata of a field in the schema Parameters&para; field_name: str The name of the field to replace the metadata for new_metadata: dict The new metadata to set replace_schema(schema) &para; Not implemented (just override pyarrow dataset to prevent segfault) See method:replace_schema_metadata or method:replace_field_metadata replace_schema_metadata(new_metadata) &para; Replace the schema metadata of the dataset Parameters&para; new_metadata: dict The new metadata to set restore() &para; Restore the currently checked out version as the latest version of the dataset. This creates a new commit. sample(num_rows, columns=None, randomize_order=True, **kwargs) &para; Select a random sample of data Parameters&para; num_rows: int number of rows to retrieve columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. **kwargs : dict, optional see scanner() method for full parameter description. Returns&para; table : Table scanner(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, fragments=None, full_text_query=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, fast_search=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, include_deleted_rows=None, scan_stats_callback=None, strict_batch_size=None) &para; Return a Scanner that can support various pushdowns. Parameters&para; columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. filter: pa.compute.Expression or str Expression or str that is a valid SQL where clause. See Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;_ for valid SQL expressions. limit: int, default None Fetch up to this many rows. All rows if None or unspecified. offset: int, default None Fetch starting with this row. 0 if None or unspecified. nearest: dict, default None Get the rows corresponding to the K most similar vectors. Example: .. code-block:: python { &quot;column&quot;: &lt;embedding col name&gt;, &quot;q&quot;: &lt;query vector as pa.Float32Array&gt;, &quot;k&quot;: 10, &quot;nprobes&quot;: 1, &quot;refine_factor&quot;: 1 } int, default None The target size of batches returned. In some cases batches can be up to twice this size (but never larger than this). In some cases batches can be smaller than this size. io_buffer_size: int, default None The size of the IO buffer. See ScannerBuilder.io_buffer_size for more information. batch_readahead: int, optional The number of batches to read ahead. fragment_readahead: int, optional The number of fragments to read ahead. scan_in_order: bool, default True Whether to read the fragments and batches in order. If false, throughput may be higher, but batches will be returned out of order and memory use might increase. fragments: iterable of LanceFragment, default None If specified, only scan these fragments. If scan_in_order is True, then the fragments will be scanned in the order given. prefilter: bool, default False If True then the filter will be applied before the vector query is run. This will generate more correct results but it may be a more costly query. It's generally good when the filter is highly selective. If False then the filter will be applied after the vector query is run. This will perform well but the results may have fewer than the requested number of rows (or be empty) if the rows closest to the query do not match the filter. It&#39;s generally good when the filter is not very selective. use_scalar_index: bool, default True Lance will automatically use scalar indices to optimize a query. In some corner cases this can make query performance worse and this parameter can be used to disable scalar indices in these cases. late_materialization: bool or List[str], default None Allows custom control over late materialization. Late materialization fetches non-query columns using a take operation after the filter. This is useful when there are few results or columns are very large. Early materialization can be better when there are many results or the columns are very narrow. If True, then all columns are late materialized. If False, then all columns are early materialized. If a list of strings, then only the columns in the list are late materialized. The default uses a heuristic that assumes filters will select about 0.1% of the rows. If your filter is more selective (e.g. find by id) you may want to set this to True. If your filter is not very selective (e.g. matches 20% of the rows) you may want to set this to False. full_text_query: str or dict, optional query string to search for, the results will be ranked by BM25. e.g. "hello world", would match documents containing "hello" or "world". or a dictionary with the following keys: - columns: list[str] The columns to search, currently only supports a single column in the columns list. - query: str The query string to search for. fast_search: bool, default False If True, then the search will only be performed on the indexed data, which yields faster search time. scan_stats_callback: Callable[[ScanStatistics], None], default None A callback function that will be called with the scan statistics after the scan is complete. Errors raised by the callback will be logged but not re-raised. include_deleted_rows: bool, default False If True, then rows that have been deleted, but are still present in the fragment, will be returned. These rows will have the _rowid column set to null. All other columns will reflect the value stored on disk and may not be null. Note: if this is a search operation, or a take operation (including scalar indexed scans) then deleted rows cannot be returned. .. note:: For now, if BOTH filter and nearest is specified, then: 1. nearest is executed first. 2. The results are filtered afterwards. For debugging ANN results, you can choose to not use the index even if present by specifying use_index=False. For example, the following will always return exact KNN results: .. code-block:: python dataset.to_table(nearest={ &quot;column&quot;: &quot;vector&quot;, &quot;k&quot;: 10, &quot;q&quot;: &lt;query vector&gt;, &quot;use_index&quot;: False } session() &para; Return the dataset session, which holds the dataset's state. take(indices, columns=None) &para; Select rows of data by index. Parameters&para; indices : Array or array-like indices of rows to select in the dataset. columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. Returns&para; table : pyarrow.Table take_blobs(blob_column, ids=None, addresses=None, indices=None) &para; Select blobs by row IDs. Instead of loading large binary blob data into memory before processing it, this API allows you to open binary blob data as a regular Python file-like object. For more details, see class:lance.BlobFile. Exactly one of ids, addresses, or indices must be specified. Parameters blob_column : str The name of the blob column to select. ids : Integer Array or array-like row IDs to select in the dataset. addresses: Integer Array or array-like The (unstable) row addresses to select in the dataset. indices : Integer Array or array-like The offset / indices of the row in the dataset. Returns&para; blob_files : List[BlobFile] to_batches(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, full_text_query=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, strict_batch_size=None, **kwargs) &para; Read the dataset as materialized record batches. Parameters&para; **kwargs : dict, optional Arguments for meth:~LanceDataset.scanner. Returns&para; record_batches : Iterator of class:~pyarrow.RecordBatch to_table(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, fast_search=None, full_text_query=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, include_deleted_rows=None) &para; Read the data into memory as a class:pyarrow.Table Parameters&para; columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. filter : pa.compute.Expression or str Expression or str that is a valid SQL where clause. See Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;_ for valid SQL expressions. limit: int, default None Fetch up to this many rows. All rows if None or unspecified. offset: int, default None Fetch starting with this row. 0 if None or unspecified. nearest: dict, default None Get the rows corresponding to the K most similar vectors. Example: .. code-block:: python { &quot;column&quot;: &lt;embedding col name&gt;, &quot;q&quot;: &lt;query vector as pa.Float32Array&gt;, &quot;k&quot;: 10, &quot;metric&quot;: &quot;cosine&quot;, &quot;nprobes&quot;: 1, &quot;refine_factor&quot;: 1 } int, optional The number of rows to read at a time. io_buffer_size: int, default None The size of the IO buffer. See ScannerBuilder.io_buffer_size for more information. batch_readahead: int, optional The number of batches to read ahead. fragment_readahead: int, optional The number of fragments to read ahead. scan_in_order: bool, optional, default True Whether to read the fragments and batches in order. If false, throughput may be higher, but batches will be returned out of order and memory use might increase. prefilter: bool, optional, default False Run filter before the vector search. late_materialization: bool or List[str], default None Allows custom control over late materialization. See ScannerBuilder.late_materialization for more information. use_scalar_index: bool, default True Allows custom control over scalar index usage. See ScannerBuilder.use_scalar_index for more information. with_row_id: bool, optional, default False Return row ID. with_row_address: bool, optional, default False Return row address use_stats: bool, optional, default True Use stats pushdown during filters. fast_search: bool, optional, default False full_text_query: str or dict, optional query string to search for, the results will be ranked by BM25. e.g. "hello world", would match documents contains "hello" or "world". or a dictionary with the following keys: - columns: list[str] The columns to search, currently only supports a single column in the columns list. - query: str The query string to search for. include_deleted_rows: bool, optional, default False If True, then rows that have been deleted, but are still present in the fragment, will be returned. These rows will have the _rowid column set to null. All other columns will reflect the value stored on disk and may not be null. Note: if this is a search operation, or a take operation (including scalar indexed scans) then deleted rows cannot be returned. Notes&para; If BOTH filter and nearest is specified, then: nearest is executed first. The results are filtered afterward, unless pre-filter sets to True. update(updates, where=None) &para; Update column values for rows matching where. Parameters&para; updates : dict of str to str A mapping of column names to a SQL expression. where : str, optional A SQL predicate indicating which rows should be updated. Returns&para; updates : dict A dictionary containing the number of rows updated. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") update_stats = dataset.update(dict(a = 'a + 2'), where="b != 'a'") update_stats["num_updated_rows"] = 2 dataset.to_table().to_pandas() a b 0 1 a 1 4 b 2 5 c validate() &para; Validate the dataset. This checks the integrity of the dataset and will raise an exception if the dataset is corrupted. versions() &para; Return all versions in this dataset. LanceOperation &para; Append dataclass &para; Bases: BaseOperation Append new rows to the dataset. Attributes&para; fragments: list[FragmentMetadata] The fragments that contain the new rows. Warning&para; This is an advanced API for distributed operations. To append to a dataset on a single machine, use :func:lance.write_dataset. Examples&para; To append new rows to a dataset, first use :meth:lance.fragment.LanceFragment.create to create fragments. Then collect the fragment metadata into a list and pass it to this class. Finally, pass the operation to the :meth:LanceDataset.commit method to create the new dataset. import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) dataset = lance.write_dataset(tab1, "example") tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment = lance.fragment.LanceFragment.create("example", tab2) operation = lance.LanceOperation.Append([fragment]) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d BaseOperation &para; Bases: ABC Base class for operations that can be applied to a dataset. See available operations under :class:LanceOperation. CreateIndex dataclass &para; Bases: BaseOperation Operation that creates an index on the dataset. DataReplacement dataclass &para; Bases: BaseOperation Operation that replaces existing datafiles in the dataset. DataReplacementGroup dataclass &para; Group of data replacements Delete dataclass &para; Bases: BaseOperation Remove fragments or rows from the dataset. Attributes&para; updated_fragments: list[FragmentMetadata] The fragments that have been updated with new deletion vectors. deleted_fragment_ids: list[int] The ids of the fragments that have been deleted entirely. These are the fragments where :meth:LanceFragment.delete() returned None. predicate: str The original SQL predicate used to select the rows to delete. Warning&para; This is an advanced API for distributed operations. To delete rows from dataset on a single machine, use :meth:lance.LanceDataset.delete. Examples&para; To delete rows from a dataset, call :meth:lance.fragment.LanceFragment.delete on each of the fragments. If that returns a new fragment, add that to the updated_fragments list. If it returns None, that means the whole fragment was deleted, so add the fragment id to the deleted_fragment_ids. Finally, pass the operation to the :meth:LanceDataset.commit method to complete the deletion operation. import lance import pyarrow as pa table = pa.table({"a": [1, 2], "b": ["a", "b"]}) dataset = lance.write_dataset(table, "example") table = pa.table({"a": [3, 4], "b": ["c", "d"]}) dataset = lance.write_dataset(table, "example", mode="append") dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d predicate = "a &gt;= 2" updated_fragments = [] deleted_fragment_ids = [] for fragment in dataset.get_fragments(): ... new_fragment = fragment.delete(predicate) ... if new_fragment is not None: ... updated_fragments.append(new_fragment) ... else: ... deleted_fragment_ids.append(fragment.fragment_id) operation = lance.LanceOperation.Delete(updated_fragments, ... deleted_fragment_ids, ... predicate) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b 0 1 a Merge dataclass &para; Bases: BaseOperation Operation that adds columns. Unlike Overwrite, this should not change the structure of the fragments, allowing existing indices to be kept. Attributes&para; fragments: iterable of FragmentMetadata The fragments that make up the new dataset. schema: LanceSchema or pyarrow.Schema The schema of the new dataset. Passing a LanceSchema is preferred, and passing a pyarrow.Schema is deprecated. Warning&para; This is an advanced API for distributed operations. To overwrite or create new dataset on a single machine, use :func:lance.write_dataset. Examples&para; To add new columns to a dataset, first define a method that will create the new columns based on the existing columns. Then use :meth:lance.fragment.LanceFragment.add_columns import lance import pyarrow as pa import pyarrow.compute as pc table = pa.table({"a": [1, 2, 3, 4], "b": ["a", "b", "c", "d"]}) dataset = lance.write_dataset(table, "example") dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d def double_a(batch: pa.RecordBatch) -&gt; pa.RecordBatch: ... doubled = pc.multiply(batch["a"], 2) ... return pa.record_batch([doubled], ["a_doubled"]) fragments = [] for fragment in dataset.get_fragments(): ... new_fragment, new_schema = fragment.merge_columns(double_a, ... columns=['a']) ... fragments.append(new_fragment) operation = lance.LanceOperation.Merge(fragments, new_schema) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b a_doubled 0 1 a 2 1 2 b 4 2 3 c 6 3 4 d 8 Overwrite dataclass &para; Bases: BaseOperation Overwrite or create a new dataset. Attributes&para; new_schema: pyarrow.Schema The schema of the new dataset. fragments: list[FragmentMetadata] The fragments that make up the new dataset. Warning&para; This is an advanced API for distributed operations. To overwrite or create new dataset on a single machine, use :func:lance.write_dataset. Examples&para; To create or overwrite a dataset, first use :meth:lance.fragment.LanceFragment.create to create fragments. Then collect the fragment metadata into a list and pass it along with the schema to this class. Finally, pass the operation to the :meth:LanceDataset.commit method to create the new dataset. import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment1 = lance.fragment.LanceFragment.create("example", tab1) fragment2 = lance.fragment.LanceFragment.create("example", tab2) fragments = [fragment1, fragment2] operation = lance.LanceOperation.Overwrite(tab1.schema, fragments) dataset = lance.LanceDataset.commit("example", operation) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d Project dataclass &para; Bases: BaseOperation Operation that project columns. Use this operator for drop column or rename/swap column. Attributes&para; schema: LanceSchema The lance schema of the new dataset. Examples&para; Use the projece operator to swap column: import lance import pyarrow as pa import pyarrow.compute as pc from lance.schema import LanceSchema table = pa.table({"a": [1, 2], "b": ["a", "b"], "b1": ["c", "d"]}) dataset = lance.write_dataset(table, "example") dataset.to_table().to_pandas() a b b1 0 1 a c 1 2 b d rename column b into b0 and rename b1 into b&para; table = pa.table({"a": [3, 4], "b0": ["a", "b"], "b": ["c", "d"]}) lance_schema = LanceSchema.from_pyarrow(table.schema) operation = lance.LanceOperation.Project(lance_schema) dataset = lance.LanceDataset.commit("example", operation, read_version=1) dataset.to_table().to_pandas() a b0 b 0 1 a c 1 2 b d Restore dataclass &para; Bases: BaseOperation Operation that restores a previous version of the dataset. Rewrite dataclass &para; Bases: BaseOperation Operation that rewrites one or more files and indices into one or more files and indices. Attributes&para; groups: list[RewriteGroup] Groups of files that have been rewritten. rewritten_indices: list[RewrittenIndex] Indices that have been rewritten. Warning&para; This is an advanced API not intended for general use. RewriteGroup dataclass &para; Collection of rewritten files RewrittenIndex dataclass &para; An index that has been rewritten Update dataclass &para; Bases: BaseOperation Operation that updates rows in the dataset. Attributes&para; removed_fragment_ids: list[int] The ids of the fragments that have been removed entirely. updated_fragments: list[FragmentMetadata] The fragments that have been updated with new deletion vectors. new_fragments: list[FragmentMetadata] The fragments that contain the new rows. LanceScanner &para; Bases: Scanner dataset_schema property &para; The schema with which batches will be read from fragments. analyze_plan() &para; Execute the plan for this scanner and display with runtime metrics. Parameters&para; verbose : bool, default False Use a verbose output format. Returns&para; plan : str count_rows() &para; Count rows matching the scanner filter. Returns&para; count : int explain_plan(verbose=False) &para; Return the execution plan for this scanner. Parameters&para; verbose : bool, default False Use a verbose output format. Returns&para; plan : str from_batches(*args, **kwargs) staticmethod &para; Not implemented from_dataset(*args, **kwargs) staticmethod &para; Not implemented from_fragment(*args, **kwargs) staticmethod &para; Not implemented head(num_rows) &para; Load the first N rows of the dataset. Parameters&para; num_rows : int The number of rows to load. Returns&para; Table scan_batches() &para; Consume a Scanner in record batches with corresponding fragments. Returns&para; record_batches : iterator of TaggedRecordBatch take(indices) &para; Not implemented to_table() &para; Read the data into memory and return a pyarrow Table. LanceStats &para; Statistics about a LanceDataset. data_stats() &para; Statistics about the data in the dataset. dataset_stats(max_rows_per_group=1024) &para; Statistics about the dataset. index_stats(index_name) &para; Statistics about an index. Parameters&para; index_name: str The name of the index to get statistics for. MergeInsertBuilder &para; Bases: _MergeInsertBuilder conflict_retries(max_retries) &para; Set number of times to retry the operation if there is contention. If this is set &gt; 0, then the operation will keep a copy of the input data either in memory or on disk (depending on the size of the data) and will retry the operation if there is contention. Default is 10. execute(data_obj, *, schema=None) &para; Executes the merge insert operation This function updates the original dataset and returns a dictionary with information about merge statistics - i.e. the number of inserted, updated, and deleted rows. Parameters&para; ReaderLike The new data to use as the source table for the operation. This parameter can be any source of data (e.g. table / dataset) that :func:~lance.write_dataset accepts. schema: Optional[pa.Schema] The schema of the data. This only needs to be supplied whenever the data source is some kind of generator. execute_uncommitted(data_obj, *, schema=None) &para; Executes the merge insert operation without committing This function updates the original dataset and returns a dictionary with information about merge statistics - i.e. the number of inserted, updated, and deleted rows. Parameters&para; ReaderLike The new data to use as the source table for the operation. This parameter can be any source of data (e.g. table / dataset) that :func:~lance.write_dataset accepts. schema: Optional[pa.Schema] The schema of the data. This only needs to be supplied whenever the data source is some kind of generator. retry_timeout(timeout) &para; Set the timeout used to limit retries. This is the maximum time to spend on the operation before giving up. At least one attempt will be made, regardless of how long it takes to complete. Subsequent attempts will be cancelled once this timeout is reached. If the timeout has been reached during the first attempt, the operation will be cancelled immediately before making a second attempt. The default is 30 seconds. when_matched_update_all(condition=None) &para; Configure the operation to update matched rows After this method is called, when the merge insert operation executes, any rows that match both the source table and the target table will be updated. The rows from the target table will be removed and the rows from the source table will be added. An optional condition may be specified. This should be an SQL filter and, if present, then only matched rows that also satisfy this filter will be updated. The SQL filter should use the prefix target. to refer to columns in the target table and the prefix source. to refer to columns in the source table. For example, source.last_update &lt; target.last_update. If a condition is specified and rows do not satisfy the condition then these rows will not be updated. Failure to satisfy the filter does not cause a "matched" row to become a "not matched" row. when_not_matched_by_source_delete(expr=None) &para; Configure the operation to delete source rows that do not match After this method is called, when the merge insert operation executes, any rows that exist only in the target table will be deleted. An optional filter can be specified to limit the scope of the delete operation. If given (as an SQL filter) then only rows which match the filter will be deleted. when_not_matched_insert_all() &para; Configure the operation to insert not matched rows After this method is called, when the merge insert operation executes, any rows that exist only in the source table will be inserted into the target table. ScannerBuilder &para; batch_readahead(nbatches=None) &para; This parameter is ignored when reading v2 files batch_size(batch_size) &para; Set batch size for Scanner fast_search(flag) &para; Enable fast search, which only perform search on the indexed data. Users can use Table::optimize() or create_index() to include the new data into index, thus make new data searchable. full_text_search(query, columns=None) &para; Filter rows by full text searching. Experimental API, may remove it after we support to do this within filter SQL-like expression Must create inverted index on the given column before searching, Parameters&para; query : str | Query If str, the query string to search for, a match query would be performed. If Query, the query object to search for, and the columns parameter will be ignored. columns : list of str, optional The columns to search in. If None, search in all indexed columns. include_deleted_rows(flag) &para; Include deleted rows Rows which have been deleted, but are still present in the fragment, will be returned. These rows will have all columns (except _rowaddr) set to null io_buffer_size(io_buffer_size) &para; Set the I/O buffer size for the Scanner This is the amount of RAM that will be reserved for holding I/O received from storage before it is processed. This is used to control the amount of memory used by the scanner. If the buffer is full then the scanner will block until the buffer is processed. Generally this should scale with the number of concurrent I/O threads. The default is 2GiB which comfortably provides enough space for somewhere between 32 and 256 concurrent I/O threads. This value is not a hard cap on the amount of RAM the scanner will use. Some space is used for the compute (which can be controlled by the batch size) and Lance does not keep track of memory after it is returned to the user. Currently, if there is a single batch of data which is larger than the io buffer size then the scanner will deadlock. This is a known issue and will be fixed in a future release. This parameter is only used when reading v2 files scan_in_order(scan_in_order=True) &para; Whether to scan the dataset in order of fragments and batches. If set to False, the scanner may read fragments concurrently and yield batches out of order. This may improve performance since it allows more concurrency in the scan, but can also use more memory. This parameter is ignored when using v2 files. In the v2 file format there is no penalty to scanning in order and so all scans will scan in order. scan_stats_callback(callback) &para; Set a callback function that will be called with the scan statistics after the scan is complete. Errors raised by the callback will be logged but not re-raised. strict_batch_size(strict_batch_size=False) &para; If True, then all batches except the last batch will have exactly batch_size rows. By default, it is false. If this is true then small batches will need to be merged together which will require a data copy and incur a (typically very small) performance penalty. use_scalar_index(use_scalar_index=True) &para; Set whether scalar indices should be used in a query Scans will use scalar indices, when available, to optimize queries with filters. However, in some corner cases, scalar indices may make performance worse. This parameter allows users to disable scalar indices in these cases. use_stats(use_stats=True) &para; Enable use of statistics for query planning. Disabling statistics is used for debugging and benchmarking purposes. This should be left on for normal use. with_row_address(with_row_address=True) &para; Enables returns with row addresses. Row addresses are a unique but unstable identifier for each row in the dataset that consists of the fragment id (upper 32 bits) and the row offset in the fragment (lower 32 bits). Row IDs are generally preferred since they do not change when a row is modified or compacted. However, row addresses may be useful in some advanced use cases. with_row_id(with_row_id=True) &para; Enable returns with row IDs. Tags &para; Dataset tag manager. create(tag, version) &para; Create a tag for a given dataset version. Parameters&para; tag: str, The name of the tag to create. This name must be unique among all tag names for the dataset. version: int, The dataset version to tag. delete(tag) &para; Delete tag from the dataset. Parameters&para; tag: str, The name of the tag to delete. list() &para; List all dataset tags. Returns&para; dict[str, Tag] A dictionary mapping tag names to version numbers. update(tag, version) &para; Update tag to a new version. Parameters&para; tag: str, The name of the tag to update. version: int, The new dataset version to tag. VectorIndexReader &para; This class allows you to initialize a reader for a specific vector index, retrieve the number of partitions, access the centroids of the index, and read specific partitions of the index. Parameters&para; dataset: LanceDataset The dataset containing the index. index_name: str The name of the vector index to read. Examples&para; .. code-block:: python import lance from lance.dataset import VectorIndexReader import numpy as np import pyarrow as pa vectors = np.random.rand(256, 2) data = pa.table({&quot;vector&quot;: pa.array(vectors.tolist(), type=pa.list_(pa.float32(), 2))}) dataset = lance.write_dataset(data, &quot;/tmp/index_reader_demo&quot;) dataset.create_index(&quot;vector&quot;, index_type=&quot;IVF_PQ&quot;, num_partitions=4, num_sub_vectors=2) reader = VectorIndexReader(dataset, &quot;vector_idx&quot;) assert reader.num_partitions() == 4 partition = reader.read_partition(0) assert &quot;_rowid&quot; in partition.column_names Exceptions&para; ValueError If the specified index is not a vector index. centroids() &para; Returns the centroids of the index Returns&para; np.ndarray The centroids of IVF with shape (num_partitions, dim) num_partitions() &para; Returns the number of partitions in the dataset. Returns&para; int The number of partitions. read_partition(partition_id, *, with_vector=False) &para; Returns a pyarrow table for the given IVF partition Parameters&para; partition_id: int The id of the partition to read with_vector: bool, default False Whether to include the vector column in the reader, for IVF_PQ, the vector column is PQ codes Returns&para; pa.Table A pyarrow table for the given partition, containing the row IDs, and quantized vectors (if with_vector is True). write_dataset(data_obj, uri, schema=None, mode=&#39;create&#39;, *, max_rows_per_file=1024 * 1024, max_rows_per_group=1024, max_bytes_per_file=90 * 1024 * 1024 * 1024, commit_lock=None, progress=None, storage_options=None, data_storage_version=None, use_legacy_format=None, enable_v2_manifest_paths=False, enable_move_stable_row_ids=False) &para; Write a given data_obj to the given uri Parameters&para; data_obj: Reader-like The data to be written. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader - Huggingface dataset uri: str, Path, or LanceDataset Where to write the dataset to (directory). If a LanceDataset is passed, the session will be reused. schema: Schema, optional If specified and the input is a pandas DataFrame, use this schema instead of the default pandas to arrow table conversion. mode: str create - create a new dataset (raises if uri already exists). overwrite - create a new snapshot version append - create a new version that is the concat of the input the latest version (raises if uri does not exist) max_rows_per_file: int, default 1024 * 1024 The max number of rows to write before starting a new file max_rows_per_group: int, default 1024 The max number of rows before starting a new group (in the same file) max_bytes_per_file: int, default 90 * 1024 * 1024 * 1024 The max number of bytes to write before starting a new file. This is a soft limit. This limit is checked after each group is written, which means larger groups may cause this to be overshot meaningfully. This defaults to 90 GB, since we have a hard limit of 100 GB per file on object stores. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. progress: FragmentWriteProgress, optional Experimental API. Progress tracking for writing the fragment. Pass a custom class that defines hooks to be called when each fragment is starting to write and finishing writing. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. data_storage_version: optional, str, default None The version of the data storage format to use. Newer versions are more efficient but require newer versions of lance to read. The default (None) will use the latest stable version. See the user guide for more details. use_legacy_format : optional, bool, default None Deprecated method for setting the data storage version. Use the data_storage_version parameter instead. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:LanceDataset.migrate_manifest_paths_v2 method. Default is False. enable_move_stable_row_ids : bool, optional Experimental parameter: if set to true, the writer will use move-stable row ids. These row ids are stable after compaction operations, but not after updates. This makes compaction more efficient, since with stable row ids no secondary indices need to be updated to point to new row ids. and rename b1 into
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Restore" class="md-nav__link">
    <span class="md-ellipsis">
      Restore
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Rewrite" class="md-nav__link">
    <span class="md-ellipsis">
      Rewrite
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Rewrite">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Rewrite--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Rewrite--warning" class="md-nav__link">
    <span class="md-ellipsis">
      Warning
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.RewriteGroup" class="md-nav__link">
    <span class="md-ellipsis">
      RewriteGroup
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.RewrittenIndex" class="md-nav__link">
    <span class="md-ellipsis">
      RewrittenIndex
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Update" class="md-nav__link">
    <span class="md-ellipsis">
      Update
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Update--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner" class="md-nav__link">
    <span class="md-ellipsis">
      LanceScanner
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LanceScanner">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.dataset_schema" class="md-nav__link">
    <span class="md-ellipsis">
      dataset_schema
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.analyze_plan" class="md-nav__link">
    <span class="md-ellipsis">
      analyze_plan
    </span>
  </a>
  
    <nav class="md-nav" aria-label="analyze_plan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.analyze_plan--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.analyze_plan--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.count_rows" class="md-nav__link">
    <span class="md-ellipsis">
      count_rows
    </span>
  </a>
  
    <nav class="md-nav" aria-label="count_rows">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.count_rows--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.explain_plan" class="md-nav__link">
    <span class="md-ellipsis">
      explain_plan
    </span>
  </a>
  
    <nav class="md-nav" aria-label="explain_plan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.explain_plan--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.explain_plan--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.from_batches" class="md-nav__link">
    <span class="md-ellipsis">
      from_batches
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.from_dataset" class="md-nav__link">
    <span class="md-ellipsis">
      from_dataset
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.from_fragment" class="md-nav__link">
    <span class="md-ellipsis">
      from_fragment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.head" class="md-nav__link">
    <span class="md-ellipsis">
      head
    </span>
  </a>
  
    <nav class="md-nav" aria-label="head">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.head--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.head--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.scan_batches" class="md-nav__link">
    <span class="md-ellipsis">
      scan_batches
    </span>
  </a>
  
    <nav class="md-nav" aria-label="scan_batches">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.scan_batches--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.take" class="md-nav__link">
    <span class="md-ellipsis">
      take
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.to_table" class="md-nav__link">
    <span class="md-ellipsis">
      to_table
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.MergeInsertBuilder" class="md-nav__link">
    <span class="md-ellipsis">
      MergeInsertBuilder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MergeInsertBuilder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.MergeInsertBuilder.conflict_retries" class="md-nav__link">
    <span class="md-ellipsis">
      conflict_retries
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.MergeInsertBuilder.execute" class="md-nav__link">
    <span class="md-ellipsis">
      execute
    </span>
  </a>
  
    <nav class="md-nav" aria-label="execute">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.MergeInsertBuilder.execute--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.MergeInsertBuilder.execute_uncommitted" class="md-nav__link">
    <span class="md-ellipsis">
      execute_uncommitted
    </span>
  </a>
  
    <nav class="md-nav" aria-label="execute_uncommitted">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.MergeInsertBuilder.execute_uncommitted--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.MergeInsertBuilder.retry_timeout" class="md-nav__link">
    <span class="md-ellipsis">
      retry_timeout
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.MergeInsertBuilder.when_matched_update_all" class="md-nav__link">
    <span class="md-ellipsis">
      when_matched_update_all
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.MergeInsertBuilder.when_not_matched_by_source_delete" class="md-nav__link">
    <span class="md-ellipsis">
      when_not_matched_by_source_delete
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.MergeInsertBuilder.when_not_matched_insert_all" class="md-nav__link">
    <span class="md-ellipsis">
      when_not_matched_insert_all
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.batch_udf" class="md-nav__link">
    <span class="md-ellipsis">
      batch_udf
    </span>
  </a>
  
    <nav class="md-nav" aria-label="batch_udf">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.batch_udf--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.batch_udf--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.json_to_schema" class="md-nav__link">
    <span class="md-ellipsis">
      json_to_schema
    </span>
  </a>
  
    <nav class="md-nav" aria-label="json_to_schema">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.json_to_schema--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.schema_to_json" class="md-nav__link">
    <span class="md-ellipsis">
      schema_to_json
    </span>
  </a>
  
    <nav class="md-nav" aria-label="schema_to_json">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.schema_to_json--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.write_dataset" class="md-nav__link">
    <span class="md-ellipsis">
      write_dataset
    </span>
  </a>
  
    <nav class="md-nav" aria-label="write_dataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.write_dataset--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lancedataset" class="md-nav__link">
    <span class="md-ellipsis">
      lance.dataset
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lance.dataset" class="md-nav__link">
    <span class="md-ellipsis">
      dataset
    </span>
  </a>
  
    <nav class="md-nav" aria-label="dataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.DataStatistics" class="md-nav__link">
    <span class="md-ellipsis">
      DataStatistics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.DatasetOptimizer" class="md-nav__link">
    <span class="md-ellipsis">
      DatasetOptimizer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="DatasetOptimizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.DatasetOptimizer.compact_files" class="md-nav__link">
    <span class="md-ellipsis">
      compact_files
    </span>
  </a>
  
    <nav class="md-nav" aria-label="compact_files">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.DatasetOptimizer.compact_files--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.DatasetOptimizer.compact_files--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.DatasetOptimizer.compact_files--see-also" class="md-nav__link">
    <span class="md-ellipsis">
      See Also
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.DatasetOptimizer.optimize_indices" class="md-nav__link">
    <span class="md-ellipsis">
      optimize_indices
    </span>
  </a>
  
    <nav class="md-nav" aria-label="optimize_indices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.DatasetOptimizer.optimize_indices--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.FieldStatistics" class="md-nav__link">
    <span class="md-ellipsis">
      FieldStatistics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset" class="md-nav__link">
    <span class="md-ellipsis">
      LanceDataset
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LanceDataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.data_storage_version" class="md-nav__link">
    <span class="md-ellipsis">
      data_storage_version
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.lance_schema" class="md-nav__link">
    <span class="md-ellipsis">
      lance_schema
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.latest_version" class="md-nav__link">
    <span class="md-ellipsis">
      latest_version
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.max_field_id" class="md-nav__link">
    <span class="md-ellipsis">
      max_field_id
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.partition_expression" class="md-nav__link">
    <span class="md-ellipsis">
      partition_expression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.schema" class="md-nav__link">
    <span class="md-ellipsis">
      schema
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.stats" class="md-nav__link">
    <span class="md-ellipsis">
      stats
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.tags" class="md-nav__link">
    <span class="md-ellipsis">
      tags
    </span>
  </a>
  
    <nav class="md-nav" aria-label="tags">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.tags--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.uri" class="md-nav__link">
    <span class="md-ellipsis">
      uri
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.version" class="md-nav__link">
    <span class="md-ellipsis">
      version
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.add_columns" class="md-nav__link">
    <span class="md-ellipsis">
      add_columns
    </span>
  </a>
  
    <nav class="md-nav" aria-label="add_columns">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.add_columns--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.add_columns--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.add_columns--see-also" class="md-nav__link">
    <span class="md-ellipsis">
      See Also
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.alter_columns" class="md-nav__link">
    <span class="md-ellipsis">
      alter_columns
    </span>
  </a>
  
    <nav class="md-nav" aria-label="alter_columns">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.alter_columns--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.alter_columns--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.checkout_version" class="md-nav__link">
    <span class="md-ellipsis">
      checkout_version
    </span>
  </a>
  
    <nav class="md-nav" aria-label="checkout_version">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.checkout_version--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.checkout_version--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.cleanup_old_versions" class="md-nav__link">
    <span class="md-ellipsis">
      cleanup_old_versions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="cleanup_old_versions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.cleanup_old_versions--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.commit" class="md-nav__link">
    <span class="md-ellipsis">
      commit
    </span>
  </a>
  
    <nav class="md-nav" aria-label="commit">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.commit--warnings" class="md-nav__link">
    <span class="md-ellipsis">
      Warnings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.commit--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.commit--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.commit--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.commit_batch" class="md-nav__link">
    <span class="md-ellipsis">
      commit_batch
    </span>
  </a>
  
    <nav class="md-nav" aria-label="commit_batch">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.commit_batch--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.commit_batch--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.count_rows" class="md-nav__link">
    <span class="md-ellipsis">
      count_rows
    </span>
  </a>
  
    <nav class="md-nav" aria-label="count_rows">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.count_rows--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.count_rows--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.create_index" class="md-nav__link">
    <span class="md-ellipsis">
      create_index
    </span>
  </a>
  
    <nav class="md-nav" aria-label="create_index">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.create_index--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.create_index--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.create_index--references" class="md-nav__link">
    <span class="md-ellipsis">
      References
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.create_scalar_index" class="md-nav__link">
    <span class="md-ellipsis">
      create_scalar_index
    </span>
  </a>
  
    <nav class="md-nav" aria-label="create_scalar_index">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.create_scalar_index--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.create_scalar_index--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.delete" class="md-nav__link">
    <span class="md-ellipsis">
      delete
    </span>
  </a>
  
    <nav class="md-nav" aria-label="delete">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.delete--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.delete--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.drop_columns" class="md-nav__link">
    <span class="md-ellipsis">
      drop_columns
    </span>
  </a>
  
    <nav class="md-nav" aria-label="drop_columns">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.drop_columns--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.drop_columns--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.drop_index" class="md-nav__link">
    <span class="md-ellipsis">
      drop_index
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.get_fragment" class="md-nav__link">
    <span class="md-ellipsis">
      get_fragment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.get_fragments" class="md-nav__link">
    <span class="md-ellipsis">
      get_fragments
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.head" class="md-nav__link">
    <span class="md-ellipsis">
      head
    </span>
  </a>
  
    <nav class="md-nav" aria-label="head">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.head--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.head--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.insert" class="md-nav__link">
    <span class="md-ellipsis">
      insert
    </span>
  </a>
  
    <nav class="md-nav" aria-label="insert">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.insert--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.join" class="md-nav__link">
    <span class="md-ellipsis">
      join
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.merge" class="md-nav__link">
    <span class="md-ellipsis">
      merge
    </span>
  </a>
  
    <nav class="md-nav" aria-label="merge">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.merge--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.merge--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.merge--see-also" class="md-nav__link">
    <span class="md-ellipsis">
      See Also
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.merge_insert" class="md-nav__link">
    <span class="md-ellipsis">
      merge_insert
    </span>
  </a>
  
    <nav class="md-nav" aria-label="merge_insert">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.merge_insert--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.merge_insert--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.merge_insert--perform-a-upsert-operation" class="md-nav__link">
    <span class="md-ellipsis">
      Perform a "upsert" operation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.merge_insert--perform-an-insert-if-not-exists-operation" class="md-nav__link">
    <span class="md-ellipsis">
      Perform an "insert if not exists" operation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.merge_insert--perform-an-upsert-operation-only-updating-column-a" class="md-nav__link">
    <span class="md-ellipsis">
      Perform an "upsert" operation, only updating column "a"
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.migrate_manifest_paths_v2" class="md-nav__link">
    <span class="md-ellipsis">
      migrate_manifest_paths_v2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.prewarm_index" class="md-nav__link">
    <span class="md-ellipsis">
      prewarm_index
    </span>
  </a>
  
    <nav class="md-nav" aria-label="prewarm_index">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.prewarm_index--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.replace_field_metadata" class="md-nav__link">
    <span class="md-ellipsis">
      replace_field_metadata
    </span>
  </a>
  
    <nav class="md-nav" aria-label="replace_field_metadata">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.replace_field_metadata--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.replace_schema" class="md-nav__link">
    <span class="md-ellipsis">
      replace_schema
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.replace_schema_metadata" class="md-nav__link">
    <span class="md-ellipsis">
      replace_schema_metadata
    </span>
  </a>
  
    <nav class="md-nav" aria-label="replace_schema_metadata">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.replace_schema_metadata--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.restore" class="md-nav__link">
    <span class="md-ellipsis">
      restore
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.sample" class="md-nav__link">
    <span class="md-ellipsis">
      sample
    </span>
  </a>
  
    <nav class="md-nav" aria-label="sample">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.sample--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.sample--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.scanner" class="md-nav__link">
    <span class="md-ellipsis">
      scanner
    </span>
  </a>
  
    <nav class="md-nav" aria-label="scanner">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.scanner--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.session" class="md-nav__link">
    <span class="md-ellipsis">
      session
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.take" class="md-nav__link">
    <span class="md-ellipsis">
      take
    </span>
  </a>
  
    <nav class="md-nav" aria-label="take">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.take--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.take--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.take_blobs" class="md-nav__link">
    <span class="md-ellipsis">
      take_blobs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="take_blobs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.take_blobs--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.to_batches" class="md-nav__link">
    <span class="md-ellipsis">
      to_batches
    </span>
  </a>
  
    <nav class="md-nav" aria-label="to_batches">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.to_batches--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.to_batches--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.to_table" class="md-nav__link">
    <span class="md-ellipsis">
      to_table
    </span>
  </a>
  
    <nav class="md-nav" aria-label="to_table">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.to_table--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.to_table--notes" class="md-nav__link">
    <span class="md-ellipsis">
      Notes
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.update" class="md-nav__link">
    <span class="md-ellipsis">
      update
    </span>
  </a>
  
    <nav class="md-nav" aria-label="update">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.update--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.update--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.update--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.validate" class="md-nav__link">
    <span class="md-ellipsis">
      validate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.versions" class="md-nav__link">
    <span class="md-ellipsis">
      versions
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation" class="md-nav__link">
    <span class="md-ellipsis">
      LanceOperation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LanceOperation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Append" class="md-nav__link">
    <span class="md-ellipsis">
      Append
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Append">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Append--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Append--warning" class="md-nav__link">
    <span class="md-ellipsis">
      Warning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Append--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.BaseOperation" class="md-nav__link">
    <span class="md-ellipsis">
      BaseOperation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.CreateIndex" class="md-nav__link">
    <span class="md-ellipsis">
      CreateIndex
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.DataReplacement" class="md-nav__link">
    <span class="md-ellipsis">
      DataReplacement
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.DataReplacementGroup" class="md-nav__link">
    <span class="md-ellipsis">
      DataReplacementGroup
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Delete" class="md-nav__link">
    <span class="md-ellipsis">
      Delete
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Delete">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Delete--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Delete--warning" class="md-nav__link">
    <span class="md-ellipsis">
      Warning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Delete--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Merge" class="md-nav__link">
    <span class="md-ellipsis">
      Merge
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Merge">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Merge--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Merge--warning" class="md-nav__link">
    <span class="md-ellipsis">
      Warning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Merge--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Overwrite" class="md-nav__link">
    <span class="md-ellipsis">
      Overwrite
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Overwrite">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Overwrite--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Overwrite--warning" class="md-nav__link">
    <span class="md-ellipsis">
      Warning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Overwrite--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Project" class="md-nav__link">
    <span class="md-ellipsis">
      Project
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Project">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Project--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Project--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Project--rename-column-b-into-b0-and-rename-b1-into-b" class="md-nav__link">
    <span class="md-ellipsis">
      rename column lance &para; __version__ = &#39;0.27.3&#39; module &para; str(object='') -&gt; str str(bytes_or_buffer[, encoding[, errors]]) -&gt; str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object.str() (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. BlobColumn &para; A utility to wrap a Pyarrow binary column and iterate over the rows as file-like objects. This can be useful for working with medium-to-small binary objects that need to interface with APIs that expect file-like objects. For very large binary objects (4-8MB or more per value) you might be better off creating a blob column and using meth:lance.Dataset.take_blobs to access the blob data. BlobFile &para; Bases: RawIOBase Represents a blob in a Lance dataset as a file-like object. __init__(inner) &para; Internal only: To obtain a BlobFile use meth:lance.dataset.Dataset.take_blobs. size() &para; Returns the size of the blob in bytes. DataStatistics dataclass &para; Statistics about the data in the dataset FieldStatistics dataclass &para; Statistics about a field in the dataset FragmentMetadata dataclass &para; Metadata for a fragment. Attributes&para; id : int The ID of the fragment. files : List[DataFile] The data files of the fragment. Each data file must have the same number of rows. Each file stores a different subset of the columns. physical_rows : int The number of rows originally in this fragment. This is the number of rows in the data files before deletions. deletion_file : Optional[DeletionFile] The deletion file, if any. row_id_meta : Optional[RowIdMeta] The row id metadata, if any. num_deletions property &para; The number of rows that have been deleted from this fragment. num_rows property &para; The number of rows in this fragment after deletions. to_json() &para; Get this as a simple JSON-serializable dictionary. LanceDataset &para; Bases: Dataset A Lance Dataset in Lance format where the data is stored at the given uri. data_storage_version property &para; The version of the data storage format this dataset is using lance_schema property &para; The LanceSchema for this dataset latest_version property &para; Returns the latest version of the dataset. max_field_id property &para; The max_field_id in manifest partition_expression property &para; Not implemented (just override pyarrow dataset to prevent segfault) schema property &para; The pyarrow Schema for this dataset stats property &para; Experimental API tags property &para; Tag management for the dataset. Similar to Git, tags are a way to add metadata to a specific version of the dataset. .. warning:: Tagged versions are exempted from the :py:meth:`cleanup_old_versions()` process. To remove a version that has been tagged, you must first :py:meth:`~Tags.delete` the associated tag. Examples&para; .. code-block:: python ds = lance.open(&quot;dataset.lance&quot;) ds.tags.create(&quot;v2-prod-20250203&quot;, 10) tags = ds.tags.list() uri property &para; The location of the data version property &para; Returns the currently checked out version of the dataset add_columns(transforms, read_columns=None, reader_schema=None, batch_size=None) &para; Add new columns with defined values. There are several ways to specify the new columns. First, you can provide SQL expressions for each new column. Second you can provide a UDF that takes a batch of existing data and returns a new batch with the new columns. These new columns will be appended to the dataset. You can also provide a RecordBatchReader which will read the new column values from some external source. This is often useful when the new column values have already been staged to files (often by some distributed process) See the :func:lance.add_columns_udf decorator for more information on writing UDFs. Parameters&para; transforms : dict or AddColumnsUDF or ReaderLike If this is a dictionary, then the keys are the names of the new columns and the values are SQL expression strings. These strings can reference existing columns in the dataset. If this is a AddColumnsUDF, then it is a UDF that takes a batch of existing data and returns a new batch with the new columns. If this is :class:pyarrow.Field or :class:pyarrow.Schema, it adds all NULL columns with the given schema, in a metadata-only operation. read_columns : list of str, optional The names of the columns that the UDF will read. If None, then the UDF will read all columns. This is only used when transforms is a UDF. Otherwise, the read columns are inferred from the SQL expressions. reader_schema: pa.Schema, optional Only valid if transforms is a ReaderLike object. This will be used to determine the schema of the reader. batch_size: int, optional The number of rows to read at a time from the source dataset when applying the transform. This is ignored if the dataset is a v1 dataset. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3]}) dataset = lance.write_dataset(table, "my_dataset") @lance.batch_udf() ... def double_a(batch): ... df = batch.to_pandas() ... return pd.DataFrame({'double_a': 2 * df['a']}) dataset.add_columns(double_a) dataset.to_table().to_pandas() a double_a 0 1 2 1 2 4 2 3 6 dataset.add_columns({"triple_a": "a * 3"}) dataset.to_table().to_pandas() a double_a triple_a 0 1 2 3 1 2 4 6 2 3 6 9 See Also&para; LanceDataset.merge : Merge a pre-computed set of columns into the dataset. alter_columns(*alterations) &para; Alter column name, data type, and nullability. Columns that are renamed can keep any indices that are on them. If a column has an IVF_PQ index, it can be kept if the column is casted to another type. However, other index types don't support casting at this time. Column types can be upcasted (such as int32 to int64) or downcasted (such as int64 to int32). However, downcasting will fail if there are any values that cannot be represented in the new type. In general, columns can be casted to same general type: integers to integers, floats to floats, and strings to strings. However, strings, binary, and list columns can be casted between their size variants. For example, string to large string, binary to large binary, and list to large list. Columns that are renamed can keep any indices that are on them. However, if the column is casted to a different type, its indices will be dropped. Parameters&para; alterations : Iterable[Dict[str, Any]] A sequence of dictionaries, each with the following keys: - &quot;path&quot;: str The column path to alter. For a top-level column, this is the name. For a nested column, this is the dot-separated path, e.g. &quot;a.b.c&quot;. - &quot;name&quot;: str, optional The new name of the column. If not specified, the column name is not changed. - &quot;nullable&quot;: bool, optional Whether the column should be nullable. If not specified, the column nullability is not changed. Only non-nullable columns can be changed to nullable. Currently, you cannot change a nullable column to non-nullable. - &quot;data_type&quot;: pyarrow.DataType, optional The new data type to cast the column to. If not specified, the column data type is not changed. Examples&para; import lance import pyarrow as pa schema = pa.schema([pa.field('a', pa.int64()), ... pa.field('b', pa.string(), nullable=False)]) table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.alter_columns({"path": "a", "name": "x"}, ... {"path": "b", "nullable": True}) dataset.to_table().to_pandas() x b 0 1 a 1 2 b 2 3 c dataset.alter_columns({"path": "x", "data_type": pa.int32()}) dataset.schema x: int32 b: string checkout_version(version) &para; Load the given version of the dataset. Unlike the :func:dataset constructor, this will re-use the current cache. This is a no-op if the dataset is already at the given version. Parameters&para; version: int | str, The version to check out. A version number (int) or a tag (str) can be provided. Returns&para; LanceDataset cleanup_old_versions(older_than=None, *, delete_unverified=False, error_if_tagged_old_versions=True) &para; Cleans up old versions of the dataset. Some dataset changes, such as overwriting, leave behind data that is not referenced by the latest dataset version. The old data is left in place to allow the dataset to be restored back to an older version. This method will remove older versions and any data files they reference. Once this cleanup task has run you will not be able to checkout or restore these older versions. Parameters&para; timedelta, optional Only versions older than this will be removed. If not specified, this will default to two weeks. bool, default False Files leftover from a failed transaction may appear to be part of an in-progress operation (e.g. appending new data) and these files will not be deleted unless they are at least 7 days old. If delete_unverified is True then these files will be deleted regardless of their age. This should only be set to True if you can guarantee that no other process is currently working on this dataset. Otherwise the dataset could be put into a corrupted state. bool, default True Some versions may have tags associated with them. Tagged versions will not be cleaned up, regardless of how old they are. If this argument is set to True (the default), an exception will be raised if any tagged versions match the parameters. Otherwise, tagged versions will be ignored without any error and only untagged versions will be cleaned up. commit(base_uri, operation, blobs_op=None, read_version=None, commit_lock=None, storage_options=None, enable_v2_manifest_paths=None, detached=False, max_retries=20) staticmethod &para; Create a new version of dataset This method is an advanced method which allows users to describe a change that has been made to the data files. This method is not needed when using Lance to apply changes (e.g. when using class:LanceDataset or func:write_dataset.) It's current purpose is to allow for changes being made in a distributed environment where no single process is doing all of the work. For example, a distributed bulk update or a distributed bulk modify operation. Once all of the changes have been made, this method can be called to make the changes visible by updating the dataset manifest. Warnings&para; This is an advanced API and doesn't provide the same level of validation as the other APIs. For example, it's the responsibility of the caller to ensure that the fragments are valid for the schema. Parameters&para; base_uri: str, Path, or LanceDataset The base uri of the dataset, or the dataset object itself. Using the dataset object can be more efficient because it can re-use the file metadata cache. operation: BaseOperation The operation to apply to the dataset. This describes what changes have been made. See available operations under :class:LanceOperation. read_version: int, optional The version of the dataset that was used as the base for the changes. This is not needed for overwrite or restore operations. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:migrate_manifest_paths_v2 method. Default is False. WARNING: turning this on will make the dataset unreadable for older versions of Lance (prior to 0.17.0). detached : bool, optional If True, then the commit will not be part of the dataset lineage. It will never show up as the latest dataset and the only way to check it out in the future will be to specifically check it out by version. The version will be a random version that is only unique amongst detached commits. The caller should store this somewhere as there will be no other way to obtain it in the future. max_retries : int The maximum number of retries to perform when committing the dataset. Returns&para; LanceDataset A new version of Lance Dataset. Examples&para; Creating a new dataset with the :class:LanceOperation.Overwrite operation: import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment1 = lance.fragment.LanceFragment.create("example", tab1) fragment2 = lance.fragment.LanceFragment.create("example", tab2) fragments = [fragment1, fragment2] operation = lance.LanceOperation.Overwrite(tab1.schema, fragments) dataset = lance.LanceDataset.commit("example", operation) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d commit_batch(dest, transactions, commit_lock=None, storage_options=None, enable_v2_manifest_paths=None, detached=False, max_retries=20) staticmethod &para; Create a new version of dataset with multiple transactions. This method is an advanced method which allows users to describe a change that has been made to the data files. This method is not needed when using Lance to apply changes (e.g. when using class:LanceDataset or func:write_dataset.) Parameters&para; dest: str, Path, or LanceDataset The base uri of the dataset, or the dataset object itself. Using the dataset object can be more efficient because it can re-use the file metadata cache. transactions: Iterable[Transaction] The transactions to apply to the dataset. These will be merged into a single transaction and applied to the dataset. Note: Only append transactions are currently supported. Other transaction types will be supported in the future. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:migrate_manifest_paths_v2 method. Default is False. WARNING: turning this on will make the dataset unreadable for older versions of Lance (prior to 0.17.0). detached : bool, optional If True, then the commit will not be part of the dataset lineage. It will never show up as the latest dataset and the only way to check it out in the future will be to specifically check it out by version. The version will be a random version that is only unique amongst detached commits. The caller should store this somewhere as there will be no other way to obtain it in the future. max_retries : int The maximum number of retries to perform when committing the dataset. Returns&para; dict with keys: dataset: LanceDataset A new version of Lance Dataset. merged: Transaction The merged transaction that was applied to the dataset. count_rows(filter=None, **kwargs) &para; Count rows matching the scanner filter. Parameters&para; **kwargs : dict, optional See py:method:scanner method for full parameter description. Returns&para; count : int The total number of rows in the dataset. create_index(column, index_type, name=None, metric=&#39;L2&#39;, replace=False, num_partitions=None, ivf_centroids=None, pq_codebook=None, num_sub_vectors=None, accelerator=None, index_cache_size=None, shuffle_partition_batches=None, shuffle_partition_concurrency=None, ivf_centroids_file=None, precomputed_partition_dataset=None, storage_options=None, filter_nan=True, one_pass_ivfpq=False, **kwargs) &para; Create index on column. Experimental API Parameters&para; column : str The column to be indexed. index_type : str The type of the index. "IVF_PQ, IVF_HNSW_PQ and IVF_HNSW_SQ" are supported now. name : str, optional The index name. If not provided, it will be generated from the column name. metric : str The distance metric type, i.e., "L2" (alias to "euclidean"), "cosine" or "dot" (dot product). Default is "L2". replace : bool Replace the existing index if it exists. num_partitions : int, optional The number of partitions of IVF (Inverted File Index). ivf_centroids : optional It can be either class:np.ndarray, class:pyarrow.FixedSizeListArray or class:pyarrow.FixedShapeTensorArray. A num_partitions x dimension array of existing K-mean centroids for IVF clustering. If not provided, a new KMeans model will be trained. pq_codebook : optional, It can be class:np.ndarray, class:pyarrow.FixedSizeListArray, or class:pyarrow.FixedShapeTensorArray. A num_sub_vectors x (2 ^ nbits * dimensions // num_sub_vectors) array of K-mean centroids for PQ codebook. Note: ``nbits`` is always 8 for now. If not provided, a new PQ model will be trained. num_sub_vectors : int, optional The number of sub-vectors for PQ (Product Quantization). accelerator : str or torch.Device, optional If set, use an accelerator to speed up the training process. Accepted accelerator: "cuda" (Nvidia GPU) and "mps" (Apple Silicon GPU). If not set, use the CPU. index_cache_size : int, optional The size of the index cache in number of entries. Default value is 256. shuffle_partition_batches : int, optional The number of batches, using the row group size of the dataset, to include in each shuffle partition. Default value is 10240. Assuming the row group size is 1024, each shuffle partition will hold 10240 * 1024 = 10,485,760 rows. By making this value smaller, this shuffle will consume less memory but will take longer to complete, and vice versa. shuffle_partition_concurrency : int, optional The number of shuffle partitions to process concurrently. Default value is 2 By making this value smaller, this shuffle will consume less memory but will take longer to complete, and vice versa. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. filter_nan: bool Defaults to True. False is UNSAFE, and will cause a crash if any null/nan values are present (and otherwise will not). Disables the null filter used for nullable columns. Obtains a small speed boost. one_pass_ivfpq: bool Defaults to False. If enabled, index type must be "IVF_PQ". Reduces disk IO. kwargs : Parameters passed to the index building process. The SQ (Scalar Quantization) is available for only IVF_HNSW_SQ index type, this quantization method is used to reduce the memory usage of the index, it maps the float vectors to integer vectors, each integer is of num_bits, now only 8 bits are supported. If index_type is "IVF_*", then the following parameters are required: num_partitions If index_type is with "PQ", then the following parameters are required: num_sub_vectors Optional parameters for IVF_PQ: - ivf_centroids Existing K-mean centroids for IVF clustering. - num_bits The number of bits for PQ (Product Quantization). Default is 8. Only 4, 8 are supported. Optional parameters for IVF_HNSW_*: max_level Int, the maximum number of levels in the graph. m Int, the number of edges per node in the graph. ef_construction Int, the number of nodes to examine during the construction. Examples&para; .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_PQ&quot;, num_partitions=256, num_sub_vectors=16 ) .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_HNSW_SQ&quot;, num_partitions=256, ) Experimental Accelerator (GPU) support: accelerate: use GPU to train IVF partitions. Only supports CUDA (Nvidia) or MPS (Apple) currently. Requires PyTorch being installed. .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_PQ&quot;, num_partitions=256, num_sub_vectors=16, accelerator=&quot;cuda&quot; ) References&para; Faiss Index &lt;https://github.com/facebookresearch/faiss/wiki/Faiss-indexes&gt;_ IVF introduced in Video Google: a text retrieval approach to object matching in videos &lt;https://ieeexplore.ieee.org/abstract/document/1238663&gt;_ Product quantization for nearest neighbor search &lt;https://hal.inria.fr/inria-00514462v2/document&gt;_ create_scalar_index(column, index_type, name=None, *, replace=True, **kwargs) &para; Create a scalar index on a column. Scalar indices, like vector indices, can be used to speed up scans. A scalar index can speed up scans that contain filter expressions on the indexed column. For example, the following scan will be faster if the column my_col has a scalar index: .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) my_table = dataset.scanner(filter=&quot;my_col != 7&quot;).to_table() Vector search with pre-filers can also benefit from scalar indices. For example, .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) my_table = dataset.scanner( nearest=dict( column=&quot;vector&quot;, q=[1, 2, 3, 4], k=10, ) filter=&quot;my_col != 7&quot;, prefilter=True ) There are 5 types of scalar indices available today. BTREE. The most common type is BTREE. This index is inspired by the btree data structure although only the first few layers of the btree are cached in memory. It will perform well on columns with a large number of unique values and few rows per value. BITMAP. This index stores a bitmap for each unique value in the column. This index is useful for columns with a small number of unique values and many rows per value. LABEL_LIST. A special index that is used to index list columns whose values have small cardinality. For example, a column that contains lists of tags (e.g. ["tag1", "tag2", "tag3"]) can be indexed with a LABEL_LIST index. This index can only speedup queries with array_has_any or array_has_all filters. NGRAM. A special index that is used to index string columns. This index creates a bitmap for each ngram in the string. By default we use trigrams. This index can currently speed up queries using the contains function in filters. FTS/INVERTED. It is used to index document columns. This index can conduct full-text searches. For example, a column that contains any word of query string "hello world". The results will be ranked by BM25. Note that the LANCE_BYPASS_SPILLING environment variable can be used to bypass spilling to disk. Setting this to true can avoid memory exhaustion issues (see https://github.com/apache/datafusion/issues/10073 for more info). Experimental API Parameters&para; column : str The column to be indexed. Must be a boolean, integer, float, or string column. index_type : str The type of the index. One of "BTREE", "BITMAP", "LABEL_LIST", "NGRAM", "FTS" or "INVERTED". name : str, optional The index name. If not provided, it will be generated from the column name. replace : bool, default True Replace the existing index if it exists. bool, default True This is for the INVERTED index. If True, the index will store the positions of the words in the document, so that you can conduct phrase query. This will significantly increase the index size. It won't impact the performance of non-phrase queries even if it is set to True. base_tokenizer: str, default "simple" This is for the INVERTED index. The base tokenizer to use. The value can be: * "simple": splits tokens on whitespace and punctuation. * "whitespace": splits tokens on whitespace. * "raw": no tokenization. language: str, default "English" This is for the INVERTED index. The language for stemming and stop words. This is only used when stem or remove_stop_words is true max_token_length: Optional[int], default 40 This is for the INVERTED index. The maximum token length. Any token longer than this will be removed. lower_case: bool, default True This is for the INVERTED index. If True, the index will convert all text to lowercase. stem: bool, default False This is for the INVERTED index. If True, the index will stem the tokens. remove_stop_words: bool, default False This is for the INVERTED index. If True, the index will remove stop words. ascii_folding: bool, default False This is for the INVERTED index. If True, the index will convert non-ascii characters to ascii characters if possible. This would remove accents like "é" -&gt; "e". Examples&para; .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) dataset.create_index( &quot;category&quot;, &quot;BTREE&quot;, ) Scalar indices can only speed up scans for basic filters using equality, comparison, range (e.g. my_col BETWEEN 0 AND 100), and set membership (e.g. my_col IN (0, 1, 2)) Scalar indices can be used if the filter contains multiple indexed columns and the filter criteria are AND'd or OR'd together (e.g. my_col &lt; 0 AND other_col&gt; 100) Scalar indices may be used if the filter contains non-indexed columns but, depending on the structure of the filter, they may not be usable. For example, if the column not_indexed does not have a scalar index then the filter my_col = 0 OR not_indexed = 1 will not be able to use any scalar index on my_col. To determine if a scan is making use of a scalar index you can use explain_plan to look at the query plan that lance has created. Queries that use scalar indices will either have a ScalarIndexQuery relation or a MaterializeIndex operator. delete(predicate) &para; Delete rows from the dataset. This marks rows as deleted, but does not physically remove them from the files. This keeps the existing indexes still valid. Parameters&para; predicate : str or pa.compute.Expression The predicate to use to select rows to delete. May either be a SQL string or a pyarrow Expression. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.delete("a = 1 or b in ('a', 'b')") dataset.to_table() pyarrow.Table a: int64 b: string a: [[3]] b: [["c"]] drop_columns(columns) &para; Drop one or more columns from the dataset This is a metadata-only operation and does not remove the data from the underlying storage. In order to remove the data, you must subsequently call compact_files to rewrite the data without the removed columns and then call cleanup_old_versions to remove the old files. Parameters&para; columns : list of str The names of the columns to drop. These can be nested column references (e.g. "a.b.c") or top-level column names (e.g. "a"). Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.drop_columns(["a"]) dataset.to_table().to_pandas() b 0 a 1 b 2 c drop_index(name) &para; Drops an index from the dataset Note: Indices are dropped by "index name". This is not the same as the field name. If you did not specify a name when you created the index then a name was generated for you. You can use the list_indices method to get the names of the indices. get_fragment(fragment_id) &para; Get the fragment with fragment id. get_fragments(filter=None) &para; Get all fragments from the dataset. Note: filter is not supported yet. head(num_rows, **kwargs) &para; Load the first N rows of the dataset. Parameters&para; num_rows : int The number of rows to load. **kwargs : dict, optional See scanner() method for full parameter description. Returns&para; table : Table insert(data, *, mode=&#39;append&#39;, **kwargs) &para; Insert data into the dataset. Parameters&para; data_obj: Reader-like The data to be written. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader - Huggingface dataset mode: str, default 'append' The mode to use when writing the data. Options are: create - create a new dataset (raises if uri already exists). overwrite - create a new snapshot version append - create a new version that is the concat of the input the latest version (raises if uri does not exist) **kwargs : dict, optional Additional keyword arguments to pass to :func:write_dataset. join(right_dataset, keys, right_keys=None, join_type=&#39;left outer&#39;, left_suffix=None, right_suffix=None, coalesce_keys=True, use_threads=True) &para; Not implemented (just override pyarrow dataset to prevent segfault) merge(data_obj, left_on, right_on=None, schema=None) &para; Merge another dataset into this one. Performs a left join, where the dataset is the left side and data_obj is the right side. Rows existing in the dataset but not on the left will be filled with null values, unless Lance doesn't support null values for some types, in which case an error will be raised. Parameters&para; data_obj: Reader-like The data to be merged. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, Iterator[RecordBatch], or RecordBatchReader left_on: str The name of the column in the dataset to join on. right_on: str or None The name of the column in data_obj to join on. If None, defaults to left_on. Examples&para; import lance import pyarrow as pa df = pa.table({'x': [1, 2, 3], 'y': ['a', 'b', 'c']}) dataset = lance.write_dataset(df, "dataset") dataset.to_table().to_pandas() x y 0 1 a 1 2 b 2 3 c new_df = pa.table({'x': [1, 2, 3], 'z': ['d', 'e', 'f']}) dataset.merge(new_df, 'x') dataset.to_table().to_pandas() x y z 0 1 a d 1 2 b e 2 3 c f See Also&para; LanceDataset.add_columns : Add new columns by computing batch-by-batch. merge_insert(on) &para; Returns a builder that can be used to create a "merge insert" operation This operation can add rows, update rows, and remove rows in a single transaction. It is a very generic tool that can be used to create behaviors like "insert if not exists", "update or insert (i.e. upsert)", or even replace a portion of existing data with new data (e.g. replace all data where month="january") The merge insert operation works by combining new data from a source table with existing data in a target table by using a join. There are three categories of records. "Matched" records are records that exist in both the source table and the target table. "Not matched" records exist only in the source table (e.g. these are new data). "Not matched by source" records exist only in the target table (this is old data). The builder returned by this method can be used to customize what should happen for each category of data. Please note that the data will be reordered as part of this operation. This is because updated rows will be deleted from the dataset and then reinserted at the end with the new values. The order of the newly inserted rows may fluctuate randomly because a hash-join operation is used internally. Parameters&para; Union[str, Iterable[str]] A column (or columns) to join on. This is how records from the source table and target table are matched. Typically this is some kind of key or id column. Examples&para; Use when_matched_update_all() and when_not_matched_insert_all() to perform an "upsert" operation. This will update rows that already exist in the dataset and insert rows that do not exist. import lance import pyarrow as pa table = pa.table({"a": [2, 1, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform a "upsert" operation&para; dataset.merge_insert("a") \ ... .when_matched_update_all() \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b 0 1 b 1 2 x 2 3 y 3 4 z Use when_not_matched_insert_all() to perform an "insert if not exists" operation. This will only insert rows that do not already exist in the dataset. import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example2") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform an "insert if not exists" operation&para; dataset.merge_insert("a") \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 0, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 z You are not required to provide all the columns. If you only want to update a subset of columns, you can omit columns you don't want to update. Omitted columns will keep their existing values if they are updated, or will be null if they are inserted. import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"], \ ... "c": ["x", "y", "z"]}) dataset = lance.write_dataset(table, "example3") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform an "upsert" operation, only updating column "a"&para; dataset.merge_insert("a") \ ... .when_matched_update_all() \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b c 0 1 a x 1 2 x y 2 3 y z 3 4 z None migrate_manifest_paths_v2() &para; Migrate the manifest paths to the new format. This will update the manifest to use the new v2 format for paths. This function is idempotent, and can be run multiple times without changing the state of the object store. DANGER: this should not be run while other concurrent operations are happening. And it should also run until completion before resuming other operations. prewarm_index(name) &para; Prewarm an index This will load the entire index into memory. This can help avoid cold start issues with index queries. If the index does not fit in the index cache, then this will result in wasted I/O. Parameters&para; name: str The name of the index to prewarm. replace_field_metadata(field_name, new_metadata) &para; Replace the metadata of a field in the schema Parameters&para; field_name: str The name of the field to replace the metadata for new_metadata: dict The new metadata to set replace_schema(schema) &para; Not implemented (just override pyarrow dataset to prevent segfault) See method:replace_schema_metadata or method:replace_field_metadata replace_schema_metadata(new_metadata) &para; Replace the schema metadata of the dataset Parameters&para; new_metadata: dict The new metadata to set restore() &para; Restore the currently checked out version as the latest version of the dataset. This creates a new commit. sample(num_rows, columns=None, randomize_order=True, **kwargs) &para; Select a random sample of data Parameters&para; num_rows: int number of rows to retrieve columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. **kwargs : dict, optional see scanner() method for full parameter description. Returns&para; table : Table scanner(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, fragments=None, full_text_query=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, fast_search=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, include_deleted_rows=None, scan_stats_callback=None, strict_batch_size=None) &para; Return a Scanner that can support various pushdowns. Parameters&para; columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. filter: pa.compute.Expression or str Expression or str that is a valid SQL where clause. See Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;_ for valid SQL expressions. limit: int, default None Fetch up to this many rows. All rows if None or unspecified. offset: int, default None Fetch starting with this row. 0 if None or unspecified. nearest: dict, default None Get the rows corresponding to the K most similar vectors. Example: .. code-block:: python { &quot;column&quot;: &lt;embedding col name&gt;, &quot;q&quot;: &lt;query vector as pa.Float32Array&gt;, &quot;k&quot;: 10, &quot;nprobes&quot;: 1, &quot;refine_factor&quot;: 1 } int, default None The target size of batches returned. In some cases batches can be up to twice this size (but never larger than this). In some cases batches can be smaller than this size. io_buffer_size: int, default None The size of the IO buffer. See ScannerBuilder.io_buffer_size for more information. batch_readahead: int, optional The number of batches to read ahead. fragment_readahead: int, optional The number of fragments to read ahead. scan_in_order: bool, default True Whether to read the fragments and batches in order. If false, throughput may be higher, but batches will be returned out of order and memory use might increase. fragments: iterable of LanceFragment, default None If specified, only scan these fragments. If scan_in_order is True, then the fragments will be scanned in the order given. prefilter: bool, default False If True then the filter will be applied before the vector query is run. This will generate more correct results but it may be a more costly query. It's generally good when the filter is highly selective. If False then the filter will be applied after the vector query is run. This will perform well but the results may have fewer than the requested number of rows (or be empty) if the rows closest to the query do not match the filter. It&#39;s generally good when the filter is not very selective. use_scalar_index: bool, default True Lance will automatically use scalar indices to optimize a query. In some corner cases this can make query performance worse and this parameter can be used to disable scalar indices in these cases. late_materialization: bool or List[str], default None Allows custom control over late materialization. Late materialization fetches non-query columns using a take operation after the filter. This is useful when there are few results or columns are very large. Early materialization can be better when there are many results or the columns are very narrow. If True, then all columns are late materialized. If False, then all columns are early materialized. If a list of strings, then only the columns in the list are late materialized. The default uses a heuristic that assumes filters will select about 0.1% of the rows. If your filter is more selective (e.g. find by id) you may want to set this to True. If your filter is not very selective (e.g. matches 20% of the rows) you may want to set this to False. full_text_query: str or dict, optional query string to search for, the results will be ranked by BM25. e.g. "hello world", would match documents containing "hello" or "world". or a dictionary with the following keys: - columns: list[str] The columns to search, currently only supports a single column in the columns list. - query: str The query string to search for. fast_search: bool, default False If True, then the search will only be performed on the indexed data, which yields faster search time. scan_stats_callback: Callable[[ScanStatistics], None], default None A callback function that will be called with the scan statistics after the scan is complete. Errors raised by the callback will be logged but not re-raised. include_deleted_rows: bool, default False If True, then rows that have been deleted, but are still present in the fragment, will be returned. These rows will have the _rowid column set to null. All other columns will reflect the value stored on disk and may not be null. Note: if this is a search operation, or a take operation (including scalar indexed scans) then deleted rows cannot be returned. .. note:: For now, if BOTH filter and nearest is specified, then: 1. nearest is executed first. 2. The results are filtered afterwards. For debugging ANN results, you can choose to not use the index even if present by specifying use_index=False. For example, the following will always return exact KNN results: .. code-block:: python dataset.to_table(nearest={ &quot;column&quot;: &quot;vector&quot;, &quot;k&quot;: 10, &quot;q&quot;: &lt;query vector&gt;, &quot;use_index&quot;: False } session() &para; Return the dataset session, which holds the dataset's state. take(indices, columns=None) &para; Select rows of data by index. Parameters&para; indices : Array or array-like indices of rows to select in the dataset. columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. Returns&para; table : pyarrow.Table take_blobs(blob_column, ids=None, addresses=None, indices=None) &para; Select blobs by row IDs. Instead of loading large binary blob data into memory before processing it, this API allows you to open binary blob data as a regular Python file-like object. For more details, see class:lance.BlobFile. Exactly one of ids, addresses, or indices must be specified. Parameters blob_column : str The name of the blob column to select. ids : Integer Array or array-like row IDs to select in the dataset. addresses: Integer Array or array-like The (unstable) row addresses to select in the dataset. indices : Integer Array or array-like The offset / indices of the row in the dataset. Returns&para; blob_files : List[BlobFile] to_batches(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, full_text_query=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, strict_batch_size=None, **kwargs) &para; Read the dataset as materialized record batches. Parameters&para; **kwargs : dict, optional Arguments for meth:~LanceDataset.scanner. Returns&para; record_batches : Iterator of class:~pyarrow.RecordBatch to_table(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, fast_search=None, full_text_query=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, include_deleted_rows=None) &para; Read the data into memory as a class:pyarrow.Table Parameters&para; columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. filter : pa.compute.Expression or str Expression or str that is a valid SQL where clause. See Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;_ for valid SQL expressions. limit: int, default None Fetch up to this many rows. All rows if None or unspecified. offset: int, default None Fetch starting with this row. 0 if None or unspecified. nearest: dict, default None Get the rows corresponding to the K most similar vectors. Example: .. code-block:: python { &quot;column&quot;: &lt;embedding col name&gt;, &quot;q&quot;: &lt;query vector as pa.Float32Array&gt;, &quot;k&quot;: 10, &quot;metric&quot;: &quot;cosine&quot;, &quot;nprobes&quot;: 1, &quot;refine_factor&quot;: 1 } int, optional The number of rows to read at a time. io_buffer_size: int, default None The size of the IO buffer. See ScannerBuilder.io_buffer_size for more information. batch_readahead: int, optional The number of batches to read ahead. fragment_readahead: int, optional The number of fragments to read ahead. scan_in_order: bool, optional, default True Whether to read the fragments and batches in order. If false, throughput may be higher, but batches will be returned out of order and memory use might increase. prefilter: bool, optional, default False Run filter before the vector search. late_materialization: bool or List[str], default None Allows custom control over late materialization. See ScannerBuilder.late_materialization for more information. use_scalar_index: bool, default True Allows custom control over scalar index usage. See ScannerBuilder.use_scalar_index for more information. with_row_id: bool, optional, default False Return row ID. with_row_address: bool, optional, default False Return row address use_stats: bool, optional, default True Use stats pushdown during filters. fast_search: bool, optional, default False full_text_query: str or dict, optional query string to search for, the results will be ranked by BM25. e.g. "hello world", would match documents contains "hello" or "world". or a dictionary with the following keys: - columns: list[str] The columns to search, currently only supports a single column in the columns list. - query: str The query string to search for. include_deleted_rows: bool, optional, default False If True, then rows that have been deleted, but are still present in the fragment, will be returned. These rows will have the _rowid column set to null. All other columns will reflect the value stored on disk and may not be null. Note: if this is a search operation, or a take operation (including scalar indexed scans) then deleted rows cannot be returned. Notes&para; If BOTH filter and nearest is specified, then: nearest is executed first. The results are filtered afterward, unless pre-filter sets to True. update(updates, where=None) &para; Update column values for rows matching where. Parameters&para; updates : dict of str to str A mapping of column names to a SQL expression. where : str, optional A SQL predicate indicating which rows should be updated. Returns&para; updates : dict A dictionary containing the number of rows updated. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") update_stats = dataset.update(dict(a = 'a + 2'), where="b != 'a'") update_stats["num_updated_rows"] = 2 dataset.to_table().to_pandas() a b 0 1 a 1 4 b 2 5 c validate() &para; Validate the dataset. This checks the integrity of the dataset and will raise an exception if the dataset is corrupted. versions() &para; Return all versions in this dataset. LanceFragment &para; Bases: Fragment metadata property &para; Return the metadata of this fragment. Returns&para; FragmentMetadata num_deletions property &para; Return the number of deleted rows in this fragment. physical_rows property &para; Return the number of rows originally in this fragment. To get the number of rows after deletions, use :meth:count_rows instead. schema property &para; Return the schema of this fragment. create(dataset_uri, data, fragment_id=None, schema=None, max_rows_per_group=1024, progress=None, mode=&#39;append&#39;, *, data_storage_version=None, use_legacy_format=None, storage_options=None) staticmethod &para; Create a :class:FragmentMetadata from the given data. This can be used if the dataset is not yet created. .. warning:: Internal API. This method is not intended to be used by end users. Parameters&para; dataset_uri: str The URI of the dataset. fragment_id: int The ID of the fragment. data: pa.Table or pa.RecordBatchReader The data to be written to the fragment. schema: pa.Schema, optional The schema of the data. If not specified, the schema will be inferred from the data. max_rows_per_group: int, default 1024 The maximum number of rows per group in the data file. progress: FragmentWriteProgress, optional Experimental API. Progress tracking for writing the fragment. Pass a custom class that defines hooks to be called when each fragment is starting to write and finishing writing. mode: str, default "append" The write mode. If "append" is specified, the data will be checked against the existing dataset's schema. Otherwise, pass "create" or "overwrite" to assign new field ids to the schema. data_storage_version: optional, str, default None The version of the data storage format to use. Newer versions are more efficient but require newer versions of lance to read. The default (None) will use the latest stable version. See the user guide for more details. use_legacy_format: bool, default None Deprecated parameter. Use data_storage_version instead. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. See Also&para; lance.dataset.LanceOperation.Overwrite : The operation used to create a new dataset or overwrite one using fragments created with this API. See the doc page for an example of using this API. lance.dataset.LanceOperation.Append : The operation used to append fragments created with this API to an existing dataset. See the doc page for an example of using this API. Returns&para; FragmentMetadata create_from_file(filename, dataset, fragment_id) staticmethod &para; Create a fragment from the given datafile uri. This can be used if the datafile is loss from dataset. .. warning:: Internal API. This method is not intended to be used by end users. Parameters&para; filename: str The filename of the datafile. dataset: LanceDataset The dataset that the fragment belongs to. fragment_id: int The ID of the fragment. data_files() &para; Return the data files of this fragment. delete(predicate) &para; Delete rows from this Fragment. This will add or update the deletion file of this fragment. It does not modify or delete the data files of this fragment. If no rows are left after the deletion, this method will return None. .. warning:: Internal API. This method is not intended to be used by end users. Parameters&para; predicate: str A SQL predicate that specifies the rows to delete. Returns&para; FragmentMetadata or None A new fragment containing the new deletion file, or None if no rows left. Examples&para; import lance import pyarrow as pa tab = pa.table({"a": [1, 2, 3], "b": [4, 5, 6]}) dataset = lance.write_dataset(tab, "dataset") frag = dataset.get_fragment(0) frag.delete("a &gt; 1") FragmentMetadata(id=0, files=[DataFile(path='...', fields=[0, 1], ...), ...) frag.delete("a &gt; 0") is None True See Also&para; lance.dataset.LanceOperation.Delete : The operation used to commit these changes to a dataset. See the doc page for an example of using this API. deletion_file() &para; Return the deletion file, if any merge(data_obj, left_on, right_on=None, schema=None) &para; Merge another dataset into this fragment. Performs a left join, where the fragment is the left side and data_obj is the right side. Rows existing in the dataset but not on the left will be filled with null values, unless Lance doesn't support null values for some types, in which case an error will be raised. Parameters&para; data_obj: Reader-like The data to be merged. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, Iterator[RecordBatch], or RecordBatchReader left_on: str The name of the column in the dataset to join on. right_on: str or None The name of the column in data_obj to join on. If None, defaults to left_on. Examples&para; import lance import pyarrow as pa df = pa.table({'x': [1, 2, 3], 'y': ['a', 'b', 'c']}) dataset = lance.write_dataset(df, "dataset") dataset.to_table().to_pandas() x y 0 1 a 1 2 b 2 3 c fragments = dataset.get_fragments() new_df = pa.table({'x': [1, 2, 3], 'z': ['d', 'e', 'f']}) merged = [] schema = None for f in fragments: ... f, schema = f.merge(new_df, 'x') ... merged.append(f) merge = lance.LanceOperation.Merge(merged, schema) dataset = lance.LanceDataset.commit("dataset", merge, read_version=1) dataset.to_table().to_pandas() x y z 0 1 a d 1 2 b e 2 3 c f See Also&para; LanceDataset.merge_columns : Add columns to this Fragment. Returns&para; Tuple[FragmentMetadata, LanceSchema] A new fragment with the merged column(s) and the final schema. merge_columns(value_func, columns=None, batch_size=None, reader_schema=None) &para; Add columns to this Fragment. .. warning:: Internal API. This method is not intended to be used by end users. The parameters and their interpretation are the same as in the :meth:lance.dataset.LanceDataset.add_columns operation. The only difference is that, instead of modifying the dataset, a new fragment is created. The new schema of the fragment is returned as well. These can be used in a later operation to commit the changes to the dataset. See Also&para; lance.dataset.LanceOperation.Merge : The operation used to commit these changes to the dataset. See the doc page for an example of using this API. Returns&para; Tuple[FragmentMetadata, LanceSchema] A new fragment with the added column(s) and the final schema. scanner(*, columns=None, batch_size=None, filter=None, limit=None, offset=None, with_row_id=False, with_row_address=False, batch_readahead=16) &para; See Dataset::scanner for details LanceOperation &para; Append dataclass &para; Bases: BaseOperation Append new rows to the dataset. Attributes&para; fragments: list[FragmentMetadata] The fragments that contain the new rows. Warning&para; This is an advanced API for distributed operations. To append to a dataset on a single machine, use :func:lance.write_dataset. Examples&para; To append new rows to a dataset, first use :meth:lance.fragment.LanceFragment.create to create fragments. Then collect the fragment metadata into a list and pass it to this class. Finally, pass the operation to the :meth:LanceDataset.commit method to create the new dataset. import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) dataset = lance.write_dataset(tab1, "example") tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment = lance.fragment.LanceFragment.create("example", tab2) operation = lance.LanceOperation.Append([fragment]) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d BaseOperation &para; Bases: ABC Base class for operations that can be applied to a dataset. See available operations under :class:LanceOperation. CreateIndex dataclass &para; Bases: BaseOperation Operation that creates an index on the dataset. DataReplacement dataclass &para; Bases: BaseOperation Operation that replaces existing datafiles in the dataset. DataReplacementGroup dataclass &para; Group of data replacements Delete dataclass &para; Bases: BaseOperation Remove fragments or rows from the dataset. Attributes&para; updated_fragments: list[FragmentMetadata] The fragments that have been updated with new deletion vectors. deleted_fragment_ids: list[int] The ids of the fragments that have been deleted entirely. These are the fragments where :meth:LanceFragment.delete() returned None. predicate: str The original SQL predicate used to select the rows to delete. Warning&para; This is an advanced API for distributed operations. To delete rows from dataset on a single machine, use :meth:lance.LanceDataset.delete. Examples&para; To delete rows from a dataset, call :meth:lance.fragment.LanceFragment.delete on each of the fragments. If that returns a new fragment, add that to the updated_fragments list. If it returns None, that means the whole fragment was deleted, so add the fragment id to the deleted_fragment_ids. Finally, pass the operation to the :meth:LanceDataset.commit method to complete the deletion operation. import lance import pyarrow as pa table = pa.table({"a": [1, 2], "b": ["a", "b"]}) dataset = lance.write_dataset(table, "example") table = pa.table({"a": [3, 4], "b": ["c", "d"]}) dataset = lance.write_dataset(table, "example", mode="append") dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d predicate = "a &gt;= 2" updated_fragments = [] deleted_fragment_ids = [] for fragment in dataset.get_fragments(): ... new_fragment = fragment.delete(predicate) ... if new_fragment is not None: ... updated_fragments.append(new_fragment) ... else: ... deleted_fragment_ids.append(fragment.fragment_id) operation = lance.LanceOperation.Delete(updated_fragments, ... deleted_fragment_ids, ... predicate) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b 0 1 a Merge dataclass &para; Bases: BaseOperation Operation that adds columns. Unlike Overwrite, this should not change the structure of the fragments, allowing existing indices to be kept. Attributes&para; fragments: iterable of FragmentMetadata The fragments that make up the new dataset. schema: LanceSchema or pyarrow.Schema The schema of the new dataset. Passing a LanceSchema is preferred, and passing a pyarrow.Schema is deprecated. Warning&para; This is an advanced API for distributed operations. To overwrite or create new dataset on a single machine, use :func:lance.write_dataset. Examples&para; To add new columns to a dataset, first define a method that will create the new columns based on the existing columns. Then use :meth:lance.fragment.LanceFragment.add_columns import lance import pyarrow as pa import pyarrow.compute as pc table = pa.table({"a": [1, 2, 3, 4], "b": ["a", "b", "c", "d"]}) dataset = lance.write_dataset(table, "example") dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d def double_a(batch: pa.RecordBatch) -&gt; pa.RecordBatch: ... doubled = pc.multiply(batch["a"], 2) ... return pa.record_batch([doubled], ["a_doubled"]) fragments = [] for fragment in dataset.get_fragments(): ... new_fragment, new_schema = fragment.merge_columns(double_a, ... columns=['a']) ... fragments.append(new_fragment) operation = lance.LanceOperation.Merge(fragments, new_schema) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b a_doubled 0 1 a 2 1 2 b 4 2 3 c 6 3 4 d 8 Overwrite dataclass &para; Bases: BaseOperation Overwrite or create a new dataset. Attributes&para; new_schema: pyarrow.Schema The schema of the new dataset. fragments: list[FragmentMetadata] The fragments that make up the new dataset. Warning&para; This is an advanced API for distributed operations. To overwrite or create new dataset on a single machine, use :func:lance.write_dataset. Examples&para; To create or overwrite a dataset, first use :meth:lance.fragment.LanceFragment.create to create fragments. Then collect the fragment metadata into a list and pass it along with the schema to this class. Finally, pass the operation to the :meth:LanceDataset.commit method to create the new dataset. import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment1 = lance.fragment.LanceFragment.create("example", tab1) fragment2 = lance.fragment.LanceFragment.create("example", tab2) fragments = [fragment1, fragment2] operation = lance.LanceOperation.Overwrite(tab1.schema, fragments) dataset = lance.LanceDataset.commit("example", operation) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d Project dataclass &para; Bases: BaseOperation Operation that project columns. Use this operator for drop column or rename/swap column. Attributes&para; schema: LanceSchema The lance schema of the new dataset. Examples&para; Use the projece operator to swap column: import lance import pyarrow as pa import pyarrow.compute as pc from lance.schema import LanceSchema table = pa.table({"a": [1, 2], "b": ["a", "b"], "b1": ["c", "d"]}) dataset = lance.write_dataset(table, "example") dataset.to_table().to_pandas() a b b1 0 1 a c 1 2 b d rename column b into b0 and rename b1 into b&para; table = pa.table({"a": [3, 4], "b0": ["a", "b"], "b": ["c", "d"]}) lance_schema = LanceSchema.from_pyarrow(table.schema) operation = lance.LanceOperation.Project(lance_schema) dataset = lance.LanceDataset.commit("example", operation, read_version=1) dataset.to_table().to_pandas() a b0 b 0 1 a c 1 2 b d Restore dataclass &para; Bases: BaseOperation Operation that restores a previous version of the dataset. Rewrite dataclass &para; Bases: BaseOperation Operation that rewrites one or more files and indices into one or more files and indices. Attributes&para; groups: list[RewriteGroup] Groups of files that have been rewritten. rewritten_indices: list[RewrittenIndex] Indices that have been rewritten. Warning&para; This is an advanced API not intended for general use. RewriteGroup dataclass &para; Collection of rewritten files RewrittenIndex dataclass &para; An index that has been rewritten Update dataclass &para; Bases: BaseOperation Operation that updates rows in the dataset. Attributes&para; removed_fragment_ids: list[int] The ids of the fragments that have been removed entirely. updated_fragments: list[FragmentMetadata] The fragments that have been updated with new deletion vectors. new_fragments: list[FragmentMetadata] The fragments that contain the new rows. LanceScanner &para; Bases: Scanner dataset_schema property &para; The schema with which batches will be read from fragments. analyze_plan() &para; Execute the plan for this scanner and display with runtime metrics. Parameters&para; verbose : bool, default False Use a verbose output format. Returns&para; plan : str count_rows() &para; Count rows matching the scanner filter. Returns&para; count : int explain_plan(verbose=False) &para; Return the execution plan for this scanner. Parameters&para; verbose : bool, default False Use a verbose output format. Returns&para; plan : str from_batches(*args, **kwargs) staticmethod &para; Not implemented from_dataset(*args, **kwargs) staticmethod &para; Not implemented from_fragment(*args, **kwargs) staticmethod &para; Not implemented head(num_rows) &para; Load the first N rows of the dataset. Parameters&para; num_rows : int The number of rows to load. Returns&para; Table scan_batches() &para; Consume a Scanner in record batches with corresponding fragments. Returns&para; record_batches : iterator of TaggedRecordBatch take(indices) &para; Not implemented to_table() &para; Read the data into memory and return a pyarrow Table. MergeInsertBuilder &para; Bases: _MergeInsertBuilder conflict_retries(max_retries) &para; Set number of times to retry the operation if there is contention. If this is set &gt; 0, then the operation will keep a copy of the input data either in memory or on disk (depending on the size of the data) and will retry the operation if there is contention. Default is 10. execute(data_obj, *, schema=None) &para; Executes the merge insert operation This function updates the original dataset and returns a dictionary with information about merge statistics - i.e. the number of inserted, updated, and deleted rows. Parameters&para; ReaderLike The new data to use as the source table for the operation. This parameter can be any source of data (e.g. table / dataset) that :func:~lance.write_dataset accepts. schema: Optional[pa.Schema] The schema of the data. This only needs to be supplied whenever the data source is some kind of generator. execute_uncommitted(data_obj, *, schema=None) &para; Executes the merge insert operation without committing This function updates the original dataset and returns a dictionary with information about merge statistics - i.e. the number of inserted, updated, and deleted rows. Parameters&para; ReaderLike The new data to use as the source table for the operation. This parameter can be any source of data (e.g. table / dataset) that :func:~lance.write_dataset accepts. schema: Optional[pa.Schema] The schema of the data. This only needs to be supplied whenever the data source is some kind of generator. retry_timeout(timeout) &para; Set the timeout used to limit retries. This is the maximum time to spend on the operation before giving up. At least one attempt will be made, regardless of how long it takes to complete. Subsequent attempts will be cancelled once this timeout is reached. If the timeout has been reached during the first attempt, the operation will be cancelled immediately before making a second attempt. The default is 30 seconds. when_matched_update_all(condition=None) &para; Configure the operation to update matched rows After this method is called, when the merge insert operation executes, any rows that match both the source table and the target table will be updated. The rows from the target table will be removed and the rows from the source table will be added. An optional condition may be specified. This should be an SQL filter and, if present, then only matched rows that also satisfy this filter will be updated. The SQL filter should use the prefix target. to refer to columns in the target table and the prefix source. to refer to columns in the source table. For example, source.last_update &lt; target.last_update. If a condition is specified and rows do not satisfy the condition then these rows will not be updated. Failure to satisfy the filter does not cause a "matched" row to become a "not matched" row. when_not_matched_by_source_delete(expr=None) &para; Configure the operation to delete source rows that do not match After this method is called, when the merge insert operation executes, any rows that exist only in the target table will be deleted. An optional filter can be specified to limit the scope of the delete operation. If given (as an SQL filter) then only rows which match the filter will be deleted. when_not_matched_insert_all() &para; Configure the operation to insert not matched rows After this method is called, when the merge insert operation executes, any rows that exist only in the source table will be inserted into the target table. batch_udf(output_schema=None, checkpoint_file=None) &para; Create a user defined function (UDF) that adds columns to a dataset. This function is used to add columns to a dataset. It takes a function that takes a single argument, a RecordBatch, and returns a RecordBatch. The function is called once for each batch in the dataset. The function should not modify the input batch, but instead create a new batch with the new columns added. Parameters&para; output_schema : Schema, optional The schema of the output RecordBatch. This is used to validate the output of the function. If not provided, the schema of the first output RecordBatch will be used. checkpoint_file : str or Path, optional If specified, this file will be used as a cache for unsaved results of this UDF. If the process fails, and you call add_columns again with this same file, it will resume from the last saved state. This is useful for long running processes that may fail and need to be resumed. This file may get very large. It will hold up to an entire data files' worth of results on disk, which can be multiple gigabytes of data. Returns&para; AddColumnsUDF json_to_schema(schema_json) &para; Converts a JSON string to a PyArrow schema. Parameters&para; schema_json: Dict[str, Any] The JSON payload to convert to a PyArrow Schema. schema_to_json(schema) &para; Converts a pyarrow schema to a JSON string. Parameters&para; write_dataset(data_obj, uri, schema=None, mode=&#39;create&#39;, *, max_rows_per_file=1024 * 1024, max_rows_per_group=1024, max_bytes_per_file=90 * 1024 * 1024 * 1024, commit_lock=None, progress=None, storage_options=None, data_storage_version=None, use_legacy_format=None, enable_v2_manifest_paths=False, enable_move_stable_row_ids=False) &para; Write a given data_obj to the given uri Parameters&para; data_obj: Reader-like The data to be written. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader - Huggingface dataset uri: str, Path, or LanceDataset Where to write the dataset to (directory). If a LanceDataset is passed, the session will be reused. schema: Schema, optional If specified and the input is a pandas DataFrame, use this schema instead of the default pandas to arrow table conversion. mode: str create - create a new dataset (raises if uri already exists). overwrite - create a new snapshot version append - create a new version that is the concat of the input the latest version (raises if uri does not exist) max_rows_per_file: int, default 1024 * 1024 The max number of rows to write before starting a new file max_rows_per_group: int, default 1024 The max number of rows before starting a new group (in the same file) max_bytes_per_file: int, default 90 * 1024 * 1024 * 1024 The max number of bytes to write before starting a new file. This is a soft limit. This limit is checked after each group is written, which means larger groups may cause this to be overshot meaningfully. This defaults to 90 GB, since we have a hard limit of 100 GB per file on object stores. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. progress: FragmentWriteProgress, optional Experimental API. Progress tracking for writing the fragment. Pass a custom class that defines hooks to be called when each fragment is starting to write and finishing writing. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. data_storage_version: optional, str, default None The version of the data storage format to use. Newer versions are more efficient but require newer versions of lance to read. The default (None) will use the latest stable version. See the user guide for more details. use_legacy_format : optional, bool, default None Deprecated method for setting the data storage version. Use the data_storage_version parameter instead. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:LanceDataset.migrate_manifest_paths_v2 method. Default is False. enable_move_stable_row_ids : bool, optional Experimental parameter: if set to true, the writer will use move-stable row ids. These row ids are stable after compaction operations, but not after updates. This makes compaction more efficient, since with stable row ids no secondary indices need to be updated to point to new row ids. into lance.dataset &para; DataStatistics dataclass &para; Statistics about the data in the dataset DatasetOptimizer &para; compact_files(*, target_rows_per_fragment=1024 * 1024, max_rows_per_group=1024, max_bytes_per_file=None, materialize_deletions=True, materialize_deletions_threshold=0.1, num_threads=None, batch_size=None) &para; Compacts small files in the dataset, reducing total number of files. This does a few things Removes deleted rows from fragments Removes dropped columns from fragments Merges small fragments into larger ones This method preserves the insertion order of the dataset. This may mean it leaves small fragments in the dataset if they are not adjacent to other fragments that need compaction. For example, if you have fragments with row counts 5 million, 100, and 5 million, the middle fragment will not be compacted because the fragments it is adjacent to do not need compaction. Parameters&para; target_rows_per_fragment: int, default 1024*1024 The target number of rows per fragment. This is the number of rows that will be in each fragment after compaction. max_rows_per_group: int, default 1024 Max number of rows per group. This does not affect which fragments need compaction, but does affect how they are re-written if selected. This setting only affects datasets using the legacy storage format. The newer format does not require row groups. max_bytes_per_file: Optional[int], default None Max number of bytes in a single file. This does not affect which fragments need compaction, but does affect how they are re-written if selected. If this value is too small you may end up with fragments that are smaller than target_rows_per_fragment. The default will use the default from ``write_dataset``. materialize_deletions: bool, default True Whether to compact fragments with soft deleted rows so they are no longer present in the file. materialize_deletions_threshold: float, default 0.1 The fraction of original rows that are soft deleted in a fragment before the fragment is a candidate for compaction. num_threads: int, optional The number of threads to use when performing compaction. If not specified, defaults to the number of cores on the machine. batch_size: int, optional The batch size to use when scanning input fragments. You may want to reduce this if you are running out of memory during compaction. The default will use the same default from ``scanner``. Returns&para; CompactionMetrics Metrics about the compaction process See Also&para; lance.optimize.Compaction optimize_indices(**kwargs) &para; Optimizes index performance. As new data arrives it is not added to existing indexes automatically. When searching we need to perform an indexed search of the old data plus an expensive unindexed search on the new data. As the amount of new unindexed data grows this can have an impact on search latency. This function will add the new data to existing indexes, restoring the performance. This function does not retrain the index, it only assigns the new data to existing partitions. This means an update is much quicker than retraining the entire index but may have less accuracy (especially if the new data exhibits new patterns, concepts, or trends) Parameters&para; num_indices_to_merge: int, default 1 The number of indices to merge. If set to 0, new delta index will be created. index_names: List[str], default None The names of the indices to optimize. If None, all indices will be optimized. retrain: bool, default False Whether to retrain the whole index. If true, the index will be retrained based on the current data, num_indices_to_merge will be ignored, and all indices will be merged into one. This is useful when the data distribution has changed significantly, and we want to retrain the index to improve the search quality. This would be faster than re-create the index from scratch. FieldStatistics dataclass &para; Statistics about a field in the dataset LanceDataset &para; Bases: Dataset A Lance Dataset in Lance format where the data is stored at the given uri. data_storage_version property &para; The version of the data storage format this dataset is using lance_schema property &para; The LanceSchema for this dataset latest_version property &para; Returns the latest version of the dataset. max_field_id property &para; The max_field_id in manifest partition_expression property &para; Not implemented (just override pyarrow dataset to prevent segfault) schema property &para; The pyarrow Schema for this dataset stats property &para; Experimental API tags property &para; Tag management for the dataset. Similar to Git, tags are a way to add metadata to a specific version of the dataset. .. warning:: Tagged versions are exempted from the :py:meth:`cleanup_old_versions()` process. To remove a version that has been tagged, you must first :py:meth:`~Tags.delete` the associated tag. Examples&para; .. code-block:: python ds = lance.open(&quot;dataset.lance&quot;) ds.tags.create(&quot;v2-prod-20250203&quot;, 10) tags = ds.tags.list() uri property &para; The location of the data version property &para; Returns the currently checked out version of the dataset add_columns(transforms, read_columns=None, reader_schema=None, batch_size=None) &para; Add new columns with defined values. There are several ways to specify the new columns. First, you can provide SQL expressions for each new column. Second you can provide a UDF that takes a batch of existing data and returns a new batch with the new columns. These new columns will be appended to the dataset. You can also provide a RecordBatchReader which will read the new column values from some external source. This is often useful when the new column values have already been staged to files (often by some distributed process) See the :func:lance.add_columns_udf decorator for more information on writing UDFs. Parameters&para; transforms : dict or AddColumnsUDF or ReaderLike If this is a dictionary, then the keys are the names of the new columns and the values are SQL expression strings. These strings can reference existing columns in the dataset. If this is a AddColumnsUDF, then it is a UDF that takes a batch of existing data and returns a new batch with the new columns. If this is :class:pyarrow.Field or :class:pyarrow.Schema, it adds all NULL columns with the given schema, in a metadata-only operation. read_columns : list of str, optional The names of the columns that the UDF will read. If None, then the UDF will read all columns. This is only used when transforms is a UDF. Otherwise, the read columns are inferred from the SQL expressions. reader_schema: pa.Schema, optional Only valid if transforms is a ReaderLike object. This will be used to determine the schema of the reader. batch_size: int, optional The number of rows to read at a time from the source dataset when applying the transform. This is ignored if the dataset is a v1 dataset. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3]}) dataset = lance.write_dataset(table, "my_dataset") @lance.batch_udf() ... def double_a(batch): ... df = batch.to_pandas() ... return pd.DataFrame({'double_a': 2 * df['a']}) dataset.add_columns(double_a) dataset.to_table().to_pandas() a double_a 0 1 2 1 2 4 2 3 6 dataset.add_columns({"triple_a": "a * 3"}) dataset.to_table().to_pandas() a double_a triple_a 0 1 2 3 1 2 4 6 2 3 6 9 See Also&para; LanceDataset.merge : Merge a pre-computed set of columns into the dataset. alter_columns(*alterations) &para; Alter column name, data type, and nullability. Columns that are renamed can keep any indices that are on them. If a column has an IVF_PQ index, it can be kept if the column is casted to another type. However, other index types don't support casting at this time. Column types can be upcasted (such as int32 to int64) or downcasted (such as int64 to int32). However, downcasting will fail if there are any values that cannot be represented in the new type. In general, columns can be casted to same general type: integers to integers, floats to floats, and strings to strings. However, strings, binary, and list columns can be casted between their size variants. For example, string to large string, binary to large binary, and list to large list. Columns that are renamed can keep any indices that are on them. However, if the column is casted to a different type, its indices will be dropped. Parameters&para; alterations : Iterable[Dict[str, Any]] A sequence of dictionaries, each with the following keys: - &quot;path&quot;: str The column path to alter. For a top-level column, this is the name. For a nested column, this is the dot-separated path, e.g. &quot;a.b.c&quot;. - &quot;name&quot;: str, optional The new name of the column. If not specified, the column name is not changed. - &quot;nullable&quot;: bool, optional Whether the column should be nullable. If not specified, the column nullability is not changed. Only non-nullable columns can be changed to nullable. Currently, you cannot change a nullable column to non-nullable. - &quot;data_type&quot;: pyarrow.DataType, optional The new data type to cast the column to. If not specified, the column data type is not changed. Examples&para; import lance import pyarrow as pa schema = pa.schema([pa.field('a', pa.int64()), ... pa.field('b', pa.string(), nullable=False)]) table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.alter_columns({"path": "a", "name": "x"}, ... {"path": "b", "nullable": True}) dataset.to_table().to_pandas() x b 0 1 a 1 2 b 2 3 c dataset.alter_columns({"path": "x", "data_type": pa.int32()}) dataset.schema x: int32 b: string checkout_version(version) &para; Load the given version of the dataset. Unlike the :func:dataset constructor, this will re-use the current cache. This is a no-op if the dataset is already at the given version. Parameters&para; version: int | str, The version to check out. A version number (int) or a tag (str) can be provided. Returns&para; LanceDataset cleanup_old_versions(older_than=None, *, delete_unverified=False, error_if_tagged_old_versions=True) &para; Cleans up old versions of the dataset. Some dataset changes, such as overwriting, leave behind data that is not referenced by the latest dataset version. The old data is left in place to allow the dataset to be restored back to an older version. This method will remove older versions and any data files they reference. Once this cleanup task has run you will not be able to checkout or restore these older versions. Parameters&para; timedelta, optional Only versions older than this will be removed. If not specified, this will default to two weeks. bool, default False Files leftover from a failed transaction may appear to be part of an in-progress operation (e.g. appending new data) and these files will not be deleted unless they are at least 7 days old. If delete_unverified is True then these files will be deleted regardless of their age. This should only be set to True if you can guarantee that no other process is currently working on this dataset. Otherwise the dataset could be put into a corrupted state. bool, default True Some versions may have tags associated with them. Tagged versions will not be cleaned up, regardless of how old they are. If this argument is set to True (the default), an exception will be raised if any tagged versions match the parameters. Otherwise, tagged versions will be ignored without any error and only untagged versions will be cleaned up. commit(base_uri, operation, blobs_op=None, read_version=None, commit_lock=None, storage_options=None, enable_v2_manifest_paths=None, detached=False, max_retries=20) staticmethod &para; Create a new version of dataset This method is an advanced method which allows users to describe a change that has been made to the data files. This method is not needed when using Lance to apply changes (e.g. when using class:LanceDataset or func:write_dataset.) It's current purpose is to allow for changes being made in a distributed environment where no single process is doing all of the work. For example, a distributed bulk update or a distributed bulk modify operation. Once all of the changes have been made, this method can be called to make the changes visible by updating the dataset manifest. Warnings&para; This is an advanced API and doesn't provide the same level of validation as the other APIs. For example, it's the responsibility of the caller to ensure that the fragments are valid for the schema. Parameters&para; base_uri: str, Path, or LanceDataset The base uri of the dataset, or the dataset object itself. Using the dataset object can be more efficient because it can re-use the file metadata cache. operation: BaseOperation The operation to apply to the dataset. This describes what changes have been made. See available operations under :class:LanceOperation. read_version: int, optional The version of the dataset that was used as the base for the changes. This is not needed for overwrite or restore operations. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:migrate_manifest_paths_v2 method. Default is False. WARNING: turning this on will make the dataset unreadable for older versions of Lance (prior to 0.17.0). detached : bool, optional If True, then the commit will not be part of the dataset lineage. It will never show up as the latest dataset and the only way to check it out in the future will be to specifically check it out by version. The version will be a random version that is only unique amongst detached commits. The caller should store this somewhere as there will be no other way to obtain it in the future. max_retries : int The maximum number of retries to perform when committing the dataset. Returns&para; LanceDataset A new version of Lance Dataset. Examples&para; Creating a new dataset with the :class:LanceOperation.Overwrite operation: import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment1 = lance.fragment.LanceFragment.create("example", tab1) fragment2 = lance.fragment.LanceFragment.create("example", tab2) fragments = [fragment1, fragment2] operation = lance.LanceOperation.Overwrite(tab1.schema, fragments) dataset = lance.LanceDataset.commit("example", operation) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d commit_batch(dest, transactions, commit_lock=None, storage_options=None, enable_v2_manifest_paths=None, detached=False, max_retries=20) staticmethod &para; Create a new version of dataset with multiple transactions. This method is an advanced method which allows users to describe a change that has been made to the data files. This method is not needed when using Lance to apply changes (e.g. when using class:LanceDataset or func:write_dataset.) Parameters&para; dest: str, Path, or LanceDataset The base uri of the dataset, or the dataset object itself. Using the dataset object can be more efficient because it can re-use the file metadata cache. transactions: Iterable[Transaction] The transactions to apply to the dataset. These will be merged into a single transaction and applied to the dataset. Note: Only append transactions are currently supported. Other transaction types will be supported in the future. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:migrate_manifest_paths_v2 method. Default is False. WARNING: turning this on will make the dataset unreadable for older versions of Lance (prior to 0.17.0). detached : bool, optional If True, then the commit will not be part of the dataset lineage. It will never show up as the latest dataset and the only way to check it out in the future will be to specifically check it out by version. The version will be a random version that is only unique amongst detached commits. The caller should store this somewhere as there will be no other way to obtain it in the future. max_retries : int The maximum number of retries to perform when committing the dataset. Returns&para; dict with keys: dataset: LanceDataset A new version of Lance Dataset. merged: Transaction The merged transaction that was applied to the dataset. count_rows(filter=None, **kwargs) &para; Count rows matching the scanner filter. Parameters&para; **kwargs : dict, optional See py:method:scanner method for full parameter description. Returns&para; count : int The total number of rows in the dataset. create_index(column, index_type, name=None, metric=&#39;L2&#39;, replace=False, num_partitions=None, ivf_centroids=None, pq_codebook=None, num_sub_vectors=None, accelerator=None, index_cache_size=None, shuffle_partition_batches=None, shuffle_partition_concurrency=None, ivf_centroids_file=None, precomputed_partition_dataset=None, storage_options=None, filter_nan=True, one_pass_ivfpq=False, **kwargs) &para; Create index on column. Experimental API Parameters&para; column : str The column to be indexed. index_type : str The type of the index. "IVF_PQ, IVF_HNSW_PQ and IVF_HNSW_SQ" are supported now. name : str, optional The index name. If not provided, it will be generated from the column name. metric : str The distance metric type, i.e., "L2" (alias to "euclidean"), "cosine" or "dot" (dot product). Default is "L2". replace : bool Replace the existing index if it exists. num_partitions : int, optional The number of partitions of IVF (Inverted File Index). ivf_centroids : optional It can be either class:np.ndarray, class:pyarrow.FixedSizeListArray or class:pyarrow.FixedShapeTensorArray. A num_partitions x dimension array of existing K-mean centroids for IVF clustering. If not provided, a new KMeans model will be trained. pq_codebook : optional, It can be class:np.ndarray, class:pyarrow.FixedSizeListArray, or class:pyarrow.FixedShapeTensorArray. A num_sub_vectors x (2 ^ nbits * dimensions // num_sub_vectors) array of K-mean centroids for PQ codebook. Note: ``nbits`` is always 8 for now. If not provided, a new PQ model will be trained. num_sub_vectors : int, optional The number of sub-vectors for PQ (Product Quantization). accelerator : str or torch.Device, optional If set, use an accelerator to speed up the training process. Accepted accelerator: "cuda" (Nvidia GPU) and "mps" (Apple Silicon GPU). If not set, use the CPU. index_cache_size : int, optional The size of the index cache in number of entries. Default value is 256. shuffle_partition_batches : int, optional The number of batches, using the row group size of the dataset, to include in each shuffle partition. Default value is 10240. Assuming the row group size is 1024, each shuffle partition will hold 10240 * 1024 = 10,485,760 rows. By making this value smaller, this shuffle will consume less memory but will take longer to complete, and vice versa. shuffle_partition_concurrency : int, optional The number of shuffle partitions to process concurrently. Default value is 2 By making this value smaller, this shuffle will consume less memory but will take longer to complete, and vice versa. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. filter_nan: bool Defaults to True. False is UNSAFE, and will cause a crash if any null/nan values are present (and otherwise will not). Disables the null filter used for nullable columns. Obtains a small speed boost. one_pass_ivfpq: bool Defaults to False. If enabled, index type must be "IVF_PQ". Reduces disk IO. kwargs : Parameters passed to the index building process. The SQ (Scalar Quantization) is available for only IVF_HNSW_SQ index type, this quantization method is used to reduce the memory usage of the index, it maps the float vectors to integer vectors, each integer is of num_bits, now only 8 bits are supported. If index_type is "IVF_*", then the following parameters are required: num_partitions If index_type is with "PQ", then the following parameters are required: num_sub_vectors Optional parameters for IVF_PQ: - ivf_centroids Existing K-mean centroids for IVF clustering. - num_bits The number of bits for PQ (Product Quantization). Default is 8. Only 4, 8 are supported. Optional parameters for IVF_HNSW_*: max_level Int, the maximum number of levels in the graph. m Int, the number of edges per node in the graph. ef_construction Int, the number of nodes to examine during the construction. Examples&para; .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_PQ&quot;, num_partitions=256, num_sub_vectors=16 ) .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_HNSW_SQ&quot;, num_partitions=256, ) Experimental Accelerator (GPU) support: accelerate: use GPU to train IVF partitions. Only supports CUDA (Nvidia) or MPS (Apple) currently. Requires PyTorch being installed. .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_PQ&quot;, num_partitions=256, num_sub_vectors=16, accelerator=&quot;cuda&quot; ) References&para; Faiss Index &lt;https://github.com/facebookresearch/faiss/wiki/Faiss-indexes&gt;_ IVF introduced in Video Google: a text retrieval approach to object matching in videos &lt;https://ieeexplore.ieee.org/abstract/document/1238663&gt;_ Product quantization for nearest neighbor search &lt;https://hal.inria.fr/inria-00514462v2/document&gt;_ create_scalar_index(column, index_type, name=None, *, replace=True, **kwargs) &para; Create a scalar index on a column. Scalar indices, like vector indices, can be used to speed up scans. A scalar index can speed up scans that contain filter expressions on the indexed column. For example, the following scan will be faster if the column my_col has a scalar index: .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) my_table = dataset.scanner(filter=&quot;my_col != 7&quot;).to_table() Vector search with pre-filers can also benefit from scalar indices. For example, .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) my_table = dataset.scanner( nearest=dict( column=&quot;vector&quot;, q=[1, 2, 3, 4], k=10, ) filter=&quot;my_col != 7&quot;, prefilter=True ) There are 5 types of scalar indices available today. BTREE. The most common type is BTREE. This index is inspired by the btree data structure although only the first few layers of the btree are cached in memory. It will perform well on columns with a large number of unique values and few rows per value. BITMAP. This index stores a bitmap for each unique value in the column. This index is useful for columns with a small number of unique values and many rows per value. LABEL_LIST. A special index that is used to index list columns whose values have small cardinality. For example, a column that contains lists of tags (e.g. ["tag1", "tag2", "tag3"]) can be indexed with a LABEL_LIST index. This index can only speedup queries with array_has_any or array_has_all filters. NGRAM. A special index that is used to index string columns. This index creates a bitmap for each ngram in the string. By default we use trigrams. This index can currently speed up queries using the contains function in filters. FTS/INVERTED. It is used to index document columns. This index can conduct full-text searches. For example, a column that contains any word of query string "hello world". The results will be ranked by BM25. Note that the LANCE_BYPASS_SPILLING environment variable can be used to bypass spilling to disk. Setting this to true can avoid memory exhaustion issues (see https://github.com/apache/datafusion/issues/10073 for more info). Experimental API Parameters&para; column : str The column to be indexed. Must be a boolean, integer, float, or string column. index_type : str The type of the index. One of "BTREE", "BITMAP", "LABEL_LIST", "NGRAM", "FTS" or "INVERTED". name : str, optional The index name. If not provided, it will be generated from the column name. replace : bool, default True Replace the existing index if it exists. bool, default True This is for the INVERTED index. If True, the index will store the positions of the words in the document, so that you can conduct phrase query. This will significantly increase the index size. It won't impact the performance of non-phrase queries even if it is set to True. base_tokenizer: str, default "simple" This is for the INVERTED index. The base tokenizer to use. The value can be: * "simple": splits tokens on whitespace and punctuation. * "whitespace": splits tokens on whitespace. * "raw": no tokenization. language: str, default "English" This is for the INVERTED index. The language for stemming and stop words. This is only used when stem or remove_stop_words is true max_token_length: Optional[int], default 40 This is for the INVERTED index. The maximum token length. Any token longer than this will be removed. lower_case: bool, default True This is for the INVERTED index. If True, the index will convert all text to lowercase. stem: bool, default False This is for the INVERTED index. If True, the index will stem the tokens. remove_stop_words: bool, default False This is for the INVERTED index. If True, the index will remove stop words. ascii_folding: bool, default False This is for the INVERTED index. If True, the index will convert non-ascii characters to ascii characters if possible. This would remove accents like "é" -&gt; "e". Examples&para; .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) dataset.create_index( &quot;category&quot;, &quot;BTREE&quot;, ) Scalar indices can only speed up scans for basic filters using equality, comparison, range (e.g. my_col BETWEEN 0 AND 100), and set membership (e.g. my_col IN (0, 1, 2)) Scalar indices can be used if the filter contains multiple indexed columns and the filter criteria are AND'd or OR'd together (e.g. my_col &lt; 0 AND other_col&gt; 100) Scalar indices may be used if the filter contains non-indexed columns but, depending on the structure of the filter, they may not be usable. For example, if the column not_indexed does not have a scalar index then the filter my_col = 0 OR not_indexed = 1 will not be able to use any scalar index on my_col. To determine if a scan is making use of a scalar index you can use explain_plan to look at the query plan that lance has created. Queries that use scalar indices will either have a ScalarIndexQuery relation or a MaterializeIndex operator. delete(predicate) &para; Delete rows from the dataset. This marks rows as deleted, but does not physically remove them from the files. This keeps the existing indexes still valid. Parameters&para; predicate : str or pa.compute.Expression The predicate to use to select rows to delete. May either be a SQL string or a pyarrow Expression. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.delete("a = 1 or b in ('a', 'b')") dataset.to_table() pyarrow.Table a: int64 b: string a: [[3]] b: [["c"]] drop_columns(columns) &para; Drop one or more columns from the dataset This is a metadata-only operation and does not remove the data from the underlying storage. In order to remove the data, you must subsequently call compact_files to rewrite the data without the removed columns and then call cleanup_old_versions to remove the old files. Parameters&para; columns : list of str The names of the columns to drop. These can be nested column references (e.g. "a.b.c") or top-level column names (e.g. "a"). Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.drop_columns(["a"]) dataset.to_table().to_pandas() b 0 a 1 b 2 c drop_index(name) &para; Drops an index from the dataset Note: Indices are dropped by "index name". This is not the same as the field name. If you did not specify a name when you created the index then a name was generated for you. You can use the list_indices method to get the names of the indices. get_fragment(fragment_id) &para; Get the fragment with fragment id. get_fragments(filter=None) &para; Get all fragments from the dataset. Note: filter is not supported yet. head(num_rows, **kwargs) &para; Load the first N rows of the dataset. Parameters&para; num_rows : int The number of rows to load. **kwargs : dict, optional See scanner() method for full parameter description. Returns&para; table : Table insert(data, *, mode=&#39;append&#39;, **kwargs) &para; Insert data into the dataset. Parameters&para; data_obj: Reader-like The data to be written. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader - Huggingface dataset mode: str, default 'append' The mode to use when writing the data. Options are: create - create a new dataset (raises if uri already exists). overwrite - create a new snapshot version append - create a new version that is the concat of the input the latest version (raises if uri does not exist) **kwargs : dict, optional Additional keyword arguments to pass to :func:write_dataset. join(right_dataset, keys, right_keys=None, join_type=&#39;left outer&#39;, left_suffix=None, right_suffix=None, coalesce_keys=True, use_threads=True) &para; Not implemented (just override pyarrow dataset to prevent segfault) merge(data_obj, left_on, right_on=None, schema=None) &para; Merge another dataset into this one. Performs a left join, where the dataset is the left side and data_obj is the right side. Rows existing in the dataset but not on the left will be filled with null values, unless Lance doesn't support null values for some types, in which case an error will be raised. Parameters&para; data_obj: Reader-like The data to be merged. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, Iterator[RecordBatch], or RecordBatchReader left_on: str The name of the column in the dataset to join on. right_on: str or None The name of the column in data_obj to join on. If None, defaults to left_on. Examples&para; import lance import pyarrow as pa df = pa.table({'x': [1, 2, 3], 'y': ['a', 'b', 'c']}) dataset = lance.write_dataset(df, "dataset") dataset.to_table().to_pandas() x y 0 1 a 1 2 b 2 3 c new_df = pa.table({'x': [1, 2, 3], 'z': ['d', 'e', 'f']}) dataset.merge(new_df, 'x') dataset.to_table().to_pandas() x y z 0 1 a d 1 2 b e 2 3 c f See Also&para; LanceDataset.add_columns : Add new columns by computing batch-by-batch. merge_insert(on) &para; Returns a builder that can be used to create a "merge insert" operation This operation can add rows, update rows, and remove rows in a single transaction. It is a very generic tool that can be used to create behaviors like "insert if not exists", "update or insert (i.e. upsert)", or even replace a portion of existing data with new data (e.g. replace all data where month="january") The merge insert operation works by combining new data from a source table with existing data in a target table by using a join. There are three categories of records. "Matched" records are records that exist in both the source table and the target table. "Not matched" records exist only in the source table (e.g. these are new data). "Not matched by source" records exist only in the target table (this is old data). The builder returned by this method can be used to customize what should happen for each category of data. Please note that the data will be reordered as part of this operation. This is because updated rows will be deleted from the dataset and then reinserted at the end with the new values. The order of the newly inserted rows may fluctuate randomly because a hash-join operation is used internally. Parameters&para; Union[str, Iterable[str]] A column (or columns) to join on. This is how records from the source table and target table are matched. Typically this is some kind of key or id column. Examples&para; Use when_matched_update_all() and when_not_matched_insert_all() to perform an "upsert" operation. This will update rows that already exist in the dataset and insert rows that do not exist. import lance import pyarrow as pa table = pa.table({"a": [2, 1, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform a "upsert" operation&para; dataset.merge_insert("a") \ ... .when_matched_update_all() \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b 0 1 b 1 2 x 2 3 y 3 4 z Use when_not_matched_insert_all() to perform an "insert if not exists" operation. This will only insert rows that do not already exist in the dataset. import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example2") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform an "insert if not exists" operation&para; dataset.merge_insert("a") \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 0, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 z You are not required to provide all the columns. If you only want to update a subset of columns, you can omit columns you don't want to update. Omitted columns will keep their existing values if they are updated, or will be null if they are inserted. import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"], \ ... "c": ["x", "y", "z"]}) dataset = lance.write_dataset(table, "example3") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform an "upsert" operation, only updating column "a"&para; dataset.merge_insert("a") \ ... .when_matched_update_all() \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b c 0 1 a x 1 2 x y 2 3 y z 3 4 z None migrate_manifest_paths_v2() &para; Migrate the manifest paths to the new format. This will update the manifest to use the new v2 format for paths. This function is idempotent, and can be run multiple times without changing the state of the object store. DANGER: this should not be run while other concurrent operations are happening. And it should also run until completion before resuming other operations. prewarm_index(name) &para; Prewarm an index This will load the entire index into memory. This can help avoid cold start issues with index queries. If the index does not fit in the index cache, then this will result in wasted I/O. Parameters&para; name: str The name of the index to prewarm. replace_field_metadata(field_name, new_metadata) &para; Replace the metadata of a field in the schema Parameters&para; field_name: str The name of the field to replace the metadata for new_metadata: dict The new metadata to set replace_schema(schema) &para; Not implemented (just override pyarrow dataset to prevent segfault) See method:replace_schema_metadata or method:replace_field_metadata replace_schema_metadata(new_metadata) &para; Replace the schema metadata of the dataset Parameters&para; new_metadata: dict The new metadata to set restore() &para; Restore the currently checked out version as the latest version of the dataset. This creates a new commit. sample(num_rows, columns=None, randomize_order=True, **kwargs) &para; Select a random sample of data Parameters&para; num_rows: int number of rows to retrieve columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. **kwargs : dict, optional see scanner() method for full parameter description. Returns&para; table : Table scanner(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, fragments=None, full_text_query=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, fast_search=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, include_deleted_rows=None, scan_stats_callback=None, strict_batch_size=None) &para; Return a Scanner that can support various pushdowns. Parameters&para; columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. filter: pa.compute.Expression or str Expression or str that is a valid SQL where clause. See Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;_ for valid SQL expressions. limit: int, default None Fetch up to this many rows. All rows if None or unspecified. offset: int, default None Fetch starting with this row. 0 if None or unspecified. nearest: dict, default None Get the rows corresponding to the K most similar vectors. Example: .. code-block:: python { &quot;column&quot;: &lt;embedding col name&gt;, &quot;q&quot;: &lt;query vector as pa.Float32Array&gt;, &quot;k&quot;: 10, &quot;nprobes&quot;: 1, &quot;refine_factor&quot;: 1 } int, default None The target size of batches returned. In some cases batches can be up to twice this size (but never larger than this). In some cases batches can be smaller than this size. io_buffer_size: int, default None The size of the IO buffer. See ScannerBuilder.io_buffer_size for more information. batch_readahead: int, optional The number of batches to read ahead. fragment_readahead: int, optional The number of fragments to read ahead. scan_in_order: bool, default True Whether to read the fragments and batches in order. If false, throughput may be higher, but batches will be returned out of order and memory use might increase. fragments: iterable of LanceFragment, default None If specified, only scan these fragments. If scan_in_order is True, then the fragments will be scanned in the order given. prefilter: bool, default False If True then the filter will be applied before the vector query is run. This will generate more correct results but it may be a more costly query. It's generally good when the filter is highly selective. If False then the filter will be applied after the vector query is run. This will perform well but the results may have fewer than the requested number of rows (or be empty) if the rows closest to the query do not match the filter. It&#39;s generally good when the filter is not very selective. use_scalar_index: bool, default True Lance will automatically use scalar indices to optimize a query. In some corner cases this can make query performance worse and this parameter can be used to disable scalar indices in these cases. late_materialization: bool or List[str], default None Allows custom control over late materialization. Late materialization fetches non-query columns using a take operation after the filter. This is useful when there are few results or columns are very large. Early materialization can be better when there are many results or the columns are very narrow. If True, then all columns are late materialized. If False, then all columns are early materialized. If a list of strings, then only the columns in the list are late materialized. The default uses a heuristic that assumes filters will select about 0.1% of the rows. If your filter is more selective (e.g. find by id) you may want to set this to True. If your filter is not very selective (e.g. matches 20% of the rows) you may want to set this to False. full_text_query: str or dict, optional query string to search for, the results will be ranked by BM25. e.g. "hello world", would match documents containing "hello" or "world". or a dictionary with the following keys: - columns: list[str] The columns to search, currently only supports a single column in the columns list. - query: str The query string to search for. fast_search: bool, default False If True, then the search will only be performed on the indexed data, which yields faster search time. scan_stats_callback: Callable[[ScanStatistics], None], default None A callback function that will be called with the scan statistics after the scan is complete. Errors raised by the callback will be logged but not re-raised. include_deleted_rows: bool, default False If True, then rows that have been deleted, but are still present in the fragment, will be returned. These rows will have the _rowid column set to null. All other columns will reflect the value stored on disk and may not be null. Note: if this is a search operation, or a take operation (including scalar indexed scans) then deleted rows cannot be returned. .. note:: For now, if BOTH filter and nearest is specified, then: 1. nearest is executed first. 2. The results are filtered afterwards. For debugging ANN results, you can choose to not use the index even if present by specifying use_index=False. For example, the following will always return exact KNN results: .. code-block:: python dataset.to_table(nearest={ &quot;column&quot;: &quot;vector&quot;, &quot;k&quot;: 10, &quot;q&quot;: &lt;query vector&gt;, &quot;use_index&quot;: False } session() &para; Return the dataset session, which holds the dataset's state. take(indices, columns=None) &para; Select rows of data by index. Parameters&para; indices : Array or array-like indices of rows to select in the dataset. columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. Returns&para; table : pyarrow.Table take_blobs(blob_column, ids=None, addresses=None, indices=None) &para; Select blobs by row IDs. Instead of loading large binary blob data into memory before processing it, this API allows you to open binary blob data as a regular Python file-like object. For more details, see class:lance.BlobFile. Exactly one of ids, addresses, or indices must be specified. Parameters blob_column : str The name of the blob column to select. ids : Integer Array or array-like row IDs to select in the dataset. addresses: Integer Array or array-like The (unstable) row addresses to select in the dataset. indices : Integer Array or array-like The offset / indices of the row in the dataset. Returns&para; blob_files : List[BlobFile] to_batches(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, full_text_query=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, strict_batch_size=None, **kwargs) &para; Read the dataset as materialized record batches. Parameters&para; **kwargs : dict, optional Arguments for meth:~LanceDataset.scanner. Returns&para; record_batches : Iterator of class:~pyarrow.RecordBatch to_table(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, fast_search=None, full_text_query=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, include_deleted_rows=None) &para; Read the data into memory as a class:pyarrow.Table Parameters&para; columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. filter : pa.compute.Expression or str Expression or str that is a valid SQL where clause. See Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;_ for valid SQL expressions. limit: int, default None Fetch up to this many rows. All rows if None or unspecified. offset: int, default None Fetch starting with this row. 0 if None or unspecified. nearest: dict, default None Get the rows corresponding to the K most similar vectors. Example: .. code-block:: python { &quot;column&quot;: &lt;embedding col name&gt;, &quot;q&quot;: &lt;query vector as pa.Float32Array&gt;, &quot;k&quot;: 10, &quot;metric&quot;: &quot;cosine&quot;, &quot;nprobes&quot;: 1, &quot;refine_factor&quot;: 1 } int, optional The number of rows to read at a time. io_buffer_size: int, default None The size of the IO buffer. See ScannerBuilder.io_buffer_size for more information. batch_readahead: int, optional The number of batches to read ahead. fragment_readahead: int, optional The number of fragments to read ahead. scan_in_order: bool, optional, default True Whether to read the fragments and batches in order. If false, throughput may be higher, but batches will be returned out of order and memory use might increase. prefilter: bool, optional, default False Run filter before the vector search. late_materialization: bool or List[str], default None Allows custom control over late materialization. See ScannerBuilder.late_materialization for more information. use_scalar_index: bool, default True Allows custom control over scalar index usage. See ScannerBuilder.use_scalar_index for more information. with_row_id: bool, optional, default False Return row ID. with_row_address: bool, optional, default False Return row address use_stats: bool, optional, default True Use stats pushdown during filters. fast_search: bool, optional, default False full_text_query: str or dict, optional query string to search for, the results will be ranked by BM25. e.g. "hello world", would match documents contains "hello" or "world". or a dictionary with the following keys: - columns: list[str] The columns to search, currently only supports a single column in the columns list. - query: str The query string to search for. include_deleted_rows: bool, optional, default False If True, then rows that have been deleted, but are still present in the fragment, will be returned. These rows will have the _rowid column set to null. All other columns will reflect the value stored on disk and may not be null. Note: if this is a search operation, or a take operation (including scalar indexed scans) then deleted rows cannot be returned. Notes&para; If BOTH filter and nearest is specified, then: nearest is executed first. The results are filtered afterward, unless pre-filter sets to True. update(updates, where=None) &para; Update column values for rows matching where. Parameters&para; updates : dict of str to str A mapping of column names to a SQL expression. where : str, optional A SQL predicate indicating which rows should be updated. Returns&para; updates : dict A dictionary containing the number of rows updated. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") update_stats = dataset.update(dict(a = 'a + 2'), where="b != 'a'") update_stats["num_updated_rows"] = 2 dataset.to_table().to_pandas() a b 0 1 a 1 4 b 2 5 c validate() &para; Validate the dataset. This checks the integrity of the dataset and will raise an exception if the dataset is corrupted. versions() &para; Return all versions in this dataset. LanceOperation &para; Append dataclass &para; Bases: BaseOperation Append new rows to the dataset. Attributes&para; fragments: list[FragmentMetadata] The fragments that contain the new rows. Warning&para; This is an advanced API for distributed operations. To append to a dataset on a single machine, use :func:lance.write_dataset. Examples&para; To append new rows to a dataset, first use :meth:lance.fragment.LanceFragment.create to create fragments. Then collect the fragment metadata into a list and pass it to this class. Finally, pass the operation to the :meth:LanceDataset.commit method to create the new dataset. import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) dataset = lance.write_dataset(tab1, "example") tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment = lance.fragment.LanceFragment.create("example", tab2) operation = lance.LanceOperation.Append([fragment]) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d BaseOperation &para; Bases: ABC Base class for operations that can be applied to a dataset. See available operations under :class:LanceOperation. CreateIndex dataclass &para; Bases: BaseOperation Operation that creates an index on the dataset. DataReplacement dataclass &para; Bases: BaseOperation Operation that replaces existing datafiles in the dataset. DataReplacementGroup dataclass &para; Group of data replacements Delete dataclass &para; Bases: BaseOperation Remove fragments or rows from the dataset. Attributes&para; updated_fragments: list[FragmentMetadata] The fragments that have been updated with new deletion vectors. deleted_fragment_ids: list[int] The ids of the fragments that have been deleted entirely. These are the fragments where :meth:LanceFragment.delete() returned None. predicate: str The original SQL predicate used to select the rows to delete. Warning&para; This is an advanced API for distributed operations. To delete rows from dataset on a single machine, use :meth:lance.LanceDataset.delete. Examples&para; To delete rows from a dataset, call :meth:lance.fragment.LanceFragment.delete on each of the fragments. If that returns a new fragment, add that to the updated_fragments list. If it returns None, that means the whole fragment was deleted, so add the fragment id to the deleted_fragment_ids. Finally, pass the operation to the :meth:LanceDataset.commit method to complete the deletion operation. import lance import pyarrow as pa table = pa.table({"a": [1, 2], "b": ["a", "b"]}) dataset = lance.write_dataset(table, "example") table = pa.table({"a": [3, 4], "b": ["c", "d"]}) dataset = lance.write_dataset(table, "example", mode="append") dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d predicate = "a &gt;= 2" updated_fragments = [] deleted_fragment_ids = [] for fragment in dataset.get_fragments(): ... new_fragment = fragment.delete(predicate) ... if new_fragment is not None: ... updated_fragments.append(new_fragment) ... else: ... deleted_fragment_ids.append(fragment.fragment_id) operation = lance.LanceOperation.Delete(updated_fragments, ... deleted_fragment_ids, ... predicate) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b 0 1 a Merge dataclass &para; Bases: BaseOperation Operation that adds columns. Unlike Overwrite, this should not change the structure of the fragments, allowing existing indices to be kept. Attributes&para; fragments: iterable of FragmentMetadata The fragments that make up the new dataset. schema: LanceSchema or pyarrow.Schema The schema of the new dataset. Passing a LanceSchema is preferred, and passing a pyarrow.Schema is deprecated. Warning&para; This is an advanced API for distributed operations. To overwrite or create new dataset on a single machine, use :func:lance.write_dataset. Examples&para; To add new columns to a dataset, first define a method that will create the new columns based on the existing columns. Then use :meth:lance.fragment.LanceFragment.add_columns import lance import pyarrow as pa import pyarrow.compute as pc table = pa.table({"a": [1, 2, 3, 4], "b": ["a", "b", "c", "d"]}) dataset = lance.write_dataset(table, "example") dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d def double_a(batch: pa.RecordBatch) -&gt; pa.RecordBatch: ... doubled = pc.multiply(batch["a"], 2) ... return pa.record_batch([doubled], ["a_doubled"]) fragments = [] for fragment in dataset.get_fragments(): ... new_fragment, new_schema = fragment.merge_columns(double_a, ... columns=['a']) ... fragments.append(new_fragment) operation = lance.LanceOperation.Merge(fragments, new_schema) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b a_doubled 0 1 a 2 1 2 b 4 2 3 c 6 3 4 d 8 Overwrite dataclass &para; Bases: BaseOperation Overwrite or create a new dataset. Attributes&para; new_schema: pyarrow.Schema The schema of the new dataset. fragments: list[FragmentMetadata] The fragments that make up the new dataset. Warning&para; This is an advanced API for distributed operations. To overwrite or create new dataset on a single machine, use :func:lance.write_dataset. Examples&para; To create or overwrite a dataset, first use :meth:lance.fragment.LanceFragment.create to create fragments. Then collect the fragment metadata into a list and pass it along with the schema to this class. Finally, pass the operation to the :meth:LanceDataset.commit method to create the new dataset. import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment1 = lance.fragment.LanceFragment.create("example", tab1) fragment2 = lance.fragment.LanceFragment.create("example", tab2) fragments = [fragment1, fragment2] operation = lance.LanceOperation.Overwrite(tab1.schema, fragments) dataset = lance.LanceDataset.commit("example", operation) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d Project dataclass &para; Bases: BaseOperation Operation that project columns. Use this operator for drop column or rename/swap column. Attributes&para; schema: LanceSchema The lance schema of the new dataset. Examples&para; Use the projece operator to swap column: import lance import pyarrow as pa import pyarrow.compute as pc from lance.schema import LanceSchema table = pa.table({"a": [1, 2], "b": ["a", "b"], "b1": ["c", "d"]}) dataset = lance.write_dataset(table, "example") dataset.to_table().to_pandas() a b b1 0 1 a c 1 2 b d rename column b into b0 and rename b1 into b&para; table = pa.table({"a": [3, 4], "b0": ["a", "b"], "b": ["c", "d"]}) lance_schema = LanceSchema.from_pyarrow(table.schema) operation = lance.LanceOperation.Project(lance_schema) dataset = lance.LanceDataset.commit("example", operation, read_version=1) dataset.to_table().to_pandas() a b0 b 0 1 a c 1 2 b d Restore dataclass &para; Bases: BaseOperation Operation that restores a previous version of the dataset. Rewrite dataclass &para; Bases: BaseOperation Operation that rewrites one or more files and indices into one or more files and indices. Attributes&para; groups: list[RewriteGroup] Groups of files that have been rewritten. rewritten_indices: list[RewrittenIndex] Indices that have been rewritten. Warning&para; This is an advanced API not intended for general use. RewriteGroup dataclass &para; Collection of rewritten files RewrittenIndex dataclass &para; An index that has been rewritten Update dataclass &para; Bases: BaseOperation Operation that updates rows in the dataset. Attributes&para; removed_fragment_ids: list[int] The ids of the fragments that have been removed entirely. updated_fragments: list[FragmentMetadata] The fragments that have been updated with new deletion vectors. new_fragments: list[FragmentMetadata] The fragments that contain the new rows. LanceScanner &para; Bases: Scanner dataset_schema property &para; The schema with which batches will be read from fragments. analyze_plan() &para; Execute the plan for this scanner and display with runtime metrics. Parameters&para; verbose : bool, default False Use a verbose output format. Returns&para; plan : str count_rows() &para; Count rows matching the scanner filter. Returns&para; count : int explain_plan(verbose=False) &para; Return the execution plan for this scanner. Parameters&para; verbose : bool, default False Use a verbose output format. Returns&para; plan : str from_batches(*args, **kwargs) staticmethod &para; Not implemented from_dataset(*args, **kwargs) staticmethod &para; Not implemented from_fragment(*args, **kwargs) staticmethod &para; Not implemented head(num_rows) &para; Load the first N rows of the dataset. Parameters&para; num_rows : int The number of rows to load. Returns&para; Table scan_batches() &para; Consume a Scanner in record batches with corresponding fragments. Returns&para; record_batches : iterator of TaggedRecordBatch take(indices) &para; Not implemented to_table() &para; Read the data into memory and return a pyarrow Table. LanceStats &para; Statistics about a LanceDataset. data_stats() &para; Statistics about the data in the dataset. dataset_stats(max_rows_per_group=1024) &para; Statistics about the dataset. index_stats(index_name) &para; Statistics about an index. Parameters&para; index_name: str The name of the index to get statistics for. MergeInsertBuilder &para; Bases: _MergeInsertBuilder conflict_retries(max_retries) &para; Set number of times to retry the operation if there is contention. If this is set &gt; 0, then the operation will keep a copy of the input data either in memory or on disk (depending on the size of the data) and will retry the operation if there is contention. Default is 10. execute(data_obj, *, schema=None) &para; Executes the merge insert operation This function updates the original dataset and returns a dictionary with information about merge statistics - i.e. the number of inserted, updated, and deleted rows. Parameters&para; ReaderLike The new data to use as the source table for the operation. This parameter can be any source of data (e.g. table / dataset) that :func:~lance.write_dataset accepts. schema: Optional[pa.Schema] The schema of the data. This only needs to be supplied whenever the data source is some kind of generator. execute_uncommitted(data_obj, *, schema=None) &para; Executes the merge insert operation without committing This function updates the original dataset and returns a dictionary with information about merge statistics - i.e. the number of inserted, updated, and deleted rows. Parameters&para; ReaderLike The new data to use as the source table for the operation. This parameter can be any source of data (e.g. table / dataset) that :func:~lance.write_dataset accepts. schema: Optional[pa.Schema] The schema of the data. This only needs to be supplied whenever the data source is some kind of generator. retry_timeout(timeout) &para; Set the timeout used to limit retries. This is the maximum time to spend on the operation before giving up. At least one attempt will be made, regardless of how long it takes to complete. Subsequent attempts will be cancelled once this timeout is reached. If the timeout has been reached during the first attempt, the operation will be cancelled immediately before making a second attempt. The default is 30 seconds. when_matched_update_all(condition=None) &para; Configure the operation to update matched rows After this method is called, when the merge insert operation executes, any rows that match both the source table and the target table will be updated. The rows from the target table will be removed and the rows from the source table will be added. An optional condition may be specified. This should be an SQL filter and, if present, then only matched rows that also satisfy this filter will be updated. The SQL filter should use the prefix target. to refer to columns in the target table and the prefix source. to refer to columns in the source table. For example, source.last_update &lt; target.last_update. If a condition is specified and rows do not satisfy the condition then these rows will not be updated. Failure to satisfy the filter does not cause a "matched" row to become a "not matched" row. when_not_matched_by_source_delete(expr=None) &para; Configure the operation to delete source rows that do not match After this method is called, when the merge insert operation executes, any rows that exist only in the target table will be deleted. An optional filter can be specified to limit the scope of the delete operation. If given (as an SQL filter) then only rows which match the filter will be deleted. when_not_matched_insert_all() &para; Configure the operation to insert not matched rows After this method is called, when the merge insert operation executes, any rows that exist only in the source table will be inserted into the target table. ScannerBuilder &para; batch_readahead(nbatches=None) &para; This parameter is ignored when reading v2 files batch_size(batch_size) &para; Set batch size for Scanner fast_search(flag) &para; Enable fast search, which only perform search on the indexed data. Users can use Table::optimize() or create_index() to include the new data into index, thus make new data searchable. full_text_search(query, columns=None) &para; Filter rows by full text searching. Experimental API, may remove it after we support to do this within filter SQL-like expression Must create inverted index on the given column before searching, Parameters&para; query : str | Query If str, the query string to search for, a match query would be performed. If Query, the query object to search for, and the columns parameter will be ignored. columns : list of str, optional The columns to search in. If None, search in all indexed columns. include_deleted_rows(flag) &para; Include deleted rows Rows which have been deleted, but are still present in the fragment, will be returned. These rows will have all columns (except _rowaddr) set to null io_buffer_size(io_buffer_size) &para; Set the I/O buffer size for the Scanner This is the amount of RAM that will be reserved for holding I/O received from storage before it is processed. This is used to control the amount of memory used by the scanner. If the buffer is full then the scanner will block until the buffer is processed. Generally this should scale with the number of concurrent I/O threads. The default is 2GiB which comfortably provides enough space for somewhere between 32 and 256 concurrent I/O threads. This value is not a hard cap on the amount of RAM the scanner will use. Some space is used for the compute (which can be controlled by the batch size) and Lance does not keep track of memory after it is returned to the user. Currently, if there is a single batch of data which is larger than the io buffer size then the scanner will deadlock. This is a known issue and will be fixed in a future release. This parameter is only used when reading v2 files scan_in_order(scan_in_order=True) &para; Whether to scan the dataset in order of fragments and batches. If set to False, the scanner may read fragments concurrently and yield batches out of order. This may improve performance since it allows more concurrency in the scan, but can also use more memory. This parameter is ignored when using v2 files. In the v2 file format there is no penalty to scanning in order and so all scans will scan in order. scan_stats_callback(callback) &para; Set a callback function that will be called with the scan statistics after the scan is complete. Errors raised by the callback will be logged but not re-raised. strict_batch_size(strict_batch_size=False) &para; If True, then all batches except the last batch will have exactly batch_size rows. By default, it is false. If this is true then small batches will need to be merged together which will require a data copy and incur a (typically very small) performance penalty. use_scalar_index(use_scalar_index=True) &para; Set whether scalar indices should be used in a query Scans will use scalar indices, when available, to optimize queries with filters. However, in some corner cases, scalar indices may make performance worse. This parameter allows users to disable scalar indices in these cases. use_stats(use_stats=True) &para; Enable use of statistics for query planning. Disabling statistics is used for debugging and benchmarking purposes. This should be left on for normal use. with_row_address(with_row_address=True) &para; Enables returns with row addresses. Row addresses are a unique but unstable identifier for each row in the dataset that consists of the fragment id (upper 32 bits) and the row offset in the fragment (lower 32 bits). Row IDs are generally preferred since they do not change when a row is modified or compacted. However, row addresses may be useful in some advanced use cases. with_row_id(with_row_id=True) &para; Enable returns with row IDs. Tags &para; Dataset tag manager. create(tag, version) &para; Create a tag for a given dataset version. Parameters&para; tag: str, The name of the tag to create. This name must be unique among all tag names for the dataset. version: int, The dataset version to tag. delete(tag) &para; Delete tag from the dataset. Parameters&para; tag: str, The name of the tag to delete. list() &para; List all dataset tags. Returns&para; dict[str, Tag] A dictionary mapping tag names to version numbers. update(tag, version) &para; Update tag to a new version. Parameters&para; tag: str, The name of the tag to update. version: int, The new dataset version to tag. VectorIndexReader &para; This class allows you to initialize a reader for a specific vector index, retrieve the number of partitions, access the centroids of the index, and read specific partitions of the index. Parameters&para; dataset: LanceDataset The dataset containing the index. index_name: str The name of the vector index to read. Examples&para; .. code-block:: python import lance from lance.dataset import VectorIndexReader import numpy as np import pyarrow as pa vectors = np.random.rand(256, 2) data = pa.table({&quot;vector&quot;: pa.array(vectors.tolist(), type=pa.list_(pa.float32(), 2))}) dataset = lance.write_dataset(data, &quot;/tmp/index_reader_demo&quot;) dataset.create_index(&quot;vector&quot;, index_type=&quot;IVF_PQ&quot;, num_partitions=4, num_sub_vectors=2) reader = VectorIndexReader(dataset, &quot;vector_idx&quot;) assert reader.num_partitions() == 4 partition = reader.read_partition(0) assert &quot;_rowid&quot; in partition.column_names Exceptions&para; ValueError If the specified index is not a vector index. centroids() &para; Returns the centroids of the index Returns&para; np.ndarray The centroids of IVF with shape (num_partitions, dim) num_partitions() &para; Returns the number of partitions in the dataset. Returns&para; int The number of partitions. read_partition(partition_id, *, with_vector=False) &para; Returns a pyarrow table for the given IVF partition Parameters&para; partition_id: int The id of the partition to read with_vector: bool, default False Whether to include the vector column in the reader, for IVF_PQ, the vector column is PQ codes Returns&para; pa.Table A pyarrow table for the given partition, containing the row IDs, and quantized vectors (if with_vector is True). write_dataset(data_obj, uri, schema=None, mode=&#39;create&#39;, *, max_rows_per_file=1024 * 1024, max_rows_per_group=1024, max_bytes_per_file=90 * 1024 * 1024 * 1024, commit_lock=None, progress=None, storage_options=None, data_storage_version=None, use_legacy_format=None, enable_v2_manifest_paths=False, enable_move_stable_row_ids=False) &para; Write a given data_obj to the given uri Parameters&para; data_obj: Reader-like The data to be written. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader - Huggingface dataset uri: str, Path, or LanceDataset Where to write the dataset to (directory). If a LanceDataset is passed, the session will be reused. schema: Schema, optional If specified and the input is a pandas DataFrame, use this schema instead of the default pandas to arrow table conversion. mode: str create - create a new dataset (raises if uri already exists). overwrite - create a new snapshot version append - create a new version that is the concat of the input the latest version (raises if uri does not exist) max_rows_per_file: int, default 1024 * 1024 The max number of rows to write before starting a new file max_rows_per_group: int, default 1024 The max number of rows before starting a new group (in the same file) max_bytes_per_file: int, default 90 * 1024 * 1024 * 1024 The max number of bytes to write before starting a new file. This is a soft limit. This limit is checked after each group is written, which means larger groups may cause this to be overshot meaningfully. This defaults to 90 GB, since we have a hard limit of 100 GB per file on object stores. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. progress: FragmentWriteProgress, optional Experimental API. Progress tracking for writing the fragment. Pass a custom class that defines hooks to be called when each fragment is starting to write and finishing writing. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. data_storage_version: optional, str, default None The version of the data storage format to use. Newer versions are more efficient but require newer versions of lance to read. The default (None) will use the latest stable version. See the user guide for more details. use_legacy_format : optional, bool, default None Deprecated method for setting the data storage version. Use the data_storage_version parameter instead. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:LanceDataset.migrate_manifest_paths_v2 method. Default is False. enable_move_stable_row_ids : bool, optional Experimental parameter: if set to true, the writer will use move-stable row ids. These row ids are stable after compaction operations, but not after updates. This makes compaction more efficient, since with stable row ids no secondary indices need to be updated to point to new row ids. and rename b1 into
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Restore" class="md-nav__link">
    <span class="md-ellipsis">
      Restore
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Rewrite" class="md-nav__link">
    <span class="md-ellipsis">
      Rewrite
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Rewrite">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Rewrite--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Rewrite--warning" class="md-nav__link">
    <span class="md-ellipsis">
      Warning
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.RewriteGroup" class="md-nav__link">
    <span class="md-ellipsis">
      RewriteGroup
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.RewrittenIndex" class="md-nav__link">
    <span class="md-ellipsis">
      RewrittenIndex
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Update" class="md-nav__link">
    <span class="md-ellipsis">
      Update
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Update--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner" class="md-nav__link">
    <span class="md-ellipsis">
      LanceScanner
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LanceScanner">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.dataset_schema" class="md-nav__link">
    <span class="md-ellipsis">
      dataset_schema
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.analyze_plan" class="md-nav__link">
    <span class="md-ellipsis">
      analyze_plan
    </span>
  </a>
  
    <nav class="md-nav" aria-label="analyze_plan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.analyze_plan--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.analyze_plan--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.count_rows" class="md-nav__link">
    <span class="md-ellipsis">
      count_rows
    </span>
  </a>
  
    <nav class="md-nav" aria-label="count_rows">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.count_rows--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.explain_plan" class="md-nav__link">
    <span class="md-ellipsis">
      explain_plan
    </span>
  </a>
  
    <nav class="md-nav" aria-label="explain_plan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.explain_plan--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.explain_plan--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.from_batches" class="md-nav__link">
    <span class="md-ellipsis">
      from_batches
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.from_dataset" class="md-nav__link">
    <span class="md-ellipsis">
      from_dataset
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.from_fragment" class="md-nav__link">
    <span class="md-ellipsis">
      from_fragment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.head" class="md-nav__link">
    <span class="md-ellipsis">
      head
    </span>
  </a>
  
    <nav class="md-nav" aria-label="head">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.head--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.head--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.scan_batches" class="md-nav__link">
    <span class="md-ellipsis">
      scan_batches
    </span>
  </a>
  
    <nav class="md-nav" aria-label="scan_batches">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.scan_batches--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.take" class="md-nav__link">
    <span class="md-ellipsis">
      take
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.to_table" class="md-nav__link">
    <span class="md-ellipsis">
      to_table
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceStats" class="md-nav__link">
    <span class="md-ellipsis">
      LanceStats
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LanceStats">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceStats.data_stats" class="md-nav__link">
    <span class="md-ellipsis">
      data_stats
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceStats.dataset_stats" class="md-nav__link">
    <span class="md-ellipsis">
      dataset_stats
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceStats.index_stats" class="md-nav__link">
    <span class="md-ellipsis">
      index_stats
    </span>
  </a>
  
    <nav class="md-nav" aria-label="index_stats">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceStats.index_stats--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.MergeInsertBuilder" class="md-nav__link">
    <span class="md-ellipsis">
      MergeInsertBuilder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MergeInsertBuilder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.MergeInsertBuilder.conflict_retries" class="md-nav__link">
    <span class="md-ellipsis">
      conflict_retries
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.MergeInsertBuilder.execute" class="md-nav__link">
    <span class="md-ellipsis">
      execute
    </span>
  </a>
  
    <nav class="md-nav" aria-label="execute">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.MergeInsertBuilder.execute--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.MergeInsertBuilder.execute_uncommitted" class="md-nav__link">
    <span class="md-ellipsis">
      execute_uncommitted
    </span>
  </a>
  
    <nav class="md-nav" aria-label="execute_uncommitted">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.MergeInsertBuilder.execute_uncommitted--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.MergeInsertBuilder.retry_timeout" class="md-nav__link">
    <span class="md-ellipsis">
      retry_timeout
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.MergeInsertBuilder.when_matched_update_all" class="md-nav__link">
    <span class="md-ellipsis">
      when_matched_update_all
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.MergeInsertBuilder.when_not_matched_by_source_delete" class="md-nav__link">
    <span class="md-ellipsis">
      when_not_matched_by_source_delete
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.MergeInsertBuilder.when_not_matched_insert_all" class="md-nav__link">
    <span class="md-ellipsis">
      when_not_matched_insert_all
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder" class="md-nav__link">
    <span class="md-ellipsis">
      ScannerBuilder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ScannerBuilder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.batch_readahead" class="md-nav__link">
    <span class="md-ellipsis">
      batch_readahead
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.batch_size" class="md-nav__link">
    <span class="md-ellipsis">
      batch_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.fast_search" class="md-nav__link">
    <span class="md-ellipsis">
      fast_search
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.full_text_search" class="md-nav__link">
    <span class="md-ellipsis">
      full_text_search
    </span>
  </a>
  
    <nav class="md-nav" aria-label="full_text_search">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.full_text_search--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.include_deleted_rows" class="md-nav__link">
    <span class="md-ellipsis">
      include_deleted_rows
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.io_buffer_size" class="md-nav__link">
    <span class="md-ellipsis">
      io_buffer_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.scan_in_order" class="md-nav__link">
    <span class="md-ellipsis">
      scan_in_order
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.scan_stats_callback" class="md-nav__link">
    <span class="md-ellipsis">
      scan_stats_callback
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.strict_batch_size" class="md-nav__link">
    <span class="md-ellipsis">
      strict_batch_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.use_scalar_index" class="md-nav__link">
    <span class="md-ellipsis">
      use_scalar_index
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.use_stats" class="md-nav__link">
    <span class="md-ellipsis">
      use_stats
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.with_row_address" class="md-nav__link">
    <span class="md-ellipsis">
      with_row_address
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.with_row_id" class="md-nav__link">
    <span class="md-ellipsis">
      with_row_id
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.Tags" class="md-nav__link">
    <span class="md-ellipsis">
      Tags
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Tags">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.Tags.create" class="md-nav__link">
    <span class="md-ellipsis">
      create
    </span>
  </a>
  
    <nav class="md-nav" aria-label="create">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.Tags.create--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.Tags.delete" class="md-nav__link">
    <span class="md-ellipsis">
      delete
    </span>
  </a>
  
    <nav class="md-nav" aria-label="delete">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.Tags.delete--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.Tags.list" class="md-nav__link">
    <span class="md-ellipsis">
      list
    </span>
  </a>
  
    <nav class="md-nav" aria-label="list">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.Tags.list--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.Tags.update" class="md-nav__link">
    <span class="md-ellipsis">
      update
    </span>
  </a>
  
    <nav class="md-nav" aria-label="update">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.Tags.update--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.VectorIndexReader" class="md-nav__link">
    <span class="md-ellipsis">
      VectorIndexReader
    </span>
  </a>
  
    <nav class="md-nav" aria-label="VectorIndexReader">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.VectorIndexReader--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.VectorIndexReader--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.VectorIndexReader--exceptions" class="md-nav__link">
    <span class="md-ellipsis">
      Exceptions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.VectorIndexReader.centroids" class="md-nav__link">
    <span class="md-ellipsis">
      centroids
    </span>
  </a>
  
    <nav class="md-nav" aria-label="centroids">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.VectorIndexReader.centroids--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.VectorIndexReader.num_partitions" class="md-nav__link">
    <span class="md-ellipsis">
      num_partitions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="num_partitions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.VectorIndexReader.num_partitions--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.VectorIndexReader.read_partition" class="md-nav__link">
    <span class="md-ellipsis">
      read_partition
    </span>
  </a>
  
    <nav class="md-nav" aria-label="read_partition">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.VectorIndexReader.read_partition--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.VectorIndexReader.read_partition--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.write_dataset" class="md-nav__link">
    <span class="md-ellipsis">
      write_dataset
    </span>
  </a>
  
    <nav class="md-nav" aria-label="write_dataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.write_dataset--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../contributing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Contributing
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lance_1" class="md-nav__link">
    <span class="md-ellipsis">
      lance
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lance" class="md-nav__link">
    <span class="md-ellipsis">
      lance
    </span>
  </a>
  
    <nav class="md-nav" aria-label="lance">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.__version__" class="md-nav__link">
    <span class="md-ellipsis">
      __version__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.BlobColumn" class="md-nav__link">
    <span class="md-ellipsis">
      BlobColumn
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.BlobFile" class="md-nav__link">
    <span class="md-ellipsis">
      BlobFile
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BlobFile">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.BlobFile.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.BlobFile.size" class="md-nav__link">
    <span class="md-ellipsis">
      size
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.DataStatistics" class="md-nav__link">
    <span class="md-ellipsis">
      DataStatistics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.FieldStatistics" class="md-nav__link">
    <span class="md-ellipsis">
      FieldStatistics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.FragmentMetadata" class="md-nav__link">
    <span class="md-ellipsis">
      FragmentMetadata
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FragmentMetadata">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.FragmentMetadata--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.FragmentMetadata.num_deletions" class="md-nav__link">
    <span class="md-ellipsis">
      num_deletions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.FragmentMetadata.num_rows" class="md-nav__link">
    <span class="md-ellipsis">
      num_rows
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.FragmentMetadata.to_json" class="md-nav__link">
    <span class="md-ellipsis">
      to_json
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset" class="md-nav__link">
    <span class="md-ellipsis">
      LanceDataset
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LanceDataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.data_storage_version" class="md-nav__link">
    <span class="md-ellipsis">
      data_storage_version
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.lance_schema" class="md-nav__link">
    <span class="md-ellipsis">
      lance_schema
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.latest_version" class="md-nav__link">
    <span class="md-ellipsis">
      latest_version
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.max_field_id" class="md-nav__link">
    <span class="md-ellipsis">
      max_field_id
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.partition_expression" class="md-nav__link">
    <span class="md-ellipsis">
      partition_expression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.schema" class="md-nav__link">
    <span class="md-ellipsis">
      schema
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.stats" class="md-nav__link">
    <span class="md-ellipsis">
      stats
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.tags" class="md-nav__link">
    <span class="md-ellipsis">
      tags
    </span>
  </a>
  
    <nav class="md-nav" aria-label="tags">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.tags--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.uri" class="md-nav__link">
    <span class="md-ellipsis">
      uri
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.version" class="md-nav__link">
    <span class="md-ellipsis">
      version
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.add_columns" class="md-nav__link">
    <span class="md-ellipsis">
      add_columns
    </span>
  </a>
  
    <nav class="md-nav" aria-label="add_columns">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.add_columns--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.add_columns--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.add_columns--see-also" class="md-nav__link">
    <span class="md-ellipsis">
      See Also
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.alter_columns" class="md-nav__link">
    <span class="md-ellipsis">
      alter_columns
    </span>
  </a>
  
    <nav class="md-nav" aria-label="alter_columns">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.alter_columns--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.alter_columns--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.checkout_version" class="md-nav__link">
    <span class="md-ellipsis">
      checkout_version
    </span>
  </a>
  
    <nav class="md-nav" aria-label="checkout_version">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.checkout_version--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.checkout_version--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.cleanup_old_versions" class="md-nav__link">
    <span class="md-ellipsis">
      cleanup_old_versions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="cleanup_old_versions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.cleanup_old_versions--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.commit" class="md-nav__link">
    <span class="md-ellipsis">
      commit
    </span>
  </a>
  
    <nav class="md-nav" aria-label="commit">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.commit--warnings" class="md-nav__link">
    <span class="md-ellipsis">
      Warnings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.commit--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.commit--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.commit--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.commit_batch" class="md-nav__link">
    <span class="md-ellipsis">
      commit_batch
    </span>
  </a>
  
    <nav class="md-nav" aria-label="commit_batch">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.commit_batch--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.commit_batch--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.count_rows" class="md-nav__link">
    <span class="md-ellipsis">
      count_rows
    </span>
  </a>
  
    <nav class="md-nav" aria-label="count_rows">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.count_rows--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.count_rows--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.create_index" class="md-nav__link">
    <span class="md-ellipsis">
      create_index
    </span>
  </a>
  
    <nav class="md-nav" aria-label="create_index">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.create_index--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.create_index--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.create_index--references" class="md-nav__link">
    <span class="md-ellipsis">
      References
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.create_scalar_index" class="md-nav__link">
    <span class="md-ellipsis">
      create_scalar_index
    </span>
  </a>
  
    <nav class="md-nav" aria-label="create_scalar_index">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.create_scalar_index--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.create_scalar_index--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.delete" class="md-nav__link">
    <span class="md-ellipsis">
      delete
    </span>
  </a>
  
    <nav class="md-nav" aria-label="delete">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.delete--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.delete--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.drop_columns" class="md-nav__link">
    <span class="md-ellipsis">
      drop_columns
    </span>
  </a>
  
    <nav class="md-nav" aria-label="drop_columns">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.drop_columns--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.drop_columns--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.drop_index" class="md-nav__link">
    <span class="md-ellipsis">
      drop_index
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.get_fragment" class="md-nav__link">
    <span class="md-ellipsis">
      get_fragment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.get_fragments" class="md-nav__link">
    <span class="md-ellipsis">
      get_fragments
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.head" class="md-nav__link">
    <span class="md-ellipsis">
      head
    </span>
  </a>
  
    <nav class="md-nav" aria-label="head">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.head--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.head--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.insert" class="md-nav__link">
    <span class="md-ellipsis">
      insert
    </span>
  </a>
  
    <nav class="md-nav" aria-label="insert">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.insert--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.join" class="md-nav__link">
    <span class="md-ellipsis">
      join
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.merge" class="md-nav__link">
    <span class="md-ellipsis">
      merge
    </span>
  </a>
  
    <nav class="md-nav" aria-label="merge">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.merge--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.merge--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.merge--see-also" class="md-nav__link">
    <span class="md-ellipsis">
      See Also
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.merge_insert" class="md-nav__link">
    <span class="md-ellipsis">
      merge_insert
    </span>
  </a>
  
    <nav class="md-nav" aria-label="merge_insert">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.merge_insert--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.merge_insert--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.merge_insert--perform-a-upsert-operation" class="md-nav__link">
    <span class="md-ellipsis">
      Perform a "upsert" operation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.merge_insert--perform-an-insert-if-not-exists-operation" class="md-nav__link">
    <span class="md-ellipsis">
      Perform an "insert if not exists" operation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.merge_insert--perform-an-upsert-operation-only-updating-column-a" class="md-nav__link">
    <span class="md-ellipsis">
      Perform an "upsert" operation, only updating column "a"
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.migrate_manifest_paths_v2" class="md-nav__link">
    <span class="md-ellipsis">
      migrate_manifest_paths_v2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.prewarm_index" class="md-nav__link">
    <span class="md-ellipsis">
      prewarm_index
    </span>
  </a>
  
    <nav class="md-nav" aria-label="prewarm_index">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.prewarm_index--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.replace_field_metadata" class="md-nav__link">
    <span class="md-ellipsis">
      replace_field_metadata
    </span>
  </a>
  
    <nav class="md-nav" aria-label="replace_field_metadata">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.replace_field_metadata--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.replace_schema" class="md-nav__link">
    <span class="md-ellipsis">
      replace_schema
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.replace_schema_metadata" class="md-nav__link">
    <span class="md-ellipsis">
      replace_schema_metadata
    </span>
  </a>
  
    <nav class="md-nav" aria-label="replace_schema_metadata">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.replace_schema_metadata--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.restore" class="md-nav__link">
    <span class="md-ellipsis">
      restore
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.sample" class="md-nav__link">
    <span class="md-ellipsis">
      sample
    </span>
  </a>
  
    <nav class="md-nav" aria-label="sample">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.sample--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.sample--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.scanner" class="md-nav__link">
    <span class="md-ellipsis">
      scanner
    </span>
  </a>
  
    <nav class="md-nav" aria-label="scanner">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.scanner--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.session" class="md-nav__link">
    <span class="md-ellipsis">
      session
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.take" class="md-nav__link">
    <span class="md-ellipsis">
      take
    </span>
  </a>
  
    <nav class="md-nav" aria-label="take">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.take--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.take--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.take_blobs" class="md-nav__link">
    <span class="md-ellipsis">
      take_blobs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="take_blobs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.take_blobs--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.to_batches" class="md-nav__link">
    <span class="md-ellipsis">
      to_batches
    </span>
  </a>
  
    <nav class="md-nav" aria-label="to_batches">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.to_batches--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.to_batches--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.to_table" class="md-nav__link">
    <span class="md-ellipsis">
      to_table
    </span>
  </a>
  
    <nav class="md-nav" aria-label="to_table">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.to_table--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.to_table--notes" class="md-nav__link">
    <span class="md-ellipsis">
      Notes
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.update" class="md-nav__link">
    <span class="md-ellipsis">
      update
    </span>
  </a>
  
    <nav class="md-nav" aria-label="update">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.update--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.update--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.update--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.validate" class="md-nav__link">
    <span class="md-ellipsis">
      validate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceDataset.versions" class="md-nav__link">
    <span class="md-ellipsis">
      versions
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment" class="md-nav__link">
    <span class="md-ellipsis">
      LanceFragment
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LanceFragment">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.metadata" class="md-nav__link">
    <span class="md-ellipsis">
      metadata
    </span>
  </a>
  
    <nav class="md-nav" aria-label="metadata">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.metadata--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.num_deletions" class="md-nav__link">
    <span class="md-ellipsis">
      num_deletions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.physical_rows" class="md-nav__link">
    <span class="md-ellipsis">
      physical_rows
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.schema" class="md-nav__link">
    <span class="md-ellipsis">
      schema
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.create" class="md-nav__link">
    <span class="md-ellipsis">
      create
    </span>
  </a>
  
    <nav class="md-nav" aria-label="create">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.create--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.create--see-also" class="md-nav__link">
    <span class="md-ellipsis">
      See Also
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.create--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.create_from_file" class="md-nav__link">
    <span class="md-ellipsis">
      create_from_file
    </span>
  </a>
  
    <nav class="md-nav" aria-label="create_from_file">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.create_from_file--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.data_files" class="md-nav__link">
    <span class="md-ellipsis">
      data_files
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.delete" class="md-nav__link">
    <span class="md-ellipsis">
      delete
    </span>
  </a>
  
    <nav class="md-nav" aria-label="delete">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.delete--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.delete--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.delete--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.delete--see-also" class="md-nav__link">
    <span class="md-ellipsis">
      See Also
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.deletion_file" class="md-nav__link">
    <span class="md-ellipsis">
      deletion_file
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.merge" class="md-nav__link">
    <span class="md-ellipsis">
      merge
    </span>
  </a>
  
    <nav class="md-nav" aria-label="merge">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.merge--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.merge--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.merge--see-also" class="md-nav__link">
    <span class="md-ellipsis">
      See Also
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.merge--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.merge_columns" class="md-nav__link">
    <span class="md-ellipsis">
      merge_columns
    </span>
  </a>
  
    <nav class="md-nav" aria-label="merge_columns">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.merge_columns--see-also" class="md-nav__link">
    <span class="md-ellipsis">
      See Also
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.merge_columns--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceFragment.scanner" class="md-nav__link">
    <span class="md-ellipsis">
      scanner
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation" class="md-nav__link">
    <span class="md-ellipsis">
      LanceOperation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LanceOperation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Append" class="md-nav__link">
    <span class="md-ellipsis">
      Append
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Append">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Append--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Append--warning" class="md-nav__link">
    <span class="md-ellipsis">
      Warning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Append--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.BaseOperation" class="md-nav__link">
    <span class="md-ellipsis">
      BaseOperation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.CreateIndex" class="md-nav__link">
    <span class="md-ellipsis">
      CreateIndex
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.DataReplacement" class="md-nav__link">
    <span class="md-ellipsis">
      DataReplacement
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.DataReplacementGroup" class="md-nav__link">
    <span class="md-ellipsis">
      DataReplacementGroup
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Delete" class="md-nav__link">
    <span class="md-ellipsis">
      Delete
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Delete">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Delete--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Delete--warning" class="md-nav__link">
    <span class="md-ellipsis">
      Warning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Delete--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Merge" class="md-nav__link">
    <span class="md-ellipsis">
      Merge
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Merge">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Merge--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Merge--warning" class="md-nav__link">
    <span class="md-ellipsis">
      Warning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Merge--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Overwrite" class="md-nav__link">
    <span class="md-ellipsis">
      Overwrite
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Overwrite">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Overwrite--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Overwrite--warning" class="md-nav__link">
    <span class="md-ellipsis">
      Warning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Overwrite--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Project" class="md-nav__link">
    <span class="md-ellipsis">
      Project
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Project">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Project--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Project--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Project--rename-column-b-into-b0-and-rename-b1-into-b" class="md-nav__link">
    <span class="md-ellipsis">
      rename column lance &para; __version__ = &#39;0.27.3&#39; module &para; str(object='') -&gt; str str(bytes_or_buffer[, encoding[, errors]]) -&gt; str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object.str() (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. BlobColumn &para; A utility to wrap a Pyarrow binary column and iterate over the rows as file-like objects. This can be useful for working with medium-to-small binary objects that need to interface with APIs that expect file-like objects. For very large binary objects (4-8MB or more per value) you might be better off creating a blob column and using meth:lance.Dataset.take_blobs to access the blob data. BlobFile &para; Bases: RawIOBase Represents a blob in a Lance dataset as a file-like object. __init__(inner) &para; Internal only: To obtain a BlobFile use meth:lance.dataset.Dataset.take_blobs. size() &para; Returns the size of the blob in bytes. DataStatistics dataclass &para; Statistics about the data in the dataset FieldStatistics dataclass &para; Statistics about a field in the dataset FragmentMetadata dataclass &para; Metadata for a fragment. Attributes&para; id : int The ID of the fragment. files : List[DataFile] The data files of the fragment. Each data file must have the same number of rows. Each file stores a different subset of the columns. physical_rows : int The number of rows originally in this fragment. This is the number of rows in the data files before deletions. deletion_file : Optional[DeletionFile] The deletion file, if any. row_id_meta : Optional[RowIdMeta] The row id metadata, if any. num_deletions property &para; The number of rows that have been deleted from this fragment. num_rows property &para; The number of rows in this fragment after deletions. to_json() &para; Get this as a simple JSON-serializable dictionary. LanceDataset &para; Bases: Dataset A Lance Dataset in Lance format where the data is stored at the given uri. data_storage_version property &para; The version of the data storage format this dataset is using lance_schema property &para; The LanceSchema for this dataset latest_version property &para; Returns the latest version of the dataset. max_field_id property &para; The max_field_id in manifest partition_expression property &para; Not implemented (just override pyarrow dataset to prevent segfault) schema property &para; The pyarrow Schema for this dataset stats property &para; Experimental API tags property &para; Tag management for the dataset. Similar to Git, tags are a way to add metadata to a specific version of the dataset. .. warning:: Tagged versions are exempted from the :py:meth:`cleanup_old_versions()` process. To remove a version that has been tagged, you must first :py:meth:`~Tags.delete` the associated tag. Examples&para; .. code-block:: python ds = lance.open(&quot;dataset.lance&quot;) ds.tags.create(&quot;v2-prod-20250203&quot;, 10) tags = ds.tags.list() uri property &para; The location of the data version property &para; Returns the currently checked out version of the dataset add_columns(transforms, read_columns=None, reader_schema=None, batch_size=None) &para; Add new columns with defined values. There are several ways to specify the new columns. First, you can provide SQL expressions for each new column. Second you can provide a UDF that takes a batch of existing data and returns a new batch with the new columns. These new columns will be appended to the dataset. You can also provide a RecordBatchReader which will read the new column values from some external source. This is often useful when the new column values have already been staged to files (often by some distributed process) See the :func:lance.add_columns_udf decorator for more information on writing UDFs. Parameters&para; transforms : dict or AddColumnsUDF or ReaderLike If this is a dictionary, then the keys are the names of the new columns and the values are SQL expression strings. These strings can reference existing columns in the dataset. If this is a AddColumnsUDF, then it is a UDF that takes a batch of existing data and returns a new batch with the new columns. If this is :class:pyarrow.Field or :class:pyarrow.Schema, it adds all NULL columns with the given schema, in a metadata-only operation. read_columns : list of str, optional The names of the columns that the UDF will read. If None, then the UDF will read all columns. This is only used when transforms is a UDF. Otherwise, the read columns are inferred from the SQL expressions. reader_schema: pa.Schema, optional Only valid if transforms is a ReaderLike object. This will be used to determine the schema of the reader. batch_size: int, optional The number of rows to read at a time from the source dataset when applying the transform. This is ignored if the dataset is a v1 dataset. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3]}) dataset = lance.write_dataset(table, "my_dataset") @lance.batch_udf() ... def double_a(batch): ... df = batch.to_pandas() ... return pd.DataFrame({'double_a': 2 * df['a']}) dataset.add_columns(double_a) dataset.to_table().to_pandas() a double_a 0 1 2 1 2 4 2 3 6 dataset.add_columns({"triple_a": "a * 3"}) dataset.to_table().to_pandas() a double_a triple_a 0 1 2 3 1 2 4 6 2 3 6 9 See Also&para; LanceDataset.merge : Merge a pre-computed set of columns into the dataset. alter_columns(*alterations) &para; Alter column name, data type, and nullability. Columns that are renamed can keep any indices that are on them. If a column has an IVF_PQ index, it can be kept if the column is casted to another type. However, other index types don't support casting at this time. Column types can be upcasted (such as int32 to int64) or downcasted (such as int64 to int32). However, downcasting will fail if there are any values that cannot be represented in the new type. In general, columns can be casted to same general type: integers to integers, floats to floats, and strings to strings. However, strings, binary, and list columns can be casted between their size variants. For example, string to large string, binary to large binary, and list to large list. Columns that are renamed can keep any indices that are on them. However, if the column is casted to a different type, its indices will be dropped. Parameters&para; alterations : Iterable[Dict[str, Any]] A sequence of dictionaries, each with the following keys: - &quot;path&quot;: str The column path to alter. For a top-level column, this is the name. For a nested column, this is the dot-separated path, e.g. &quot;a.b.c&quot;. - &quot;name&quot;: str, optional The new name of the column. If not specified, the column name is not changed. - &quot;nullable&quot;: bool, optional Whether the column should be nullable. If not specified, the column nullability is not changed. Only non-nullable columns can be changed to nullable. Currently, you cannot change a nullable column to non-nullable. - &quot;data_type&quot;: pyarrow.DataType, optional The new data type to cast the column to. If not specified, the column data type is not changed. Examples&para; import lance import pyarrow as pa schema = pa.schema([pa.field('a', pa.int64()), ... pa.field('b', pa.string(), nullable=False)]) table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.alter_columns({"path": "a", "name": "x"}, ... {"path": "b", "nullable": True}) dataset.to_table().to_pandas() x b 0 1 a 1 2 b 2 3 c dataset.alter_columns({"path": "x", "data_type": pa.int32()}) dataset.schema x: int32 b: string checkout_version(version) &para; Load the given version of the dataset. Unlike the :func:dataset constructor, this will re-use the current cache. This is a no-op if the dataset is already at the given version. Parameters&para; version: int | str, The version to check out. A version number (int) or a tag (str) can be provided. Returns&para; LanceDataset cleanup_old_versions(older_than=None, *, delete_unverified=False, error_if_tagged_old_versions=True) &para; Cleans up old versions of the dataset. Some dataset changes, such as overwriting, leave behind data that is not referenced by the latest dataset version. The old data is left in place to allow the dataset to be restored back to an older version. This method will remove older versions and any data files they reference. Once this cleanup task has run you will not be able to checkout or restore these older versions. Parameters&para; timedelta, optional Only versions older than this will be removed. If not specified, this will default to two weeks. bool, default False Files leftover from a failed transaction may appear to be part of an in-progress operation (e.g. appending new data) and these files will not be deleted unless they are at least 7 days old. If delete_unverified is True then these files will be deleted regardless of their age. This should only be set to True if you can guarantee that no other process is currently working on this dataset. Otherwise the dataset could be put into a corrupted state. bool, default True Some versions may have tags associated with them. Tagged versions will not be cleaned up, regardless of how old they are. If this argument is set to True (the default), an exception will be raised if any tagged versions match the parameters. Otherwise, tagged versions will be ignored without any error and only untagged versions will be cleaned up. commit(base_uri, operation, blobs_op=None, read_version=None, commit_lock=None, storage_options=None, enable_v2_manifest_paths=None, detached=False, max_retries=20) staticmethod &para; Create a new version of dataset This method is an advanced method which allows users to describe a change that has been made to the data files. This method is not needed when using Lance to apply changes (e.g. when using class:LanceDataset or func:write_dataset.) It's current purpose is to allow for changes being made in a distributed environment where no single process is doing all of the work. For example, a distributed bulk update or a distributed bulk modify operation. Once all of the changes have been made, this method can be called to make the changes visible by updating the dataset manifest. Warnings&para; This is an advanced API and doesn't provide the same level of validation as the other APIs. For example, it's the responsibility of the caller to ensure that the fragments are valid for the schema. Parameters&para; base_uri: str, Path, or LanceDataset The base uri of the dataset, or the dataset object itself. Using the dataset object can be more efficient because it can re-use the file metadata cache. operation: BaseOperation The operation to apply to the dataset. This describes what changes have been made. See available operations under :class:LanceOperation. read_version: int, optional The version of the dataset that was used as the base for the changes. This is not needed for overwrite or restore operations. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:migrate_manifest_paths_v2 method. Default is False. WARNING: turning this on will make the dataset unreadable for older versions of Lance (prior to 0.17.0). detached : bool, optional If True, then the commit will not be part of the dataset lineage. It will never show up as the latest dataset and the only way to check it out in the future will be to specifically check it out by version. The version will be a random version that is only unique amongst detached commits. The caller should store this somewhere as there will be no other way to obtain it in the future. max_retries : int The maximum number of retries to perform when committing the dataset. Returns&para; LanceDataset A new version of Lance Dataset. Examples&para; Creating a new dataset with the :class:LanceOperation.Overwrite operation: import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment1 = lance.fragment.LanceFragment.create("example", tab1) fragment2 = lance.fragment.LanceFragment.create("example", tab2) fragments = [fragment1, fragment2] operation = lance.LanceOperation.Overwrite(tab1.schema, fragments) dataset = lance.LanceDataset.commit("example", operation) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d commit_batch(dest, transactions, commit_lock=None, storage_options=None, enable_v2_manifest_paths=None, detached=False, max_retries=20) staticmethod &para; Create a new version of dataset with multiple transactions. This method is an advanced method which allows users to describe a change that has been made to the data files. This method is not needed when using Lance to apply changes (e.g. when using class:LanceDataset or func:write_dataset.) Parameters&para; dest: str, Path, or LanceDataset The base uri of the dataset, or the dataset object itself. Using the dataset object can be more efficient because it can re-use the file metadata cache. transactions: Iterable[Transaction] The transactions to apply to the dataset. These will be merged into a single transaction and applied to the dataset. Note: Only append transactions are currently supported. Other transaction types will be supported in the future. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:migrate_manifest_paths_v2 method. Default is False. WARNING: turning this on will make the dataset unreadable for older versions of Lance (prior to 0.17.0). detached : bool, optional If True, then the commit will not be part of the dataset lineage. It will never show up as the latest dataset and the only way to check it out in the future will be to specifically check it out by version. The version will be a random version that is only unique amongst detached commits. The caller should store this somewhere as there will be no other way to obtain it in the future. max_retries : int The maximum number of retries to perform when committing the dataset. Returns&para; dict with keys: dataset: LanceDataset A new version of Lance Dataset. merged: Transaction The merged transaction that was applied to the dataset. count_rows(filter=None, **kwargs) &para; Count rows matching the scanner filter. Parameters&para; **kwargs : dict, optional See py:method:scanner method for full parameter description. Returns&para; count : int The total number of rows in the dataset. create_index(column, index_type, name=None, metric=&#39;L2&#39;, replace=False, num_partitions=None, ivf_centroids=None, pq_codebook=None, num_sub_vectors=None, accelerator=None, index_cache_size=None, shuffle_partition_batches=None, shuffle_partition_concurrency=None, ivf_centroids_file=None, precomputed_partition_dataset=None, storage_options=None, filter_nan=True, one_pass_ivfpq=False, **kwargs) &para; Create index on column. Experimental API Parameters&para; column : str The column to be indexed. index_type : str The type of the index. "IVF_PQ, IVF_HNSW_PQ and IVF_HNSW_SQ" are supported now. name : str, optional The index name. If not provided, it will be generated from the column name. metric : str The distance metric type, i.e., "L2" (alias to "euclidean"), "cosine" or "dot" (dot product). Default is "L2". replace : bool Replace the existing index if it exists. num_partitions : int, optional The number of partitions of IVF (Inverted File Index). ivf_centroids : optional It can be either class:np.ndarray, class:pyarrow.FixedSizeListArray or class:pyarrow.FixedShapeTensorArray. A num_partitions x dimension array of existing K-mean centroids for IVF clustering. If not provided, a new KMeans model will be trained. pq_codebook : optional, It can be class:np.ndarray, class:pyarrow.FixedSizeListArray, or class:pyarrow.FixedShapeTensorArray. A num_sub_vectors x (2 ^ nbits * dimensions // num_sub_vectors) array of K-mean centroids for PQ codebook. Note: ``nbits`` is always 8 for now. If not provided, a new PQ model will be trained. num_sub_vectors : int, optional The number of sub-vectors for PQ (Product Quantization). accelerator : str or torch.Device, optional If set, use an accelerator to speed up the training process. Accepted accelerator: "cuda" (Nvidia GPU) and "mps" (Apple Silicon GPU). If not set, use the CPU. index_cache_size : int, optional The size of the index cache in number of entries. Default value is 256. shuffle_partition_batches : int, optional The number of batches, using the row group size of the dataset, to include in each shuffle partition. Default value is 10240. Assuming the row group size is 1024, each shuffle partition will hold 10240 * 1024 = 10,485,760 rows. By making this value smaller, this shuffle will consume less memory but will take longer to complete, and vice versa. shuffle_partition_concurrency : int, optional The number of shuffle partitions to process concurrently. Default value is 2 By making this value smaller, this shuffle will consume less memory but will take longer to complete, and vice versa. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. filter_nan: bool Defaults to True. False is UNSAFE, and will cause a crash if any null/nan values are present (and otherwise will not). Disables the null filter used for nullable columns. Obtains a small speed boost. one_pass_ivfpq: bool Defaults to False. If enabled, index type must be "IVF_PQ". Reduces disk IO. kwargs : Parameters passed to the index building process. The SQ (Scalar Quantization) is available for only IVF_HNSW_SQ index type, this quantization method is used to reduce the memory usage of the index, it maps the float vectors to integer vectors, each integer is of num_bits, now only 8 bits are supported. If index_type is "IVF_*", then the following parameters are required: num_partitions If index_type is with "PQ", then the following parameters are required: num_sub_vectors Optional parameters for IVF_PQ: - ivf_centroids Existing K-mean centroids for IVF clustering. - num_bits The number of bits for PQ (Product Quantization). Default is 8. Only 4, 8 are supported. Optional parameters for IVF_HNSW_*: max_level Int, the maximum number of levels in the graph. m Int, the number of edges per node in the graph. ef_construction Int, the number of nodes to examine during the construction. Examples&para; .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_PQ&quot;, num_partitions=256, num_sub_vectors=16 ) .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_HNSW_SQ&quot;, num_partitions=256, ) Experimental Accelerator (GPU) support: accelerate: use GPU to train IVF partitions. Only supports CUDA (Nvidia) or MPS (Apple) currently. Requires PyTorch being installed. .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_PQ&quot;, num_partitions=256, num_sub_vectors=16, accelerator=&quot;cuda&quot; ) References&para; Faiss Index &lt;https://github.com/facebookresearch/faiss/wiki/Faiss-indexes&gt;_ IVF introduced in Video Google: a text retrieval approach to object matching in videos &lt;https://ieeexplore.ieee.org/abstract/document/1238663&gt;_ Product quantization for nearest neighbor search &lt;https://hal.inria.fr/inria-00514462v2/document&gt;_ create_scalar_index(column, index_type, name=None, *, replace=True, **kwargs) &para; Create a scalar index on a column. Scalar indices, like vector indices, can be used to speed up scans. A scalar index can speed up scans that contain filter expressions on the indexed column. For example, the following scan will be faster if the column my_col has a scalar index: .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) my_table = dataset.scanner(filter=&quot;my_col != 7&quot;).to_table() Vector search with pre-filers can also benefit from scalar indices. For example, .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) my_table = dataset.scanner( nearest=dict( column=&quot;vector&quot;, q=[1, 2, 3, 4], k=10, ) filter=&quot;my_col != 7&quot;, prefilter=True ) There are 5 types of scalar indices available today. BTREE. The most common type is BTREE. This index is inspired by the btree data structure although only the first few layers of the btree are cached in memory. It will perform well on columns with a large number of unique values and few rows per value. BITMAP. This index stores a bitmap for each unique value in the column. This index is useful for columns with a small number of unique values and many rows per value. LABEL_LIST. A special index that is used to index list columns whose values have small cardinality. For example, a column that contains lists of tags (e.g. ["tag1", "tag2", "tag3"]) can be indexed with a LABEL_LIST index. This index can only speedup queries with array_has_any or array_has_all filters. NGRAM. A special index that is used to index string columns. This index creates a bitmap for each ngram in the string. By default we use trigrams. This index can currently speed up queries using the contains function in filters. FTS/INVERTED. It is used to index document columns. This index can conduct full-text searches. For example, a column that contains any word of query string "hello world". The results will be ranked by BM25. Note that the LANCE_BYPASS_SPILLING environment variable can be used to bypass spilling to disk. Setting this to true can avoid memory exhaustion issues (see https://github.com/apache/datafusion/issues/10073 for more info). Experimental API Parameters&para; column : str The column to be indexed. Must be a boolean, integer, float, or string column. index_type : str The type of the index. One of "BTREE", "BITMAP", "LABEL_LIST", "NGRAM", "FTS" or "INVERTED". name : str, optional The index name. If not provided, it will be generated from the column name. replace : bool, default True Replace the existing index if it exists. bool, default True This is for the INVERTED index. If True, the index will store the positions of the words in the document, so that you can conduct phrase query. This will significantly increase the index size. It won't impact the performance of non-phrase queries even if it is set to True. base_tokenizer: str, default "simple" This is for the INVERTED index. The base tokenizer to use. The value can be: * "simple": splits tokens on whitespace and punctuation. * "whitespace": splits tokens on whitespace. * "raw": no tokenization. language: str, default "English" This is for the INVERTED index. The language for stemming and stop words. This is only used when stem or remove_stop_words is true max_token_length: Optional[int], default 40 This is for the INVERTED index. The maximum token length. Any token longer than this will be removed. lower_case: bool, default True This is for the INVERTED index. If True, the index will convert all text to lowercase. stem: bool, default False This is for the INVERTED index. If True, the index will stem the tokens. remove_stop_words: bool, default False This is for the INVERTED index. If True, the index will remove stop words. ascii_folding: bool, default False This is for the INVERTED index. If True, the index will convert non-ascii characters to ascii characters if possible. This would remove accents like "é" -&gt; "e". Examples&para; .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) dataset.create_index( &quot;category&quot;, &quot;BTREE&quot;, ) Scalar indices can only speed up scans for basic filters using equality, comparison, range (e.g. my_col BETWEEN 0 AND 100), and set membership (e.g. my_col IN (0, 1, 2)) Scalar indices can be used if the filter contains multiple indexed columns and the filter criteria are AND'd or OR'd together (e.g. my_col &lt; 0 AND other_col&gt; 100) Scalar indices may be used if the filter contains non-indexed columns but, depending on the structure of the filter, they may not be usable. For example, if the column not_indexed does not have a scalar index then the filter my_col = 0 OR not_indexed = 1 will not be able to use any scalar index on my_col. To determine if a scan is making use of a scalar index you can use explain_plan to look at the query plan that lance has created. Queries that use scalar indices will either have a ScalarIndexQuery relation or a MaterializeIndex operator. delete(predicate) &para; Delete rows from the dataset. This marks rows as deleted, but does not physically remove them from the files. This keeps the existing indexes still valid. Parameters&para; predicate : str or pa.compute.Expression The predicate to use to select rows to delete. May either be a SQL string or a pyarrow Expression. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.delete("a = 1 or b in ('a', 'b')") dataset.to_table() pyarrow.Table a: int64 b: string a: [[3]] b: [["c"]] drop_columns(columns) &para; Drop one or more columns from the dataset This is a metadata-only operation and does not remove the data from the underlying storage. In order to remove the data, you must subsequently call compact_files to rewrite the data without the removed columns and then call cleanup_old_versions to remove the old files. Parameters&para; columns : list of str The names of the columns to drop. These can be nested column references (e.g. "a.b.c") or top-level column names (e.g. "a"). Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.drop_columns(["a"]) dataset.to_table().to_pandas() b 0 a 1 b 2 c drop_index(name) &para; Drops an index from the dataset Note: Indices are dropped by "index name". This is not the same as the field name. If you did not specify a name when you created the index then a name was generated for you. You can use the list_indices method to get the names of the indices. get_fragment(fragment_id) &para; Get the fragment with fragment id. get_fragments(filter=None) &para; Get all fragments from the dataset. Note: filter is not supported yet. head(num_rows, **kwargs) &para; Load the first N rows of the dataset. Parameters&para; num_rows : int The number of rows to load. **kwargs : dict, optional See scanner() method for full parameter description. Returns&para; table : Table insert(data, *, mode=&#39;append&#39;, **kwargs) &para; Insert data into the dataset. Parameters&para; data_obj: Reader-like The data to be written. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader - Huggingface dataset mode: str, default 'append' The mode to use when writing the data. Options are: create - create a new dataset (raises if uri already exists). overwrite - create a new snapshot version append - create a new version that is the concat of the input the latest version (raises if uri does not exist) **kwargs : dict, optional Additional keyword arguments to pass to :func:write_dataset. join(right_dataset, keys, right_keys=None, join_type=&#39;left outer&#39;, left_suffix=None, right_suffix=None, coalesce_keys=True, use_threads=True) &para; Not implemented (just override pyarrow dataset to prevent segfault) merge(data_obj, left_on, right_on=None, schema=None) &para; Merge another dataset into this one. Performs a left join, where the dataset is the left side and data_obj is the right side. Rows existing in the dataset but not on the left will be filled with null values, unless Lance doesn't support null values for some types, in which case an error will be raised. Parameters&para; data_obj: Reader-like The data to be merged. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, Iterator[RecordBatch], or RecordBatchReader left_on: str The name of the column in the dataset to join on. right_on: str or None The name of the column in data_obj to join on. If None, defaults to left_on. Examples&para; import lance import pyarrow as pa df = pa.table({'x': [1, 2, 3], 'y': ['a', 'b', 'c']}) dataset = lance.write_dataset(df, "dataset") dataset.to_table().to_pandas() x y 0 1 a 1 2 b 2 3 c new_df = pa.table({'x': [1, 2, 3], 'z': ['d', 'e', 'f']}) dataset.merge(new_df, 'x') dataset.to_table().to_pandas() x y z 0 1 a d 1 2 b e 2 3 c f See Also&para; LanceDataset.add_columns : Add new columns by computing batch-by-batch. merge_insert(on) &para; Returns a builder that can be used to create a "merge insert" operation This operation can add rows, update rows, and remove rows in a single transaction. It is a very generic tool that can be used to create behaviors like "insert if not exists", "update or insert (i.e. upsert)", or even replace a portion of existing data with new data (e.g. replace all data where month="january") The merge insert operation works by combining new data from a source table with existing data in a target table by using a join. There are three categories of records. "Matched" records are records that exist in both the source table and the target table. "Not matched" records exist only in the source table (e.g. these are new data). "Not matched by source" records exist only in the target table (this is old data). The builder returned by this method can be used to customize what should happen for each category of data. Please note that the data will be reordered as part of this operation. This is because updated rows will be deleted from the dataset and then reinserted at the end with the new values. The order of the newly inserted rows may fluctuate randomly because a hash-join operation is used internally. Parameters&para; Union[str, Iterable[str]] A column (or columns) to join on. This is how records from the source table and target table are matched. Typically this is some kind of key or id column. Examples&para; Use when_matched_update_all() and when_not_matched_insert_all() to perform an "upsert" operation. This will update rows that already exist in the dataset and insert rows that do not exist. import lance import pyarrow as pa table = pa.table({"a": [2, 1, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform a "upsert" operation&para; dataset.merge_insert("a") \ ... .when_matched_update_all() \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b 0 1 b 1 2 x 2 3 y 3 4 z Use when_not_matched_insert_all() to perform an "insert if not exists" operation. This will only insert rows that do not already exist in the dataset. import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example2") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform an "insert if not exists" operation&para; dataset.merge_insert("a") \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 0, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 z You are not required to provide all the columns. If you only want to update a subset of columns, you can omit columns you don't want to update. Omitted columns will keep their existing values if they are updated, or will be null if they are inserted. import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"], \ ... "c": ["x", "y", "z"]}) dataset = lance.write_dataset(table, "example3") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform an "upsert" operation, only updating column "a"&para; dataset.merge_insert("a") \ ... .when_matched_update_all() \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b c 0 1 a x 1 2 x y 2 3 y z 3 4 z None migrate_manifest_paths_v2() &para; Migrate the manifest paths to the new format. This will update the manifest to use the new v2 format for paths. This function is idempotent, and can be run multiple times without changing the state of the object store. DANGER: this should not be run while other concurrent operations are happening. And it should also run until completion before resuming other operations. prewarm_index(name) &para; Prewarm an index This will load the entire index into memory. This can help avoid cold start issues with index queries. If the index does not fit in the index cache, then this will result in wasted I/O. Parameters&para; name: str The name of the index to prewarm. replace_field_metadata(field_name, new_metadata) &para; Replace the metadata of a field in the schema Parameters&para; field_name: str The name of the field to replace the metadata for new_metadata: dict The new metadata to set replace_schema(schema) &para; Not implemented (just override pyarrow dataset to prevent segfault) See method:replace_schema_metadata or method:replace_field_metadata replace_schema_metadata(new_metadata) &para; Replace the schema metadata of the dataset Parameters&para; new_metadata: dict The new metadata to set restore() &para; Restore the currently checked out version as the latest version of the dataset. This creates a new commit. sample(num_rows, columns=None, randomize_order=True, **kwargs) &para; Select a random sample of data Parameters&para; num_rows: int number of rows to retrieve columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. **kwargs : dict, optional see scanner() method for full parameter description. Returns&para; table : Table scanner(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, fragments=None, full_text_query=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, fast_search=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, include_deleted_rows=None, scan_stats_callback=None, strict_batch_size=None) &para; Return a Scanner that can support various pushdowns. Parameters&para; columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. filter: pa.compute.Expression or str Expression or str that is a valid SQL where clause. See Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;_ for valid SQL expressions. limit: int, default None Fetch up to this many rows. All rows if None or unspecified. offset: int, default None Fetch starting with this row. 0 if None or unspecified. nearest: dict, default None Get the rows corresponding to the K most similar vectors. Example: .. code-block:: python { &quot;column&quot;: &lt;embedding col name&gt;, &quot;q&quot;: &lt;query vector as pa.Float32Array&gt;, &quot;k&quot;: 10, &quot;nprobes&quot;: 1, &quot;refine_factor&quot;: 1 } int, default None The target size of batches returned. In some cases batches can be up to twice this size (but never larger than this). In some cases batches can be smaller than this size. io_buffer_size: int, default None The size of the IO buffer. See ScannerBuilder.io_buffer_size for more information. batch_readahead: int, optional The number of batches to read ahead. fragment_readahead: int, optional The number of fragments to read ahead. scan_in_order: bool, default True Whether to read the fragments and batches in order. If false, throughput may be higher, but batches will be returned out of order and memory use might increase. fragments: iterable of LanceFragment, default None If specified, only scan these fragments. If scan_in_order is True, then the fragments will be scanned in the order given. prefilter: bool, default False If True then the filter will be applied before the vector query is run. This will generate more correct results but it may be a more costly query. It's generally good when the filter is highly selective. If False then the filter will be applied after the vector query is run. This will perform well but the results may have fewer than the requested number of rows (or be empty) if the rows closest to the query do not match the filter. It&#39;s generally good when the filter is not very selective. use_scalar_index: bool, default True Lance will automatically use scalar indices to optimize a query. In some corner cases this can make query performance worse and this parameter can be used to disable scalar indices in these cases. late_materialization: bool or List[str], default None Allows custom control over late materialization. Late materialization fetches non-query columns using a take operation after the filter. This is useful when there are few results or columns are very large. Early materialization can be better when there are many results or the columns are very narrow. If True, then all columns are late materialized. If False, then all columns are early materialized. If a list of strings, then only the columns in the list are late materialized. The default uses a heuristic that assumes filters will select about 0.1% of the rows. If your filter is more selective (e.g. find by id) you may want to set this to True. If your filter is not very selective (e.g. matches 20% of the rows) you may want to set this to False. full_text_query: str or dict, optional query string to search for, the results will be ranked by BM25. e.g. "hello world", would match documents containing "hello" or "world". or a dictionary with the following keys: - columns: list[str] The columns to search, currently only supports a single column in the columns list. - query: str The query string to search for. fast_search: bool, default False If True, then the search will only be performed on the indexed data, which yields faster search time. scan_stats_callback: Callable[[ScanStatistics], None], default None A callback function that will be called with the scan statistics after the scan is complete. Errors raised by the callback will be logged but not re-raised. include_deleted_rows: bool, default False If True, then rows that have been deleted, but are still present in the fragment, will be returned. These rows will have the _rowid column set to null. All other columns will reflect the value stored on disk and may not be null. Note: if this is a search operation, or a take operation (including scalar indexed scans) then deleted rows cannot be returned. .. note:: For now, if BOTH filter and nearest is specified, then: 1. nearest is executed first. 2. The results are filtered afterwards. For debugging ANN results, you can choose to not use the index even if present by specifying use_index=False. For example, the following will always return exact KNN results: .. code-block:: python dataset.to_table(nearest={ &quot;column&quot;: &quot;vector&quot;, &quot;k&quot;: 10, &quot;q&quot;: &lt;query vector&gt;, &quot;use_index&quot;: False } session() &para; Return the dataset session, which holds the dataset's state. take(indices, columns=None) &para; Select rows of data by index. Parameters&para; indices : Array or array-like indices of rows to select in the dataset. columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. Returns&para; table : pyarrow.Table take_blobs(blob_column, ids=None, addresses=None, indices=None) &para; Select blobs by row IDs. Instead of loading large binary blob data into memory before processing it, this API allows you to open binary blob data as a regular Python file-like object. For more details, see class:lance.BlobFile. Exactly one of ids, addresses, or indices must be specified. Parameters blob_column : str The name of the blob column to select. ids : Integer Array or array-like row IDs to select in the dataset. addresses: Integer Array or array-like The (unstable) row addresses to select in the dataset. indices : Integer Array or array-like The offset / indices of the row in the dataset. Returns&para; blob_files : List[BlobFile] to_batches(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, full_text_query=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, strict_batch_size=None, **kwargs) &para; Read the dataset as materialized record batches. Parameters&para; **kwargs : dict, optional Arguments for meth:~LanceDataset.scanner. Returns&para; record_batches : Iterator of class:~pyarrow.RecordBatch to_table(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, fast_search=None, full_text_query=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, include_deleted_rows=None) &para; Read the data into memory as a class:pyarrow.Table Parameters&para; columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. filter : pa.compute.Expression or str Expression or str that is a valid SQL where clause. See Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;_ for valid SQL expressions. limit: int, default None Fetch up to this many rows. All rows if None or unspecified. offset: int, default None Fetch starting with this row. 0 if None or unspecified. nearest: dict, default None Get the rows corresponding to the K most similar vectors. Example: .. code-block:: python { &quot;column&quot;: &lt;embedding col name&gt;, &quot;q&quot;: &lt;query vector as pa.Float32Array&gt;, &quot;k&quot;: 10, &quot;metric&quot;: &quot;cosine&quot;, &quot;nprobes&quot;: 1, &quot;refine_factor&quot;: 1 } int, optional The number of rows to read at a time. io_buffer_size: int, default None The size of the IO buffer. See ScannerBuilder.io_buffer_size for more information. batch_readahead: int, optional The number of batches to read ahead. fragment_readahead: int, optional The number of fragments to read ahead. scan_in_order: bool, optional, default True Whether to read the fragments and batches in order. If false, throughput may be higher, but batches will be returned out of order and memory use might increase. prefilter: bool, optional, default False Run filter before the vector search. late_materialization: bool or List[str], default None Allows custom control over late materialization. See ScannerBuilder.late_materialization for more information. use_scalar_index: bool, default True Allows custom control over scalar index usage. See ScannerBuilder.use_scalar_index for more information. with_row_id: bool, optional, default False Return row ID. with_row_address: bool, optional, default False Return row address use_stats: bool, optional, default True Use stats pushdown during filters. fast_search: bool, optional, default False full_text_query: str or dict, optional query string to search for, the results will be ranked by BM25. e.g. "hello world", would match documents contains "hello" or "world". or a dictionary with the following keys: - columns: list[str] The columns to search, currently only supports a single column in the columns list. - query: str The query string to search for. include_deleted_rows: bool, optional, default False If True, then rows that have been deleted, but are still present in the fragment, will be returned. These rows will have the _rowid column set to null. All other columns will reflect the value stored on disk and may not be null. Note: if this is a search operation, or a take operation (including scalar indexed scans) then deleted rows cannot be returned. Notes&para; If BOTH filter and nearest is specified, then: nearest is executed first. The results are filtered afterward, unless pre-filter sets to True. update(updates, where=None) &para; Update column values for rows matching where. Parameters&para; updates : dict of str to str A mapping of column names to a SQL expression. where : str, optional A SQL predicate indicating which rows should be updated. Returns&para; updates : dict A dictionary containing the number of rows updated. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") update_stats = dataset.update(dict(a = 'a + 2'), where="b != 'a'") update_stats["num_updated_rows"] = 2 dataset.to_table().to_pandas() a b 0 1 a 1 4 b 2 5 c validate() &para; Validate the dataset. This checks the integrity of the dataset and will raise an exception if the dataset is corrupted. versions() &para; Return all versions in this dataset. LanceFragment &para; Bases: Fragment metadata property &para; Return the metadata of this fragment. Returns&para; FragmentMetadata num_deletions property &para; Return the number of deleted rows in this fragment. physical_rows property &para; Return the number of rows originally in this fragment. To get the number of rows after deletions, use :meth:count_rows instead. schema property &para; Return the schema of this fragment. create(dataset_uri, data, fragment_id=None, schema=None, max_rows_per_group=1024, progress=None, mode=&#39;append&#39;, *, data_storage_version=None, use_legacy_format=None, storage_options=None) staticmethod &para; Create a :class:FragmentMetadata from the given data. This can be used if the dataset is not yet created. .. warning:: Internal API. This method is not intended to be used by end users. Parameters&para; dataset_uri: str The URI of the dataset. fragment_id: int The ID of the fragment. data: pa.Table or pa.RecordBatchReader The data to be written to the fragment. schema: pa.Schema, optional The schema of the data. If not specified, the schema will be inferred from the data. max_rows_per_group: int, default 1024 The maximum number of rows per group in the data file. progress: FragmentWriteProgress, optional Experimental API. Progress tracking for writing the fragment. Pass a custom class that defines hooks to be called when each fragment is starting to write and finishing writing. mode: str, default "append" The write mode. If "append" is specified, the data will be checked against the existing dataset's schema. Otherwise, pass "create" or "overwrite" to assign new field ids to the schema. data_storage_version: optional, str, default None The version of the data storage format to use. Newer versions are more efficient but require newer versions of lance to read. The default (None) will use the latest stable version. See the user guide for more details. use_legacy_format: bool, default None Deprecated parameter. Use data_storage_version instead. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. See Also&para; lance.dataset.LanceOperation.Overwrite : The operation used to create a new dataset or overwrite one using fragments created with this API. See the doc page for an example of using this API. lance.dataset.LanceOperation.Append : The operation used to append fragments created with this API to an existing dataset. See the doc page for an example of using this API. Returns&para; FragmentMetadata create_from_file(filename, dataset, fragment_id) staticmethod &para; Create a fragment from the given datafile uri. This can be used if the datafile is loss from dataset. .. warning:: Internal API. This method is not intended to be used by end users. Parameters&para; filename: str The filename of the datafile. dataset: LanceDataset The dataset that the fragment belongs to. fragment_id: int The ID of the fragment. data_files() &para; Return the data files of this fragment. delete(predicate) &para; Delete rows from this Fragment. This will add or update the deletion file of this fragment. It does not modify or delete the data files of this fragment. If no rows are left after the deletion, this method will return None. .. warning:: Internal API. This method is not intended to be used by end users. Parameters&para; predicate: str A SQL predicate that specifies the rows to delete. Returns&para; FragmentMetadata or None A new fragment containing the new deletion file, or None if no rows left. Examples&para; import lance import pyarrow as pa tab = pa.table({"a": [1, 2, 3], "b": [4, 5, 6]}) dataset = lance.write_dataset(tab, "dataset") frag = dataset.get_fragment(0) frag.delete("a &gt; 1") FragmentMetadata(id=0, files=[DataFile(path='...', fields=[0, 1], ...), ...) frag.delete("a &gt; 0") is None True See Also&para; lance.dataset.LanceOperation.Delete : The operation used to commit these changes to a dataset. See the doc page for an example of using this API. deletion_file() &para; Return the deletion file, if any merge(data_obj, left_on, right_on=None, schema=None) &para; Merge another dataset into this fragment. Performs a left join, where the fragment is the left side and data_obj is the right side. Rows existing in the dataset but not on the left will be filled with null values, unless Lance doesn't support null values for some types, in which case an error will be raised. Parameters&para; data_obj: Reader-like The data to be merged. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, Iterator[RecordBatch], or RecordBatchReader left_on: str The name of the column in the dataset to join on. right_on: str or None The name of the column in data_obj to join on. If None, defaults to left_on. Examples&para; import lance import pyarrow as pa df = pa.table({'x': [1, 2, 3], 'y': ['a', 'b', 'c']}) dataset = lance.write_dataset(df, "dataset") dataset.to_table().to_pandas() x y 0 1 a 1 2 b 2 3 c fragments = dataset.get_fragments() new_df = pa.table({'x': [1, 2, 3], 'z': ['d', 'e', 'f']}) merged = [] schema = None for f in fragments: ... f, schema = f.merge(new_df, 'x') ... merged.append(f) merge = lance.LanceOperation.Merge(merged, schema) dataset = lance.LanceDataset.commit("dataset", merge, read_version=1) dataset.to_table().to_pandas() x y z 0 1 a d 1 2 b e 2 3 c f See Also&para; LanceDataset.merge_columns : Add columns to this Fragment. Returns&para; Tuple[FragmentMetadata, LanceSchema] A new fragment with the merged column(s) and the final schema. merge_columns(value_func, columns=None, batch_size=None, reader_schema=None) &para; Add columns to this Fragment. .. warning:: Internal API. This method is not intended to be used by end users. The parameters and their interpretation are the same as in the :meth:lance.dataset.LanceDataset.add_columns operation. The only difference is that, instead of modifying the dataset, a new fragment is created. The new schema of the fragment is returned as well. These can be used in a later operation to commit the changes to the dataset. See Also&para; lance.dataset.LanceOperation.Merge : The operation used to commit these changes to the dataset. See the doc page for an example of using this API. Returns&para; Tuple[FragmentMetadata, LanceSchema] A new fragment with the added column(s) and the final schema. scanner(*, columns=None, batch_size=None, filter=None, limit=None, offset=None, with_row_id=False, with_row_address=False, batch_readahead=16) &para; See Dataset::scanner for details LanceOperation &para; Append dataclass &para; Bases: BaseOperation Append new rows to the dataset. Attributes&para; fragments: list[FragmentMetadata] The fragments that contain the new rows. Warning&para; This is an advanced API for distributed operations. To append to a dataset on a single machine, use :func:lance.write_dataset. Examples&para; To append new rows to a dataset, first use :meth:lance.fragment.LanceFragment.create to create fragments. Then collect the fragment metadata into a list and pass it to this class. Finally, pass the operation to the :meth:LanceDataset.commit method to create the new dataset. import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) dataset = lance.write_dataset(tab1, "example") tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment = lance.fragment.LanceFragment.create("example", tab2) operation = lance.LanceOperation.Append([fragment]) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d BaseOperation &para; Bases: ABC Base class for operations that can be applied to a dataset. See available operations under :class:LanceOperation. CreateIndex dataclass &para; Bases: BaseOperation Operation that creates an index on the dataset. DataReplacement dataclass &para; Bases: BaseOperation Operation that replaces existing datafiles in the dataset. DataReplacementGroup dataclass &para; Group of data replacements Delete dataclass &para; Bases: BaseOperation Remove fragments or rows from the dataset. Attributes&para; updated_fragments: list[FragmentMetadata] The fragments that have been updated with new deletion vectors. deleted_fragment_ids: list[int] The ids of the fragments that have been deleted entirely. These are the fragments where :meth:LanceFragment.delete() returned None. predicate: str The original SQL predicate used to select the rows to delete. Warning&para; This is an advanced API for distributed operations. To delete rows from dataset on a single machine, use :meth:lance.LanceDataset.delete. Examples&para; To delete rows from a dataset, call :meth:lance.fragment.LanceFragment.delete on each of the fragments. If that returns a new fragment, add that to the updated_fragments list. If it returns None, that means the whole fragment was deleted, so add the fragment id to the deleted_fragment_ids. Finally, pass the operation to the :meth:LanceDataset.commit method to complete the deletion operation. import lance import pyarrow as pa table = pa.table({"a": [1, 2], "b": ["a", "b"]}) dataset = lance.write_dataset(table, "example") table = pa.table({"a": [3, 4], "b": ["c", "d"]}) dataset = lance.write_dataset(table, "example", mode="append") dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d predicate = "a &gt;= 2" updated_fragments = [] deleted_fragment_ids = [] for fragment in dataset.get_fragments(): ... new_fragment = fragment.delete(predicate) ... if new_fragment is not None: ... updated_fragments.append(new_fragment) ... else: ... deleted_fragment_ids.append(fragment.fragment_id) operation = lance.LanceOperation.Delete(updated_fragments, ... deleted_fragment_ids, ... predicate) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b 0 1 a Merge dataclass &para; Bases: BaseOperation Operation that adds columns. Unlike Overwrite, this should not change the structure of the fragments, allowing existing indices to be kept. Attributes&para; fragments: iterable of FragmentMetadata The fragments that make up the new dataset. schema: LanceSchema or pyarrow.Schema The schema of the new dataset. Passing a LanceSchema is preferred, and passing a pyarrow.Schema is deprecated. Warning&para; This is an advanced API for distributed operations. To overwrite or create new dataset on a single machine, use :func:lance.write_dataset. Examples&para; To add new columns to a dataset, first define a method that will create the new columns based on the existing columns. Then use :meth:lance.fragment.LanceFragment.add_columns import lance import pyarrow as pa import pyarrow.compute as pc table = pa.table({"a": [1, 2, 3, 4], "b": ["a", "b", "c", "d"]}) dataset = lance.write_dataset(table, "example") dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d def double_a(batch: pa.RecordBatch) -&gt; pa.RecordBatch: ... doubled = pc.multiply(batch["a"], 2) ... return pa.record_batch([doubled], ["a_doubled"]) fragments = [] for fragment in dataset.get_fragments(): ... new_fragment, new_schema = fragment.merge_columns(double_a, ... columns=['a']) ... fragments.append(new_fragment) operation = lance.LanceOperation.Merge(fragments, new_schema) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b a_doubled 0 1 a 2 1 2 b 4 2 3 c 6 3 4 d 8 Overwrite dataclass &para; Bases: BaseOperation Overwrite or create a new dataset. Attributes&para; new_schema: pyarrow.Schema The schema of the new dataset. fragments: list[FragmentMetadata] The fragments that make up the new dataset. Warning&para; This is an advanced API for distributed operations. To overwrite or create new dataset on a single machine, use :func:lance.write_dataset. Examples&para; To create or overwrite a dataset, first use :meth:lance.fragment.LanceFragment.create to create fragments. Then collect the fragment metadata into a list and pass it along with the schema to this class. Finally, pass the operation to the :meth:LanceDataset.commit method to create the new dataset. import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment1 = lance.fragment.LanceFragment.create("example", tab1) fragment2 = lance.fragment.LanceFragment.create("example", tab2) fragments = [fragment1, fragment2] operation = lance.LanceOperation.Overwrite(tab1.schema, fragments) dataset = lance.LanceDataset.commit("example", operation) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d Project dataclass &para; Bases: BaseOperation Operation that project columns. Use this operator for drop column or rename/swap column. Attributes&para; schema: LanceSchema The lance schema of the new dataset. Examples&para; Use the projece operator to swap column: import lance import pyarrow as pa import pyarrow.compute as pc from lance.schema import LanceSchema table = pa.table({"a": [1, 2], "b": ["a", "b"], "b1": ["c", "d"]}) dataset = lance.write_dataset(table, "example") dataset.to_table().to_pandas() a b b1 0 1 a c 1 2 b d rename column b into b0 and rename b1 into b&para; table = pa.table({"a": [3, 4], "b0": ["a", "b"], "b": ["c", "d"]}) lance_schema = LanceSchema.from_pyarrow(table.schema) operation = lance.LanceOperation.Project(lance_schema) dataset = lance.LanceDataset.commit("example", operation, read_version=1) dataset.to_table().to_pandas() a b0 b 0 1 a c 1 2 b d Restore dataclass &para; Bases: BaseOperation Operation that restores a previous version of the dataset. Rewrite dataclass &para; Bases: BaseOperation Operation that rewrites one or more files and indices into one or more files and indices. Attributes&para; groups: list[RewriteGroup] Groups of files that have been rewritten. rewritten_indices: list[RewrittenIndex] Indices that have been rewritten. Warning&para; This is an advanced API not intended for general use. RewriteGroup dataclass &para; Collection of rewritten files RewrittenIndex dataclass &para; An index that has been rewritten Update dataclass &para; Bases: BaseOperation Operation that updates rows in the dataset. Attributes&para; removed_fragment_ids: list[int] The ids of the fragments that have been removed entirely. updated_fragments: list[FragmentMetadata] The fragments that have been updated with new deletion vectors. new_fragments: list[FragmentMetadata] The fragments that contain the new rows. LanceScanner &para; Bases: Scanner dataset_schema property &para; The schema with which batches will be read from fragments. analyze_plan() &para; Execute the plan for this scanner and display with runtime metrics. Parameters&para; verbose : bool, default False Use a verbose output format. Returns&para; plan : str count_rows() &para; Count rows matching the scanner filter. Returns&para; count : int explain_plan(verbose=False) &para; Return the execution plan for this scanner. Parameters&para; verbose : bool, default False Use a verbose output format. Returns&para; plan : str from_batches(*args, **kwargs) staticmethod &para; Not implemented from_dataset(*args, **kwargs) staticmethod &para; Not implemented from_fragment(*args, **kwargs) staticmethod &para; Not implemented head(num_rows) &para; Load the first N rows of the dataset. Parameters&para; num_rows : int The number of rows to load. Returns&para; Table scan_batches() &para; Consume a Scanner in record batches with corresponding fragments. Returns&para; record_batches : iterator of TaggedRecordBatch take(indices) &para; Not implemented to_table() &para; Read the data into memory and return a pyarrow Table. MergeInsertBuilder &para; Bases: _MergeInsertBuilder conflict_retries(max_retries) &para; Set number of times to retry the operation if there is contention. If this is set &gt; 0, then the operation will keep a copy of the input data either in memory or on disk (depending on the size of the data) and will retry the operation if there is contention. Default is 10. execute(data_obj, *, schema=None) &para; Executes the merge insert operation This function updates the original dataset and returns a dictionary with information about merge statistics - i.e. the number of inserted, updated, and deleted rows. Parameters&para; ReaderLike The new data to use as the source table for the operation. This parameter can be any source of data (e.g. table / dataset) that :func:~lance.write_dataset accepts. schema: Optional[pa.Schema] The schema of the data. This only needs to be supplied whenever the data source is some kind of generator. execute_uncommitted(data_obj, *, schema=None) &para; Executes the merge insert operation without committing This function updates the original dataset and returns a dictionary with information about merge statistics - i.e. the number of inserted, updated, and deleted rows. Parameters&para; ReaderLike The new data to use as the source table for the operation. This parameter can be any source of data (e.g. table / dataset) that :func:~lance.write_dataset accepts. schema: Optional[pa.Schema] The schema of the data. This only needs to be supplied whenever the data source is some kind of generator. retry_timeout(timeout) &para; Set the timeout used to limit retries. This is the maximum time to spend on the operation before giving up. At least one attempt will be made, regardless of how long it takes to complete. Subsequent attempts will be cancelled once this timeout is reached. If the timeout has been reached during the first attempt, the operation will be cancelled immediately before making a second attempt. The default is 30 seconds. when_matched_update_all(condition=None) &para; Configure the operation to update matched rows After this method is called, when the merge insert operation executes, any rows that match both the source table and the target table will be updated. The rows from the target table will be removed and the rows from the source table will be added. An optional condition may be specified. This should be an SQL filter and, if present, then only matched rows that also satisfy this filter will be updated. The SQL filter should use the prefix target. to refer to columns in the target table and the prefix source. to refer to columns in the source table. For example, source.last_update &lt; target.last_update. If a condition is specified and rows do not satisfy the condition then these rows will not be updated. Failure to satisfy the filter does not cause a "matched" row to become a "not matched" row. when_not_matched_by_source_delete(expr=None) &para; Configure the operation to delete source rows that do not match After this method is called, when the merge insert operation executes, any rows that exist only in the target table will be deleted. An optional filter can be specified to limit the scope of the delete operation. If given (as an SQL filter) then only rows which match the filter will be deleted. when_not_matched_insert_all() &para; Configure the operation to insert not matched rows After this method is called, when the merge insert operation executes, any rows that exist only in the source table will be inserted into the target table. batch_udf(output_schema=None, checkpoint_file=None) &para; Create a user defined function (UDF) that adds columns to a dataset. This function is used to add columns to a dataset. It takes a function that takes a single argument, a RecordBatch, and returns a RecordBatch. The function is called once for each batch in the dataset. The function should not modify the input batch, but instead create a new batch with the new columns added. Parameters&para; output_schema : Schema, optional The schema of the output RecordBatch. This is used to validate the output of the function. If not provided, the schema of the first output RecordBatch will be used. checkpoint_file : str or Path, optional If specified, this file will be used as a cache for unsaved results of this UDF. If the process fails, and you call add_columns again with this same file, it will resume from the last saved state. This is useful for long running processes that may fail and need to be resumed. This file may get very large. It will hold up to an entire data files' worth of results on disk, which can be multiple gigabytes of data. Returns&para; AddColumnsUDF json_to_schema(schema_json) &para; Converts a JSON string to a PyArrow schema. Parameters&para; schema_json: Dict[str, Any] The JSON payload to convert to a PyArrow Schema. schema_to_json(schema) &para; Converts a pyarrow schema to a JSON string. Parameters&para; write_dataset(data_obj, uri, schema=None, mode=&#39;create&#39;, *, max_rows_per_file=1024 * 1024, max_rows_per_group=1024, max_bytes_per_file=90 * 1024 * 1024 * 1024, commit_lock=None, progress=None, storage_options=None, data_storage_version=None, use_legacy_format=None, enable_v2_manifest_paths=False, enable_move_stable_row_ids=False) &para; Write a given data_obj to the given uri Parameters&para; data_obj: Reader-like The data to be written. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader - Huggingface dataset uri: str, Path, or LanceDataset Where to write the dataset to (directory). If a LanceDataset is passed, the session will be reused. schema: Schema, optional If specified and the input is a pandas DataFrame, use this schema instead of the default pandas to arrow table conversion. mode: str create - create a new dataset (raises if uri already exists). overwrite - create a new snapshot version append - create a new version that is the concat of the input the latest version (raises if uri does not exist) max_rows_per_file: int, default 1024 * 1024 The max number of rows to write before starting a new file max_rows_per_group: int, default 1024 The max number of rows before starting a new group (in the same file) max_bytes_per_file: int, default 90 * 1024 * 1024 * 1024 The max number of bytes to write before starting a new file. This is a soft limit. This limit is checked after each group is written, which means larger groups may cause this to be overshot meaningfully. This defaults to 90 GB, since we have a hard limit of 100 GB per file on object stores. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. progress: FragmentWriteProgress, optional Experimental API. Progress tracking for writing the fragment. Pass a custom class that defines hooks to be called when each fragment is starting to write and finishing writing. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. data_storage_version: optional, str, default None The version of the data storage format to use. Newer versions are more efficient but require newer versions of lance to read. The default (None) will use the latest stable version. See the user guide for more details. use_legacy_format : optional, bool, default None Deprecated method for setting the data storage version. Use the data_storage_version parameter instead. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:LanceDataset.migrate_manifest_paths_v2 method. Default is False. enable_move_stable_row_ids : bool, optional Experimental parameter: if set to true, the writer will use move-stable row ids. These row ids are stable after compaction operations, but not after updates. This makes compaction more efficient, since with stable row ids no secondary indices need to be updated to point to new row ids. into lance.dataset &para; DataStatistics dataclass &para; Statistics about the data in the dataset DatasetOptimizer &para; compact_files(*, target_rows_per_fragment=1024 * 1024, max_rows_per_group=1024, max_bytes_per_file=None, materialize_deletions=True, materialize_deletions_threshold=0.1, num_threads=None, batch_size=None) &para; Compacts small files in the dataset, reducing total number of files. This does a few things Removes deleted rows from fragments Removes dropped columns from fragments Merges small fragments into larger ones This method preserves the insertion order of the dataset. This may mean it leaves small fragments in the dataset if they are not adjacent to other fragments that need compaction. For example, if you have fragments with row counts 5 million, 100, and 5 million, the middle fragment will not be compacted because the fragments it is adjacent to do not need compaction. Parameters&para; target_rows_per_fragment: int, default 1024*1024 The target number of rows per fragment. This is the number of rows that will be in each fragment after compaction. max_rows_per_group: int, default 1024 Max number of rows per group. This does not affect which fragments need compaction, but does affect how they are re-written if selected. This setting only affects datasets using the legacy storage format. The newer format does not require row groups. max_bytes_per_file: Optional[int], default None Max number of bytes in a single file. This does not affect which fragments need compaction, but does affect how they are re-written if selected. If this value is too small you may end up with fragments that are smaller than target_rows_per_fragment. The default will use the default from ``write_dataset``. materialize_deletions: bool, default True Whether to compact fragments with soft deleted rows so they are no longer present in the file. materialize_deletions_threshold: float, default 0.1 The fraction of original rows that are soft deleted in a fragment before the fragment is a candidate for compaction. num_threads: int, optional The number of threads to use when performing compaction. If not specified, defaults to the number of cores on the machine. batch_size: int, optional The batch size to use when scanning input fragments. You may want to reduce this if you are running out of memory during compaction. The default will use the same default from ``scanner``. Returns&para; CompactionMetrics Metrics about the compaction process See Also&para; lance.optimize.Compaction optimize_indices(**kwargs) &para; Optimizes index performance. As new data arrives it is not added to existing indexes automatically. When searching we need to perform an indexed search of the old data plus an expensive unindexed search on the new data. As the amount of new unindexed data grows this can have an impact on search latency. This function will add the new data to existing indexes, restoring the performance. This function does not retrain the index, it only assigns the new data to existing partitions. This means an update is much quicker than retraining the entire index but may have less accuracy (especially if the new data exhibits new patterns, concepts, or trends) Parameters&para; num_indices_to_merge: int, default 1 The number of indices to merge. If set to 0, new delta index will be created. index_names: List[str], default None The names of the indices to optimize. If None, all indices will be optimized. retrain: bool, default False Whether to retrain the whole index. If true, the index will be retrained based on the current data, num_indices_to_merge will be ignored, and all indices will be merged into one. This is useful when the data distribution has changed significantly, and we want to retrain the index to improve the search quality. This would be faster than re-create the index from scratch. FieldStatistics dataclass &para; Statistics about a field in the dataset LanceDataset &para; Bases: Dataset A Lance Dataset in Lance format where the data is stored at the given uri. data_storage_version property &para; The version of the data storage format this dataset is using lance_schema property &para; The LanceSchema for this dataset latest_version property &para; Returns the latest version of the dataset. max_field_id property &para; The max_field_id in manifest partition_expression property &para; Not implemented (just override pyarrow dataset to prevent segfault) schema property &para; The pyarrow Schema for this dataset stats property &para; Experimental API tags property &para; Tag management for the dataset. Similar to Git, tags are a way to add metadata to a specific version of the dataset. .. warning:: Tagged versions are exempted from the :py:meth:`cleanup_old_versions()` process. To remove a version that has been tagged, you must first :py:meth:`~Tags.delete` the associated tag. Examples&para; .. code-block:: python ds = lance.open(&quot;dataset.lance&quot;) ds.tags.create(&quot;v2-prod-20250203&quot;, 10) tags = ds.tags.list() uri property &para; The location of the data version property &para; Returns the currently checked out version of the dataset add_columns(transforms, read_columns=None, reader_schema=None, batch_size=None) &para; Add new columns with defined values. There are several ways to specify the new columns. First, you can provide SQL expressions for each new column. Second you can provide a UDF that takes a batch of existing data and returns a new batch with the new columns. These new columns will be appended to the dataset. You can also provide a RecordBatchReader which will read the new column values from some external source. This is often useful when the new column values have already been staged to files (often by some distributed process) See the :func:lance.add_columns_udf decorator for more information on writing UDFs. Parameters&para; transforms : dict or AddColumnsUDF or ReaderLike If this is a dictionary, then the keys are the names of the new columns and the values are SQL expression strings. These strings can reference existing columns in the dataset. If this is a AddColumnsUDF, then it is a UDF that takes a batch of existing data and returns a new batch with the new columns. If this is :class:pyarrow.Field or :class:pyarrow.Schema, it adds all NULL columns with the given schema, in a metadata-only operation. read_columns : list of str, optional The names of the columns that the UDF will read. If None, then the UDF will read all columns. This is only used when transforms is a UDF. Otherwise, the read columns are inferred from the SQL expressions. reader_schema: pa.Schema, optional Only valid if transforms is a ReaderLike object. This will be used to determine the schema of the reader. batch_size: int, optional The number of rows to read at a time from the source dataset when applying the transform. This is ignored if the dataset is a v1 dataset. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3]}) dataset = lance.write_dataset(table, "my_dataset") @lance.batch_udf() ... def double_a(batch): ... df = batch.to_pandas() ... return pd.DataFrame({'double_a': 2 * df['a']}) dataset.add_columns(double_a) dataset.to_table().to_pandas() a double_a 0 1 2 1 2 4 2 3 6 dataset.add_columns({"triple_a": "a * 3"}) dataset.to_table().to_pandas() a double_a triple_a 0 1 2 3 1 2 4 6 2 3 6 9 See Also&para; LanceDataset.merge : Merge a pre-computed set of columns into the dataset. alter_columns(*alterations) &para; Alter column name, data type, and nullability. Columns that are renamed can keep any indices that are on them. If a column has an IVF_PQ index, it can be kept if the column is casted to another type. However, other index types don't support casting at this time. Column types can be upcasted (such as int32 to int64) or downcasted (such as int64 to int32). However, downcasting will fail if there are any values that cannot be represented in the new type. In general, columns can be casted to same general type: integers to integers, floats to floats, and strings to strings. However, strings, binary, and list columns can be casted between their size variants. For example, string to large string, binary to large binary, and list to large list. Columns that are renamed can keep any indices that are on them. However, if the column is casted to a different type, its indices will be dropped. Parameters&para; alterations : Iterable[Dict[str, Any]] A sequence of dictionaries, each with the following keys: - &quot;path&quot;: str The column path to alter. For a top-level column, this is the name. For a nested column, this is the dot-separated path, e.g. &quot;a.b.c&quot;. - &quot;name&quot;: str, optional The new name of the column. If not specified, the column name is not changed. - &quot;nullable&quot;: bool, optional Whether the column should be nullable. If not specified, the column nullability is not changed. Only non-nullable columns can be changed to nullable. Currently, you cannot change a nullable column to non-nullable. - &quot;data_type&quot;: pyarrow.DataType, optional The new data type to cast the column to. If not specified, the column data type is not changed. Examples&para; import lance import pyarrow as pa schema = pa.schema([pa.field('a', pa.int64()), ... pa.field('b', pa.string(), nullable=False)]) table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.alter_columns({"path": "a", "name": "x"}, ... {"path": "b", "nullable": True}) dataset.to_table().to_pandas() x b 0 1 a 1 2 b 2 3 c dataset.alter_columns({"path": "x", "data_type": pa.int32()}) dataset.schema x: int32 b: string checkout_version(version) &para; Load the given version of the dataset. Unlike the :func:dataset constructor, this will re-use the current cache. This is a no-op if the dataset is already at the given version. Parameters&para; version: int | str, The version to check out. A version number (int) or a tag (str) can be provided. Returns&para; LanceDataset cleanup_old_versions(older_than=None, *, delete_unverified=False, error_if_tagged_old_versions=True) &para; Cleans up old versions of the dataset. Some dataset changes, such as overwriting, leave behind data that is not referenced by the latest dataset version. The old data is left in place to allow the dataset to be restored back to an older version. This method will remove older versions and any data files they reference. Once this cleanup task has run you will not be able to checkout or restore these older versions. Parameters&para; timedelta, optional Only versions older than this will be removed. If not specified, this will default to two weeks. bool, default False Files leftover from a failed transaction may appear to be part of an in-progress operation (e.g. appending new data) and these files will not be deleted unless they are at least 7 days old. If delete_unverified is True then these files will be deleted regardless of their age. This should only be set to True if you can guarantee that no other process is currently working on this dataset. Otherwise the dataset could be put into a corrupted state. bool, default True Some versions may have tags associated with them. Tagged versions will not be cleaned up, regardless of how old they are. If this argument is set to True (the default), an exception will be raised if any tagged versions match the parameters. Otherwise, tagged versions will be ignored without any error and only untagged versions will be cleaned up. commit(base_uri, operation, blobs_op=None, read_version=None, commit_lock=None, storage_options=None, enable_v2_manifest_paths=None, detached=False, max_retries=20) staticmethod &para; Create a new version of dataset This method is an advanced method which allows users to describe a change that has been made to the data files. This method is not needed when using Lance to apply changes (e.g. when using class:LanceDataset or func:write_dataset.) It's current purpose is to allow for changes being made in a distributed environment where no single process is doing all of the work. For example, a distributed bulk update or a distributed bulk modify operation. Once all of the changes have been made, this method can be called to make the changes visible by updating the dataset manifest. Warnings&para; This is an advanced API and doesn't provide the same level of validation as the other APIs. For example, it's the responsibility of the caller to ensure that the fragments are valid for the schema. Parameters&para; base_uri: str, Path, or LanceDataset The base uri of the dataset, or the dataset object itself. Using the dataset object can be more efficient because it can re-use the file metadata cache. operation: BaseOperation The operation to apply to the dataset. This describes what changes have been made. See available operations under :class:LanceOperation. read_version: int, optional The version of the dataset that was used as the base for the changes. This is not needed for overwrite or restore operations. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:migrate_manifest_paths_v2 method. Default is False. WARNING: turning this on will make the dataset unreadable for older versions of Lance (prior to 0.17.0). detached : bool, optional If True, then the commit will not be part of the dataset lineage. It will never show up as the latest dataset and the only way to check it out in the future will be to specifically check it out by version. The version will be a random version that is only unique amongst detached commits. The caller should store this somewhere as there will be no other way to obtain it in the future. max_retries : int The maximum number of retries to perform when committing the dataset. Returns&para; LanceDataset A new version of Lance Dataset. Examples&para; Creating a new dataset with the :class:LanceOperation.Overwrite operation: import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment1 = lance.fragment.LanceFragment.create("example", tab1) fragment2 = lance.fragment.LanceFragment.create("example", tab2) fragments = [fragment1, fragment2] operation = lance.LanceOperation.Overwrite(tab1.schema, fragments) dataset = lance.LanceDataset.commit("example", operation) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d commit_batch(dest, transactions, commit_lock=None, storage_options=None, enable_v2_manifest_paths=None, detached=False, max_retries=20) staticmethod &para; Create a new version of dataset with multiple transactions. This method is an advanced method which allows users to describe a change that has been made to the data files. This method is not needed when using Lance to apply changes (e.g. when using class:LanceDataset or func:write_dataset.) Parameters&para; dest: str, Path, or LanceDataset The base uri of the dataset, or the dataset object itself. Using the dataset object can be more efficient because it can re-use the file metadata cache. transactions: Iterable[Transaction] The transactions to apply to the dataset. These will be merged into a single transaction and applied to the dataset. Note: Only append transactions are currently supported. Other transaction types will be supported in the future. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:migrate_manifest_paths_v2 method. Default is False. WARNING: turning this on will make the dataset unreadable for older versions of Lance (prior to 0.17.0). detached : bool, optional If True, then the commit will not be part of the dataset lineage. It will never show up as the latest dataset and the only way to check it out in the future will be to specifically check it out by version. The version will be a random version that is only unique amongst detached commits. The caller should store this somewhere as there will be no other way to obtain it in the future. max_retries : int The maximum number of retries to perform when committing the dataset. Returns&para; dict with keys: dataset: LanceDataset A new version of Lance Dataset. merged: Transaction The merged transaction that was applied to the dataset. count_rows(filter=None, **kwargs) &para; Count rows matching the scanner filter. Parameters&para; **kwargs : dict, optional See py:method:scanner method for full parameter description. Returns&para; count : int The total number of rows in the dataset. create_index(column, index_type, name=None, metric=&#39;L2&#39;, replace=False, num_partitions=None, ivf_centroids=None, pq_codebook=None, num_sub_vectors=None, accelerator=None, index_cache_size=None, shuffle_partition_batches=None, shuffle_partition_concurrency=None, ivf_centroids_file=None, precomputed_partition_dataset=None, storage_options=None, filter_nan=True, one_pass_ivfpq=False, **kwargs) &para; Create index on column. Experimental API Parameters&para; column : str The column to be indexed. index_type : str The type of the index. "IVF_PQ, IVF_HNSW_PQ and IVF_HNSW_SQ" are supported now. name : str, optional The index name. If not provided, it will be generated from the column name. metric : str The distance metric type, i.e., "L2" (alias to "euclidean"), "cosine" or "dot" (dot product). Default is "L2". replace : bool Replace the existing index if it exists. num_partitions : int, optional The number of partitions of IVF (Inverted File Index). ivf_centroids : optional It can be either class:np.ndarray, class:pyarrow.FixedSizeListArray or class:pyarrow.FixedShapeTensorArray. A num_partitions x dimension array of existing K-mean centroids for IVF clustering. If not provided, a new KMeans model will be trained. pq_codebook : optional, It can be class:np.ndarray, class:pyarrow.FixedSizeListArray, or class:pyarrow.FixedShapeTensorArray. A num_sub_vectors x (2 ^ nbits * dimensions // num_sub_vectors) array of K-mean centroids for PQ codebook. Note: ``nbits`` is always 8 for now. If not provided, a new PQ model will be trained. num_sub_vectors : int, optional The number of sub-vectors for PQ (Product Quantization). accelerator : str or torch.Device, optional If set, use an accelerator to speed up the training process. Accepted accelerator: "cuda" (Nvidia GPU) and "mps" (Apple Silicon GPU). If not set, use the CPU. index_cache_size : int, optional The size of the index cache in number of entries. Default value is 256. shuffle_partition_batches : int, optional The number of batches, using the row group size of the dataset, to include in each shuffle partition. Default value is 10240. Assuming the row group size is 1024, each shuffle partition will hold 10240 * 1024 = 10,485,760 rows. By making this value smaller, this shuffle will consume less memory but will take longer to complete, and vice versa. shuffle_partition_concurrency : int, optional The number of shuffle partitions to process concurrently. Default value is 2 By making this value smaller, this shuffle will consume less memory but will take longer to complete, and vice versa. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. filter_nan: bool Defaults to True. False is UNSAFE, and will cause a crash if any null/nan values are present (and otherwise will not). Disables the null filter used for nullable columns. Obtains a small speed boost. one_pass_ivfpq: bool Defaults to False. If enabled, index type must be "IVF_PQ". Reduces disk IO. kwargs : Parameters passed to the index building process. The SQ (Scalar Quantization) is available for only IVF_HNSW_SQ index type, this quantization method is used to reduce the memory usage of the index, it maps the float vectors to integer vectors, each integer is of num_bits, now only 8 bits are supported. If index_type is "IVF_*", then the following parameters are required: num_partitions If index_type is with "PQ", then the following parameters are required: num_sub_vectors Optional parameters for IVF_PQ: - ivf_centroids Existing K-mean centroids for IVF clustering. - num_bits The number of bits for PQ (Product Quantization). Default is 8. Only 4, 8 are supported. Optional parameters for IVF_HNSW_*: max_level Int, the maximum number of levels in the graph. m Int, the number of edges per node in the graph. ef_construction Int, the number of nodes to examine during the construction. Examples&para; .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_PQ&quot;, num_partitions=256, num_sub_vectors=16 ) .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_HNSW_SQ&quot;, num_partitions=256, ) Experimental Accelerator (GPU) support: accelerate: use GPU to train IVF partitions. Only supports CUDA (Nvidia) or MPS (Apple) currently. Requires PyTorch being installed. .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_PQ&quot;, num_partitions=256, num_sub_vectors=16, accelerator=&quot;cuda&quot; ) References&para; Faiss Index &lt;https://github.com/facebookresearch/faiss/wiki/Faiss-indexes&gt;_ IVF introduced in Video Google: a text retrieval approach to object matching in videos &lt;https://ieeexplore.ieee.org/abstract/document/1238663&gt;_ Product quantization for nearest neighbor search &lt;https://hal.inria.fr/inria-00514462v2/document&gt;_ create_scalar_index(column, index_type, name=None, *, replace=True, **kwargs) &para; Create a scalar index on a column. Scalar indices, like vector indices, can be used to speed up scans. A scalar index can speed up scans that contain filter expressions on the indexed column. For example, the following scan will be faster if the column my_col has a scalar index: .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) my_table = dataset.scanner(filter=&quot;my_col != 7&quot;).to_table() Vector search with pre-filers can also benefit from scalar indices. For example, .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) my_table = dataset.scanner( nearest=dict( column=&quot;vector&quot;, q=[1, 2, 3, 4], k=10, ) filter=&quot;my_col != 7&quot;, prefilter=True ) There are 5 types of scalar indices available today. BTREE. The most common type is BTREE. This index is inspired by the btree data structure although only the first few layers of the btree are cached in memory. It will perform well on columns with a large number of unique values and few rows per value. BITMAP. This index stores a bitmap for each unique value in the column. This index is useful for columns with a small number of unique values and many rows per value. LABEL_LIST. A special index that is used to index list columns whose values have small cardinality. For example, a column that contains lists of tags (e.g. ["tag1", "tag2", "tag3"]) can be indexed with a LABEL_LIST index. This index can only speedup queries with array_has_any or array_has_all filters. NGRAM. A special index that is used to index string columns. This index creates a bitmap for each ngram in the string. By default we use trigrams. This index can currently speed up queries using the contains function in filters. FTS/INVERTED. It is used to index document columns. This index can conduct full-text searches. For example, a column that contains any word of query string "hello world". The results will be ranked by BM25. Note that the LANCE_BYPASS_SPILLING environment variable can be used to bypass spilling to disk. Setting this to true can avoid memory exhaustion issues (see https://github.com/apache/datafusion/issues/10073 for more info). Experimental API Parameters&para; column : str The column to be indexed. Must be a boolean, integer, float, or string column. index_type : str The type of the index. One of "BTREE", "BITMAP", "LABEL_LIST", "NGRAM", "FTS" or "INVERTED". name : str, optional The index name. If not provided, it will be generated from the column name. replace : bool, default True Replace the existing index if it exists. bool, default True This is for the INVERTED index. If True, the index will store the positions of the words in the document, so that you can conduct phrase query. This will significantly increase the index size. It won't impact the performance of non-phrase queries even if it is set to True. base_tokenizer: str, default "simple" This is for the INVERTED index. The base tokenizer to use. The value can be: * "simple": splits tokens on whitespace and punctuation. * "whitespace": splits tokens on whitespace. * "raw": no tokenization. language: str, default "English" This is for the INVERTED index. The language for stemming and stop words. This is only used when stem or remove_stop_words is true max_token_length: Optional[int], default 40 This is for the INVERTED index. The maximum token length. Any token longer than this will be removed. lower_case: bool, default True This is for the INVERTED index. If True, the index will convert all text to lowercase. stem: bool, default False This is for the INVERTED index. If True, the index will stem the tokens. remove_stop_words: bool, default False This is for the INVERTED index. If True, the index will remove stop words. ascii_folding: bool, default False This is for the INVERTED index. If True, the index will convert non-ascii characters to ascii characters if possible. This would remove accents like "é" -&gt; "e". Examples&para; .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) dataset.create_index( &quot;category&quot;, &quot;BTREE&quot;, ) Scalar indices can only speed up scans for basic filters using equality, comparison, range (e.g. my_col BETWEEN 0 AND 100), and set membership (e.g. my_col IN (0, 1, 2)) Scalar indices can be used if the filter contains multiple indexed columns and the filter criteria are AND'd or OR'd together (e.g. my_col &lt; 0 AND other_col&gt; 100) Scalar indices may be used if the filter contains non-indexed columns but, depending on the structure of the filter, they may not be usable. For example, if the column not_indexed does not have a scalar index then the filter my_col = 0 OR not_indexed = 1 will not be able to use any scalar index on my_col. To determine if a scan is making use of a scalar index you can use explain_plan to look at the query plan that lance has created. Queries that use scalar indices will either have a ScalarIndexQuery relation or a MaterializeIndex operator. delete(predicate) &para; Delete rows from the dataset. This marks rows as deleted, but does not physically remove them from the files. This keeps the existing indexes still valid. Parameters&para; predicate : str or pa.compute.Expression The predicate to use to select rows to delete. May either be a SQL string or a pyarrow Expression. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.delete("a = 1 or b in ('a', 'b')") dataset.to_table() pyarrow.Table a: int64 b: string a: [[3]] b: [["c"]] drop_columns(columns) &para; Drop one or more columns from the dataset This is a metadata-only operation and does not remove the data from the underlying storage. In order to remove the data, you must subsequently call compact_files to rewrite the data without the removed columns and then call cleanup_old_versions to remove the old files. Parameters&para; columns : list of str The names of the columns to drop. These can be nested column references (e.g. "a.b.c") or top-level column names (e.g. "a"). Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.drop_columns(["a"]) dataset.to_table().to_pandas() b 0 a 1 b 2 c drop_index(name) &para; Drops an index from the dataset Note: Indices are dropped by "index name". This is not the same as the field name. If you did not specify a name when you created the index then a name was generated for you. You can use the list_indices method to get the names of the indices. get_fragment(fragment_id) &para; Get the fragment with fragment id. get_fragments(filter=None) &para; Get all fragments from the dataset. Note: filter is not supported yet. head(num_rows, **kwargs) &para; Load the first N rows of the dataset. Parameters&para; num_rows : int The number of rows to load. **kwargs : dict, optional See scanner() method for full parameter description. Returns&para; table : Table insert(data, *, mode=&#39;append&#39;, **kwargs) &para; Insert data into the dataset. Parameters&para; data_obj: Reader-like The data to be written. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader - Huggingface dataset mode: str, default 'append' The mode to use when writing the data. Options are: create - create a new dataset (raises if uri already exists). overwrite - create a new snapshot version append - create a new version that is the concat of the input the latest version (raises if uri does not exist) **kwargs : dict, optional Additional keyword arguments to pass to :func:write_dataset. join(right_dataset, keys, right_keys=None, join_type=&#39;left outer&#39;, left_suffix=None, right_suffix=None, coalesce_keys=True, use_threads=True) &para; Not implemented (just override pyarrow dataset to prevent segfault) merge(data_obj, left_on, right_on=None, schema=None) &para; Merge another dataset into this one. Performs a left join, where the dataset is the left side and data_obj is the right side. Rows existing in the dataset but not on the left will be filled with null values, unless Lance doesn't support null values for some types, in which case an error will be raised. Parameters&para; data_obj: Reader-like The data to be merged. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, Iterator[RecordBatch], or RecordBatchReader left_on: str The name of the column in the dataset to join on. right_on: str or None The name of the column in data_obj to join on. If None, defaults to left_on. Examples&para; import lance import pyarrow as pa df = pa.table({'x': [1, 2, 3], 'y': ['a', 'b', 'c']}) dataset = lance.write_dataset(df, "dataset") dataset.to_table().to_pandas() x y 0 1 a 1 2 b 2 3 c new_df = pa.table({'x': [1, 2, 3], 'z': ['d', 'e', 'f']}) dataset.merge(new_df, 'x') dataset.to_table().to_pandas() x y z 0 1 a d 1 2 b e 2 3 c f See Also&para; LanceDataset.add_columns : Add new columns by computing batch-by-batch. merge_insert(on) &para; Returns a builder that can be used to create a "merge insert" operation This operation can add rows, update rows, and remove rows in a single transaction. It is a very generic tool that can be used to create behaviors like "insert if not exists", "update or insert (i.e. upsert)", or even replace a portion of existing data with new data (e.g. replace all data where month="january") The merge insert operation works by combining new data from a source table with existing data in a target table by using a join. There are three categories of records. "Matched" records are records that exist in both the source table and the target table. "Not matched" records exist only in the source table (e.g. these are new data). "Not matched by source" records exist only in the target table (this is old data). The builder returned by this method can be used to customize what should happen for each category of data. Please note that the data will be reordered as part of this operation. This is because updated rows will be deleted from the dataset and then reinserted at the end with the new values. The order of the newly inserted rows may fluctuate randomly because a hash-join operation is used internally. Parameters&para; Union[str, Iterable[str]] A column (or columns) to join on. This is how records from the source table and target table are matched. Typically this is some kind of key or id column. Examples&para; Use when_matched_update_all() and when_not_matched_insert_all() to perform an "upsert" operation. This will update rows that already exist in the dataset and insert rows that do not exist. import lance import pyarrow as pa table = pa.table({"a": [2, 1, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform a "upsert" operation&para; dataset.merge_insert("a") \ ... .when_matched_update_all() \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b 0 1 b 1 2 x 2 3 y 3 4 z Use when_not_matched_insert_all() to perform an "insert if not exists" operation. This will only insert rows that do not already exist in the dataset. import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example2") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform an "insert if not exists" operation&para; dataset.merge_insert("a") \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 0, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 z You are not required to provide all the columns. If you only want to update a subset of columns, you can omit columns you don't want to update. Omitted columns will keep their existing values if they are updated, or will be null if they are inserted. import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"], \ ... "c": ["x", "y", "z"]}) dataset = lance.write_dataset(table, "example3") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform an "upsert" operation, only updating column "a"&para; dataset.merge_insert("a") \ ... .when_matched_update_all() \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b c 0 1 a x 1 2 x y 2 3 y z 3 4 z None migrate_manifest_paths_v2() &para; Migrate the manifest paths to the new format. This will update the manifest to use the new v2 format for paths. This function is idempotent, and can be run multiple times without changing the state of the object store. DANGER: this should not be run while other concurrent operations are happening. And it should also run until completion before resuming other operations. prewarm_index(name) &para; Prewarm an index This will load the entire index into memory. This can help avoid cold start issues with index queries. If the index does not fit in the index cache, then this will result in wasted I/O. Parameters&para; name: str The name of the index to prewarm. replace_field_metadata(field_name, new_metadata) &para; Replace the metadata of a field in the schema Parameters&para; field_name: str The name of the field to replace the metadata for new_metadata: dict The new metadata to set replace_schema(schema) &para; Not implemented (just override pyarrow dataset to prevent segfault) See method:replace_schema_metadata or method:replace_field_metadata replace_schema_metadata(new_metadata) &para; Replace the schema metadata of the dataset Parameters&para; new_metadata: dict The new metadata to set restore() &para; Restore the currently checked out version as the latest version of the dataset. This creates a new commit. sample(num_rows, columns=None, randomize_order=True, **kwargs) &para; Select a random sample of data Parameters&para; num_rows: int number of rows to retrieve columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. **kwargs : dict, optional see scanner() method for full parameter description. Returns&para; table : Table scanner(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, fragments=None, full_text_query=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, fast_search=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, include_deleted_rows=None, scan_stats_callback=None, strict_batch_size=None) &para; Return a Scanner that can support various pushdowns. Parameters&para; columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. filter: pa.compute.Expression or str Expression or str that is a valid SQL where clause. See Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;_ for valid SQL expressions. limit: int, default None Fetch up to this many rows. All rows if None or unspecified. offset: int, default None Fetch starting with this row. 0 if None or unspecified. nearest: dict, default None Get the rows corresponding to the K most similar vectors. Example: .. code-block:: python { &quot;column&quot;: &lt;embedding col name&gt;, &quot;q&quot;: &lt;query vector as pa.Float32Array&gt;, &quot;k&quot;: 10, &quot;nprobes&quot;: 1, &quot;refine_factor&quot;: 1 } int, default None The target size of batches returned. In some cases batches can be up to twice this size (but never larger than this). In some cases batches can be smaller than this size. io_buffer_size: int, default None The size of the IO buffer. See ScannerBuilder.io_buffer_size for more information. batch_readahead: int, optional The number of batches to read ahead. fragment_readahead: int, optional The number of fragments to read ahead. scan_in_order: bool, default True Whether to read the fragments and batches in order. If false, throughput may be higher, but batches will be returned out of order and memory use might increase. fragments: iterable of LanceFragment, default None If specified, only scan these fragments. If scan_in_order is True, then the fragments will be scanned in the order given. prefilter: bool, default False If True then the filter will be applied before the vector query is run. This will generate more correct results but it may be a more costly query. It's generally good when the filter is highly selective. If False then the filter will be applied after the vector query is run. This will perform well but the results may have fewer than the requested number of rows (or be empty) if the rows closest to the query do not match the filter. It&#39;s generally good when the filter is not very selective. use_scalar_index: bool, default True Lance will automatically use scalar indices to optimize a query. In some corner cases this can make query performance worse and this parameter can be used to disable scalar indices in these cases. late_materialization: bool or List[str], default None Allows custom control over late materialization. Late materialization fetches non-query columns using a take operation after the filter. This is useful when there are few results or columns are very large. Early materialization can be better when there are many results or the columns are very narrow. If True, then all columns are late materialized. If False, then all columns are early materialized. If a list of strings, then only the columns in the list are late materialized. The default uses a heuristic that assumes filters will select about 0.1% of the rows. If your filter is more selective (e.g. find by id) you may want to set this to True. If your filter is not very selective (e.g. matches 20% of the rows) you may want to set this to False. full_text_query: str or dict, optional query string to search for, the results will be ranked by BM25. e.g. "hello world", would match documents containing "hello" or "world". or a dictionary with the following keys: - columns: list[str] The columns to search, currently only supports a single column in the columns list. - query: str The query string to search for. fast_search: bool, default False If True, then the search will only be performed on the indexed data, which yields faster search time. scan_stats_callback: Callable[[ScanStatistics], None], default None A callback function that will be called with the scan statistics after the scan is complete. Errors raised by the callback will be logged but not re-raised. include_deleted_rows: bool, default False If True, then rows that have been deleted, but are still present in the fragment, will be returned. These rows will have the _rowid column set to null. All other columns will reflect the value stored on disk and may not be null. Note: if this is a search operation, or a take operation (including scalar indexed scans) then deleted rows cannot be returned. .. note:: For now, if BOTH filter and nearest is specified, then: 1. nearest is executed first. 2. The results are filtered afterwards. For debugging ANN results, you can choose to not use the index even if present by specifying use_index=False. For example, the following will always return exact KNN results: .. code-block:: python dataset.to_table(nearest={ &quot;column&quot;: &quot;vector&quot;, &quot;k&quot;: 10, &quot;q&quot;: &lt;query vector&gt;, &quot;use_index&quot;: False } session() &para; Return the dataset session, which holds the dataset's state. take(indices, columns=None) &para; Select rows of data by index. Parameters&para; indices : Array or array-like indices of rows to select in the dataset. columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. Returns&para; table : pyarrow.Table take_blobs(blob_column, ids=None, addresses=None, indices=None) &para; Select blobs by row IDs. Instead of loading large binary blob data into memory before processing it, this API allows you to open binary blob data as a regular Python file-like object. For more details, see class:lance.BlobFile. Exactly one of ids, addresses, or indices must be specified. Parameters blob_column : str The name of the blob column to select. ids : Integer Array or array-like row IDs to select in the dataset. addresses: Integer Array or array-like The (unstable) row addresses to select in the dataset. indices : Integer Array or array-like The offset / indices of the row in the dataset. Returns&para; blob_files : List[BlobFile] to_batches(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, full_text_query=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, strict_batch_size=None, **kwargs) &para; Read the dataset as materialized record batches. Parameters&para; **kwargs : dict, optional Arguments for meth:~LanceDataset.scanner. Returns&para; record_batches : Iterator of class:~pyarrow.RecordBatch to_table(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, fast_search=None, full_text_query=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, include_deleted_rows=None) &para; Read the data into memory as a class:pyarrow.Table Parameters&para; columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. filter : pa.compute.Expression or str Expression or str that is a valid SQL where clause. See Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;_ for valid SQL expressions. limit: int, default None Fetch up to this many rows. All rows if None or unspecified. offset: int, default None Fetch starting with this row. 0 if None or unspecified. nearest: dict, default None Get the rows corresponding to the K most similar vectors. Example: .. code-block:: python { &quot;column&quot;: &lt;embedding col name&gt;, &quot;q&quot;: &lt;query vector as pa.Float32Array&gt;, &quot;k&quot;: 10, &quot;metric&quot;: &quot;cosine&quot;, &quot;nprobes&quot;: 1, &quot;refine_factor&quot;: 1 } int, optional The number of rows to read at a time. io_buffer_size: int, default None The size of the IO buffer. See ScannerBuilder.io_buffer_size for more information. batch_readahead: int, optional The number of batches to read ahead. fragment_readahead: int, optional The number of fragments to read ahead. scan_in_order: bool, optional, default True Whether to read the fragments and batches in order. If false, throughput may be higher, but batches will be returned out of order and memory use might increase. prefilter: bool, optional, default False Run filter before the vector search. late_materialization: bool or List[str], default None Allows custom control over late materialization. See ScannerBuilder.late_materialization for more information. use_scalar_index: bool, default True Allows custom control over scalar index usage. See ScannerBuilder.use_scalar_index for more information. with_row_id: bool, optional, default False Return row ID. with_row_address: bool, optional, default False Return row address use_stats: bool, optional, default True Use stats pushdown during filters. fast_search: bool, optional, default False full_text_query: str or dict, optional query string to search for, the results will be ranked by BM25. e.g. "hello world", would match documents contains "hello" or "world". or a dictionary with the following keys: - columns: list[str] The columns to search, currently only supports a single column in the columns list. - query: str The query string to search for. include_deleted_rows: bool, optional, default False If True, then rows that have been deleted, but are still present in the fragment, will be returned. These rows will have the _rowid column set to null. All other columns will reflect the value stored on disk and may not be null. Note: if this is a search operation, or a take operation (including scalar indexed scans) then deleted rows cannot be returned. Notes&para; If BOTH filter and nearest is specified, then: nearest is executed first. The results are filtered afterward, unless pre-filter sets to True. update(updates, where=None) &para; Update column values for rows matching where. Parameters&para; updates : dict of str to str A mapping of column names to a SQL expression. where : str, optional A SQL predicate indicating which rows should be updated. Returns&para; updates : dict A dictionary containing the number of rows updated. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") update_stats = dataset.update(dict(a = 'a + 2'), where="b != 'a'") update_stats["num_updated_rows"] = 2 dataset.to_table().to_pandas() a b 0 1 a 1 4 b 2 5 c validate() &para; Validate the dataset. This checks the integrity of the dataset and will raise an exception if the dataset is corrupted. versions() &para; Return all versions in this dataset. LanceOperation &para; Append dataclass &para; Bases: BaseOperation Append new rows to the dataset. Attributes&para; fragments: list[FragmentMetadata] The fragments that contain the new rows. Warning&para; This is an advanced API for distributed operations. To append to a dataset on a single machine, use :func:lance.write_dataset. Examples&para; To append new rows to a dataset, first use :meth:lance.fragment.LanceFragment.create to create fragments. Then collect the fragment metadata into a list and pass it to this class. Finally, pass the operation to the :meth:LanceDataset.commit method to create the new dataset. import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) dataset = lance.write_dataset(tab1, "example") tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment = lance.fragment.LanceFragment.create("example", tab2) operation = lance.LanceOperation.Append([fragment]) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d BaseOperation &para; Bases: ABC Base class for operations that can be applied to a dataset. See available operations under :class:LanceOperation. CreateIndex dataclass &para; Bases: BaseOperation Operation that creates an index on the dataset. DataReplacement dataclass &para; Bases: BaseOperation Operation that replaces existing datafiles in the dataset. DataReplacementGroup dataclass &para; Group of data replacements Delete dataclass &para; Bases: BaseOperation Remove fragments or rows from the dataset. Attributes&para; updated_fragments: list[FragmentMetadata] The fragments that have been updated with new deletion vectors. deleted_fragment_ids: list[int] The ids of the fragments that have been deleted entirely. These are the fragments where :meth:LanceFragment.delete() returned None. predicate: str The original SQL predicate used to select the rows to delete. Warning&para; This is an advanced API for distributed operations. To delete rows from dataset on a single machine, use :meth:lance.LanceDataset.delete. Examples&para; To delete rows from a dataset, call :meth:lance.fragment.LanceFragment.delete on each of the fragments. If that returns a new fragment, add that to the updated_fragments list. If it returns None, that means the whole fragment was deleted, so add the fragment id to the deleted_fragment_ids. Finally, pass the operation to the :meth:LanceDataset.commit method to complete the deletion operation. import lance import pyarrow as pa table = pa.table({"a": [1, 2], "b": ["a", "b"]}) dataset = lance.write_dataset(table, "example") table = pa.table({"a": [3, 4], "b": ["c", "d"]}) dataset = lance.write_dataset(table, "example", mode="append") dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d predicate = "a &gt;= 2" updated_fragments = [] deleted_fragment_ids = [] for fragment in dataset.get_fragments(): ... new_fragment = fragment.delete(predicate) ... if new_fragment is not None: ... updated_fragments.append(new_fragment) ... else: ... deleted_fragment_ids.append(fragment.fragment_id) operation = lance.LanceOperation.Delete(updated_fragments, ... deleted_fragment_ids, ... predicate) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b 0 1 a Merge dataclass &para; Bases: BaseOperation Operation that adds columns. Unlike Overwrite, this should not change the structure of the fragments, allowing existing indices to be kept. Attributes&para; fragments: iterable of FragmentMetadata The fragments that make up the new dataset. schema: LanceSchema or pyarrow.Schema The schema of the new dataset. Passing a LanceSchema is preferred, and passing a pyarrow.Schema is deprecated. Warning&para; This is an advanced API for distributed operations. To overwrite or create new dataset on a single machine, use :func:lance.write_dataset. Examples&para; To add new columns to a dataset, first define a method that will create the new columns based on the existing columns. Then use :meth:lance.fragment.LanceFragment.add_columns import lance import pyarrow as pa import pyarrow.compute as pc table = pa.table({"a": [1, 2, 3, 4], "b": ["a", "b", "c", "d"]}) dataset = lance.write_dataset(table, "example") dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d def double_a(batch: pa.RecordBatch) -&gt; pa.RecordBatch: ... doubled = pc.multiply(batch["a"], 2) ... return pa.record_batch([doubled], ["a_doubled"]) fragments = [] for fragment in dataset.get_fragments(): ... new_fragment, new_schema = fragment.merge_columns(double_a, ... columns=['a']) ... fragments.append(new_fragment) operation = lance.LanceOperation.Merge(fragments, new_schema) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b a_doubled 0 1 a 2 1 2 b 4 2 3 c 6 3 4 d 8 Overwrite dataclass &para; Bases: BaseOperation Overwrite or create a new dataset. Attributes&para; new_schema: pyarrow.Schema The schema of the new dataset. fragments: list[FragmentMetadata] The fragments that make up the new dataset. Warning&para; This is an advanced API for distributed operations. To overwrite or create new dataset on a single machine, use :func:lance.write_dataset. Examples&para; To create or overwrite a dataset, first use :meth:lance.fragment.LanceFragment.create to create fragments. Then collect the fragment metadata into a list and pass it along with the schema to this class. Finally, pass the operation to the :meth:LanceDataset.commit method to create the new dataset. import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment1 = lance.fragment.LanceFragment.create("example", tab1) fragment2 = lance.fragment.LanceFragment.create("example", tab2) fragments = [fragment1, fragment2] operation = lance.LanceOperation.Overwrite(tab1.schema, fragments) dataset = lance.LanceDataset.commit("example", operation) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d Project dataclass &para; Bases: BaseOperation Operation that project columns. Use this operator for drop column or rename/swap column. Attributes&para; schema: LanceSchema The lance schema of the new dataset. Examples&para; Use the projece operator to swap column: import lance import pyarrow as pa import pyarrow.compute as pc from lance.schema import LanceSchema table = pa.table({"a": [1, 2], "b": ["a", "b"], "b1": ["c", "d"]}) dataset = lance.write_dataset(table, "example") dataset.to_table().to_pandas() a b b1 0 1 a c 1 2 b d rename column b into b0 and rename b1 into b&para; table = pa.table({"a": [3, 4], "b0": ["a", "b"], "b": ["c", "d"]}) lance_schema = LanceSchema.from_pyarrow(table.schema) operation = lance.LanceOperation.Project(lance_schema) dataset = lance.LanceDataset.commit("example", operation, read_version=1) dataset.to_table().to_pandas() a b0 b 0 1 a c 1 2 b d Restore dataclass &para; Bases: BaseOperation Operation that restores a previous version of the dataset. Rewrite dataclass &para; Bases: BaseOperation Operation that rewrites one or more files and indices into one or more files and indices. Attributes&para; groups: list[RewriteGroup] Groups of files that have been rewritten. rewritten_indices: list[RewrittenIndex] Indices that have been rewritten. Warning&para; This is an advanced API not intended for general use. RewriteGroup dataclass &para; Collection of rewritten files RewrittenIndex dataclass &para; An index that has been rewritten Update dataclass &para; Bases: BaseOperation Operation that updates rows in the dataset. Attributes&para; removed_fragment_ids: list[int] The ids of the fragments that have been removed entirely. updated_fragments: list[FragmentMetadata] The fragments that have been updated with new deletion vectors. new_fragments: list[FragmentMetadata] The fragments that contain the new rows. LanceScanner &para; Bases: Scanner dataset_schema property &para; The schema with which batches will be read from fragments. analyze_plan() &para; Execute the plan for this scanner and display with runtime metrics. Parameters&para; verbose : bool, default False Use a verbose output format. Returns&para; plan : str count_rows() &para; Count rows matching the scanner filter. Returns&para; count : int explain_plan(verbose=False) &para; Return the execution plan for this scanner. Parameters&para; verbose : bool, default False Use a verbose output format. Returns&para; plan : str from_batches(*args, **kwargs) staticmethod &para; Not implemented from_dataset(*args, **kwargs) staticmethod &para; Not implemented from_fragment(*args, **kwargs) staticmethod &para; Not implemented head(num_rows) &para; Load the first N rows of the dataset. Parameters&para; num_rows : int The number of rows to load. Returns&para; Table scan_batches() &para; Consume a Scanner in record batches with corresponding fragments. Returns&para; record_batches : iterator of TaggedRecordBatch take(indices) &para; Not implemented to_table() &para; Read the data into memory and return a pyarrow Table. LanceStats &para; Statistics about a LanceDataset. data_stats() &para; Statistics about the data in the dataset. dataset_stats(max_rows_per_group=1024) &para; Statistics about the dataset. index_stats(index_name) &para; Statistics about an index. Parameters&para; index_name: str The name of the index to get statistics for. MergeInsertBuilder &para; Bases: _MergeInsertBuilder conflict_retries(max_retries) &para; Set number of times to retry the operation if there is contention. If this is set &gt; 0, then the operation will keep a copy of the input data either in memory or on disk (depending on the size of the data) and will retry the operation if there is contention. Default is 10. execute(data_obj, *, schema=None) &para; Executes the merge insert operation This function updates the original dataset and returns a dictionary with information about merge statistics - i.e. the number of inserted, updated, and deleted rows. Parameters&para; ReaderLike The new data to use as the source table for the operation. This parameter can be any source of data (e.g. table / dataset) that :func:~lance.write_dataset accepts. schema: Optional[pa.Schema] The schema of the data. This only needs to be supplied whenever the data source is some kind of generator. execute_uncommitted(data_obj, *, schema=None) &para; Executes the merge insert operation without committing This function updates the original dataset and returns a dictionary with information about merge statistics - i.e. the number of inserted, updated, and deleted rows. Parameters&para; ReaderLike The new data to use as the source table for the operation. This parameter can be any source of data (e.g. table / dataset) that :func:~lance.write_dataset accepts. schema: Optional[pa.Schema] The schema of the data. This only needs to be supplied whenever the data source is some kind of generator. retry_timeout(timeout) &para; Set the timeout used to limit retries. This is the maximum time to spend on the operation before giving up. At least one attempt will be made, regardless of how long it takes to complete. Subsequent attempts will be cancelled once this timeout is reached. If the timeout has been reached during the first attempt, the operation will be cancelled immediately before making a second attempt. The default is 30 seconds. when_matched_update_all(condition=None) &para; Configure the operation to update matched rows After this method is called, when the merge insert operation executes, any rows that match both the source table and the target table will be updated. The rows from the target table will be removed and the rows from the source table will be added. An optional condition may be specified. This should be an SQL filter and, if present, then only matched rows that also satisfy this filter will be updated. The SQL filter should use the prefix target. to refer to columns in the target table and the prefix source. to refer to columns in the source table. For example, source.last_update &lt; target.last_update. If a condition is specified and rows do not satisfy the condition then these rows will not be updated. Failure to satisfy the filter does not cause a "matched" row to become a "not matched" row. when_not_matched_by_source_delete(expr=None) &para; Configure the operation to delete source rows that do not match After this method is called, when the merge insert operation executes, any rows that exist only in the target table will be deleted. An optional filter can be specified to limit the scope of the delete operation. If given (as an SQL filter) then only rows which match the filter will be deleted. when_not_matched_insert_all() &para; Configure the operation to insert not matched rows After this method is called, when the merge insert operation executes, any rows that exist only in the source table will be inserted into the target table. ScannerBuilder &para; batch_readahead(nbatches=None) &para; This parameter is ignored when reading v2 files batch_size(batch_size) &para; Set batch size for Scanner fast_search(flag) &para; Enable fast search, which only perform search on the indexed data. Users can use Table::optimize() or create_index() to include the new data into index, thus make new data searchable. full_text_search(query, columns=None) &para; Filter rows by full text searching. Experimental API, may remove it after we support to do this within filter SQL-like expression Must create inverted index on the given column before searching, Parameters&para; query : str | Query If str, the query string to search for, a match query would be performed. If Query, the query object to search for, and the columns parameter will be ignored. columns : list of str, optional The columns to search in. If None, search in all indexed columns. include_deleted_rows(flag) &para; Include deleted rows Rows which have been deleted, but are still present in the fragment, will be returned. These rows will have all columns (except _rowaddr) set to null io_buffer_size(io_buffer_size) &para; Set the I/O buffer size for the Scanner This is the amount of RAM that will be reserved for holding I/O received from storage before it is processed. This is used to control the amount of memory used by the scanner. If the buffer is full then the scanner will block until the buffer is processed. Generally this should scale with the number of concurrent I/O threads. The default is 2GiB which comfortably provides enough space for somewhere between 32 and 256 concurrent I/O threads. This value is not a hard cap on the amount of RAM the scanner will use. Some space is used for the compute (which can be controlled by the batch size) and Lance does not keep track of memory after it is returned to the user. Currently, if there is a single batch of data which is larger than the io buffer size then the scanner will deadlock. This is a known issue and will be fixed in a future release. This parameter is only used when reading v2 files scan_in_order(scan_in_order=True) &para; Whether to scan the dataset in order of fragments and batches. If set to False, the scanner may read fragments concurrently and yield batches out of order. This may improve performance since it allows more concurrency in the scan, but can also use more memory. This parameter is ignored when using v2 files. In the v2 file format there is no penalty to scanning in order and so all scans will scan in order. scan_stats_callback(callback) &para; Set a callback function that will be called with the scan statistics after the scan is complete. Errors raised by the callback will be logged but not re-raised. strict_batch_size(strict_batch_size=False) &para; If True, then all batches except the last batch will have exactly batch_size rows. By default, it is false. If this is true then small batches will need to be merged together which will require a data copy and incur a (typically very small) performance penalty. use_scalar_index(use_scalar_index=True) &para; Set whether scalar indices should be used in a query Scans will use scalar indices, when available, to optimize queries with filters. However, in some corner cases, scalar indices may make performance worse. This parameter allows users to disable scalar indices in these cases. use_stats(use_stats=True) &para; Enable use of statistics for query planning. Disabling statistics is used for debugging and benchmarking purposes. This should be left on for normal use. with_row_address(with_row_address=True) &para; Enables returns with row addresses. Row addresses are a unique but unstable identifier for each row in the dataset that consists of the fragment id (upper 32 bits) and the row offset in the fragment (lower 32 bits). Row IDs are generally preferred since they do not change when a row is modified or compacted. However, row addresses may be useful in some advanced use cases. with_row_id(with_row_id=True) &para; Enable returns with row IDs. Tags &para; Dataset tag manager. create(tag, version) &para; Create a tag for a given dataset version. Parameters&para; tag: str, The name of the tag to create. This name must be unique among all tag names for the dataset. version: int, The dataset version to tag. delete(tag) &para; Delete tag from the dataset. Parameters&para; tag: str, The name of the tag to delete. list() &para; List all dataset tags. Returns&para; dict[str, Tag] A dictionary mapping tag names to version numbers. update(tag, version) &para; Update tag to a new version. Parameters&para; tag: str, The name of the tag to update. version: int, The new dataset version to tag. VectorIndexReader &para; This class allows you to initialize a reader for a specific vector index, retrieve the number of partitions, access the centroids of the index, and read specific partitions of the index. Parameters&para; dataset: LanceDataset The dataset containing the index. index_name: str The name of the vector index to read. Examples&para; .. code-block:: python import lance from lance.dataset import VectorIndexReader import numpy as np import pyarrow as pa vectors = np.random.rand(256, 2) data = pa.table({&quot;vector&quot;: pa.array(vectors.tolist(), type=pa.list_(pa.float32(), 2))}) dataset = lance.write_dataset(data, &quot;/tmp/index_reader_demo&quot;) dataset.create_index(&quot;vector&quot;, index_type=&quot;IVF_PQ&quot;, num_partitions=4, num_sub_vectors=2) reader = VectorIndexReader(dataset, &quot;vector_idx&quot;) assert reader.num_partitions() == 4 partition = reader.read_partition(0) assert &quot;_rowid&quot; in partition.column_names Exceptions&para; ValueError If the specified index is not a vector index. centroids() &para; Returns the centroids of the index Returns&para; np.ndarray The centroids of IVF with shape (num_partitions, dim) num_partitions() &para; Returns the number of partitions in the dataset. Returns&para; int The number of partitions. read_partition(partition_id, *, with_vector=False) &para; Returns a pyarrow table for the given IVF partition Parameters&para; partition_id: int The id of the partition to read with_vector: bool, default False Whether to include the vector column in the reader, for IVF_PQ, the vector column is PQ codes Returns&para; pa.Table A pyarrow table for the given partition, containing the row IDs, and quantized vectors (if with_vector is True). write_dataset(data_obj, uri, schema=None, mode=&#39;create&#39;, *, max_rows_per_file=1024 * 1024, max_rows_per_group=1024, max_bytes_per_file=90 * 1024 * 1024 * 1024, commit_lock=None, progress=None, storage_options=None, data_storage_version=None, use_legacy_format=None, enable_v2_manifest_paths=False, enable_move_stable_row_ids=False) &para; Write a given data_obj to the given uri Parameters&para; data_obj: Reader-like The data to be written. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader - Huggingface dataset uri: str, Path, or LanceDataset Where to write the dataset to (directory). If a LanceDataset is passed, the session will be reused. schema: Schema, optional If specified and the input is a pandas DataFrame, use this schema instead of the default pandas to arrow table conversion. mode: str create - create a new dataset (raises if uri already exists). overwrite - create a new snapshot version append - create a new version that is the concat of the input the latest version (raises if uri does not exist) max_rows_per_file: int, default 1024 * 1024 The max number of rows to write before starting a new file max_rows_per_group: int, default 1024 The max number of rows before starting a new group (in the same file) max_bytes_per_file: int, default 90 * 1024 * 1024 * 1024 The max number of bytes to write before starting a new file. This is a soft limit. This limit is checked after each group is written, which means larger groups may cause this to be overshot meaningfully. This defaults to 90 GB, since we have a hard limit of 100 GB per file on object stores. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. progress: FragmentWriteProgress, optional Experimental API. Progress tracking for writing the fragment. Pass a custom class that defines hooks to be called when each fragment is starting to write and finishing writing. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. data_storage_version: optional, str, default None The version of the data storage format to use. Newer versions are more efficient but require newer versions of lance to read. The default (None) will use the latest stable version. See the user guide for more details. use_legacy_format : optional, bool, default None Deprecated method for setting the data storage version. Use the data_storage_version parameter instead. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:LanceDataset.migrate_manifest_paths_v2 method. Default is False. enable_move_stable_row_ids : bool, optional Experimental parameter: if set to true, the writer will use move-stable row ids. These row ids are stable after compaction operations, but not after updates. This makes compaction more efficient, since with stable row ids no secondary indices need to be updated to point to new row ids. and rename b1 into
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Restore" class="md-nav__link">
    <span class="md-ellipsis">
      Restore
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Rewrite" class="md-nav__link">
    <span class="md-ellipsis">
      Rewrite
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Rewrite">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Rewrite--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Rewrite--warning" class="md-nav__link">
    <span class="md-ellipsis">
      Warning
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.RewriteGroup" class="md-nav__link">
    <span class="md-ellipsis">
      RewriteGroup
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.RewrittenIndex" class="md-nav__link">
    <span class="md-ellipsis">
      RewrittenIndex
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Update" class="md-nav__link">
    <span class="md-ellipsis">
      Update
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceOperation.Update--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner" class="md-nav__link">
    <span class="md-ellipsis">
      LanceScanner
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LanceScanner">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.dataset_schema" class="md-nav__link">
    <span class="md-ellipsis">
      dataset_schema
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.analyze_plan" class="md-nav__link">
    <span class="md-ellipsis">
      analyze_plan
    </span>
  </a>
  
    <nav class="md-nav" aria-label="analyze_plan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.analyze_plan--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.analyze_plan--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.count_rows" class="md-nav__link">
    <span class="md-ellipsis">
      count_rows
    </span>
  </a>
  
    <nav class="md-nav" aria-label="count_rows">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.count_rows--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.explain_plan" class="md-nav__link">
    <span class="md-ellipsis">
      explain_plan
    </span>
  </a>
  
    <nav class="md-nav" aria-label="explain_plan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.explain_plan--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.explain_plan--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.from_batches" class="md-nav__link">
    <span class="md-ellipsis">
      from_batches
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.from_dataset" class="md-nav__link">
    <span class="md-ellipsis">
      from_dataset
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.from_fragment" class="md-nav__link">
    <span class="md-ellipsis">
      from_fragment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.head" class="md-nav__link">
    <span class="md-ellipsis">
      head
    </span>
  </a>
  
    <nav class="md-nav" aria-label="head">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.head--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.head--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.scan_batches" class="md-nav__link">
    <span class="md-ellipsis">
      scan_batches
    </span>
  </a>
  
    <nav class="md-nav" aria-label="scan_batches">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.scan_batches--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.take" class="md-nav__link">
    <span class="md-ellipsis">
      take
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.LanceScanner.to_table" class="md-nav__link">
    <span class="md-ellipsis">
      to_table
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.MergeInsertBuilder" class="md-nav__link">
    <span class="md-ellipsis">
      MergeInsertBuilder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MergeInsertBuilder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.MergeInsertBuilder.conflict_retries" class="md-nav__link">
    <span class="md-ellipsis">
      conflict_retries
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.MergeInsertBuilder.execute" class="md-nav__link">
    <span class="md-ellipsis">
      execute
    </span>
  </a>
  
    <nav class="md-nav" aria-label="execute">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.MergeInsertBuilder.execute--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.MergeInsertBuilder.execute_uncommitted" class="md-nav__link">
    <span class="md-ellipsis">
      execute_uncommitted
    </span>
  </a>
  
    <nav class="md-nav" aria-label="execute_uncommitted">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.MergeInsertBuilder.execute_uncommitted--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.MergeInsertBuilder.retry_timeout" class="md-nav__link">
    <span class="md-ellipsis">
      retry_timeout
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.MergeInsertBuilder.when_matched_update_all" class="md-nav__link">
    <span class="md-ellipsis">
      when_matched_update_all
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.MergeInsertBuilder.when_not_matched_by_source_delete" class="md-nav__link">
    <span class="md-ellipsis">
      when_not_matched_by_source_delete
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.MergeInsertBuilder.when_not_matched_insert_all" class="md-nav__link">
    <span class="md-ellipsis">
      when_not_matched_insert_all
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.batch_udf" class="md-nav__link">
    <span class="md-ellipsis">
      batch_udf
    </span>
  </a>
  
    <nav class="md-nav" aria-label="batch_udf">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.batch_udf--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.batch_udf--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.json_to_schema" class="md-nav__link">
    <span class="md-ellipsis">
      json_to_schema
    </span>
  </a>
  
    <nav class="md-nav" aria-label="json_to_schema">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.json_to_schema--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.schema_to_json" class="md-nav__link">
    <span class="md-ellipsis">
      schema_to_json
    </span>
  </a>
  
    <nav class="md-nav" aria-label="schema_to_json">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.schema_to_json--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.write_dataset" class="md-nav__link">
    <span class="md-ellipsis">
      write_dataset
    </span>
  </a>
  
    <nav class="md-nav" aria-label="write_dataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.write_dataset--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lancedataset" class="md-nav__link">
    <span class="md-ellipsis">
      lance.dataset
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lance.dataset" class="md-nav__link">
    <span class="md-ellipsis">
      dataset
    </span>
  </a>
  
    <nav class="md-nav" aria-label="dataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.DataStatistics" class="md-nav__link">
    <span class="md-ellipsis">
      DataStatistics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.DatasetOptimizer" class="md-nav__link">
    <span class="md-ellipsis">
      DatasetOptimizer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="DatasetOptimizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.DatasetOptimizer.compact_files" class="md-nav__link">
    <span class="md-ellipsis">
      compact_files
    </span>
  </a>
  
    <nav class="md-nav" aria-label="compact_files">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.DatasetOptimizer.compact_files--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.DatasetOptimizer.compact_files--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.DatasetOptimizer.compact_files--see-also" class="md-nav__link">
    <span class="md-ellipsis">
      See Also
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.DatasetOptimizer.optimize_indices" class="md-nav__link">
    <span class="md-ellipsis">
      optimize_indices
    </span>
  </a>
  
    <nav class="md-nav" aria-label="optimize_indices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.DatasetOptimizer.optimize_indices--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.FieldStatistics" class="md-nav__link">
    <span class="md-ellipsis">
      FieldStatistics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset" class="md-nav__link">
    <span class="md-ellipsis">
      LanceDataset
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LanceDataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.data_storage_version" class="md-nav__link">
    <span class="md-ellipsis">
      data_storage_version
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.lance_schema" class="md-nav__link">
    <span class="md-ellipsis">
      lance_schema
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.latest_version" class="md-nav__link">
    <span class="md-ellipsis">
      latest_version
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.max_field_id" class="md-nav__link">
    <span class="md-ellipsis">
      max_field_id
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.partition_expression" class="md-nav__link">
    <span class="md-ellipsis">
      partition_expression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.schema" class="md-nav__link">
    <span class="md-ellipsis">
      schema
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.stats" class="md-nav__link">
    <span class="md-ellipsis">
      stats
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.tags" class="md-nav__link">
    <span class="md-ellipsis">
      tags
    </span>
  </a>
  
    <nav class="md-nav" aria-label="tags">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.tags--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.uri" class="md-nav__link">
    <span class="md-ellipsis">
      uri
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.version" class="md-nav__link">
    <span class="md-ellipsis">
      version
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.add_columns" class="md-nav__link">
    <span class="md-ellipsis">
      add_columns
    </span>
  </a>
  
    <nav class="md-nav" aria-label="add_columns">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.add_columns--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.add_columns--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.add_columns--see-also" class="md-nav__link">
    <span class="md-ellipsis">
      See Also
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.alter_columns" class="md-nav__link">
    <span class="md-ellipsis">
      alter_columns
    </span>
  </a>
  
    <nav class="md-nav" aria-label="alter_columns">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.alter_columns--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.alter_columns--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.checkout_version" class="md-nav__link">
    <span class="md-ellipsis">
      checkout_version
    </span>
  </a>
  
    <nav class="md-nav" aria-label="checkout_version">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.checkout_version--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.checkout_version--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.cleanup_old_versions" class="md-nav__link">
    <span class="md-ellipsis">
      cleanup_old_versions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="cleanup_old_versions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.cleanup_old_versions--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.commit" class="md-nav__link">
    <span class="md-ellipsis">
      commit
    </span>
  </a>
  
    <nav class="md-nav" aria-label="commit">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.commit--warnings" class="md-nav__link">
    <span class="md-ellipsis">
      Warnings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.commit--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.commit--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.commit--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.commit_batch" class="md-nav__link">
    <span class="md-ellipsis">
      commit_batch
    </span>
  </a>
  
    <nav class="md-nav" aria-label="commit_batch">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.commit_batch--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.commit_batch--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.count_rows" class="md-nav__link">
    <span class="md-ellipsis">
      count_rows
    </span>
  </a>
  
    <nav class="md-nav" aria-label="count_rows">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.count_rows--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.count_rows--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.create_index" class="md-nav__link">
    <span class="md-ellipsis">
      create_index
    </span>
  </a>
  
    <nav class="md-nav" aria-label="create_index">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.create_index--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.create_index--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.create_index--references" class="md-nav__link">
    <span class="md-ellipsis">
      References
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.create_scalar_index" class="md-nav__link">
    <span class="md-ellipsis">
      create_scalar_index
    </span>
  </a>
  
    <nav class="md-nav" aria-label="create_scalar_index">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.create_scalar_index--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.create_scalar_index--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.delete" class="md-nav__link">
    <span class="md-ellipsis">
      delete
    </span>
  </a>
  
    <nav class="md-nav" aria-label="delete">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.delete--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.delete--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.drop_columns" class="md-nav__link">
    <span class="md-ellipsis">
      drop_columns
    </span>
  </a>
  
    <nav class="md-nav" aria-label="drop_columns">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.drop_columns--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.drop_columns--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.drop_index" class="md-nav__link">
    <span class="md-ellipsis">
      drop_index
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.get_fragment" class="md-nav__link">
    <span class="md-ellipsis">
      get_fragment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.get_fragments" class="md-nav__link">
    <span class="md-ellipsis">
      get_fragments
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.head" class="md-nav__link">
    <span class="md-ellipsis">
      head
    </span>
  </a>
  
    <nav class="md-nav" aria-label="head">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.head--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.head--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.insert" class="md-nav__link">
    <span class="md-ellipsis">
      insert
    </span>
  </a>
  
    <nav class="md-nav" aria-label="insert">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.insert--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.join" class="md-nav__link">
    <span class="md-ellipsis">
      join
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.merge" class="md-nav__link">
    <span class="md-ellipsis">
      merge
    </span>
  </a>
  
    <nav class="md-nav" aria-label="merge">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.merge--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.merge--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.merge--see-also" class="md-nav__link">
    <span class="md-ellipsis">
      See Also
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.merge_insert" class="md-nav__link">
    <span class="md-ellipsis">
      merge_insert
    </span>
  </a>
  
    <nav class="md-nav" aria-label="merge_insert">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.merge_insert--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.merge_insert--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.merge_insert--perform-a-upsert-operation" class="md-nav__link">
    <span class="md-ellipsis">
      Perform a "upsert" operation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.merge_insert--perform-an-insert-if-not-exists-operation" class="md-nav__link">
    <span class="md-ellipsis">
      Perform an "insert if not exists" operation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.merge_insert--perform-an-upsert-operation-only-updating-column-a" class="md-nav__link">
    <span class="md-ellipsis">
      Perform an "upsert" operation, only updating column "a"
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.migrate_manifest_paths_v2" class="md-nav__link">
    <span class="md-ellipsis">
      migrate_manifest_paths_v2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.prewarm_index" class="md-nav__link">
    <span class="md-ellipsis">
      prewarm_index
    </span>
  </a>
  
    <nav class="md-nav" aria-label="prewarm_index">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.prewarm_index--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.replace_field_metadata" class="md-nav__link">
    <span class="md-ellipsis">
      replace_field_metadata
    </span>
  </a>
  
    <nav class="md-nav" aria-label="replace_field_metadata">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.replace_field_metadata--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.replace_schema" class="md-nav__link">
    <span class="md-ellipsis">
      replace_schema
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.replace_schema_metadata" class="md-nav__link">
    <span class="md-ellipsis">
      replace_schema_metadata
    </span>
  </a>
  
    <nav class="md-nav" aria-label="replace_schema_metadata">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.replace_schema_metadata--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.restore" class="md-nav__link">
    <span class="md-ellipsis">
      restore
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.sample" class="md-nav__link">
    <span class="md-ellipsis">
      sample
    </span>
  </a>
  
    <nav class="md-nav" aria-label="sample">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.sample--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.sample--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.scanner" class="md-nav__link">
    <span class="md-ellipsis">
      scanner
    </span>
  </a>
  
    <nav class="md-nav" aria-label="scanner">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.scanner--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.session" class="md-nav__link">
    <span class="md-ellipsis">
      session
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.take" class="md-nav__link">
    <span class="md-ellipsis">
      take
    </span>
  </a>
  
    <nav class="md-nav" aria-label="take">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.take--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.take--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.take_blobs" class="md-nav__link">
    <span class="md-ellipsis">
      take_blobs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="take_blobs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.take_blobs--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.to_batches" class="md-nav__link">
    <span class="md-ellipsis">
      to_batches
    </span>
  </a>
  
    <nav class="md-nav" aria-label="to_batches">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.to_batches--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.to_batches--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.to_table" class="md-nav__link">
    <span class="md-ellipsis">
      to_table
    </span>
  </a>
  
    <nav class="md-nav" aria-label="to_table">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.to_table--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.to_table--notes" class="md-nav__link">
    <span class="md-ellipsis">
      Notes
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.update" class="md-nav__link">
    <span class="md-ellipsis">
      update
    </span>
  </a>
  
    <nav class="md-nav" aria-label="update">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.update--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.update--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.update--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.validate" class="md-nav__link">
    <span class="md-ellipsis">
      validate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceDataset.versions" class="md-nav__link">
    <span class="md-ellipsis">
      versions
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation" class="md-nav__link">
    <span class="md-ellipsis">
      LanceOperation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LanceOperation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Append" class="md-nav__link">
    <span class="md-ellipsis">
      Append
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Append">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Append--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Append--warning" class="md-nav__link">
    <span class="md-ellipsis">
      Warning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Append--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.BaseOperation" class="md-nav__link">
    <span class="md-ellipsis">
      BaseOperation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.CreateIndex" class="md-nav__link">
    <span class="md-ellipsis">
      CreateIndex
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.DataReplacement" class="md-nav__link">
    <span class="md-ellipsis">
      DataReplacement
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.DataReplacementGroup" class="md-nav__link">
    <span class="md-ellipsis">
      DataReplacementGroup
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Delete" class="md-nav__link">
    <span class="md-ellipsis">
      Delete
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Delete">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Delete--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Delete--warning" class="md-nav__link">
    <span class="md-ellipsis">
      Warning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Delete--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Merge" class="md-nav__link">
    <span class="md-ellipsis">
      Merge
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Merge">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Merge--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Merge--warning" class="md-nav__link">
    <span class="md-ellipsis">
      Warning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Merge--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Overwrite" class="md-nav__link">
    <span class="md-ellipsis">
      Overwrite
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Overwrite">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Overwrite--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Overwrite--warning" class="md-nav__link">
    <span class="md-ellipsis">
      Warning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Overwrite--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Project" class="md-nav__link">
    <span class="md-ellipsis">
      Project
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Project">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Project--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Project--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Project--rename-column-b-into-b0-and-rename-b1-into-b" class="md-nav__link">
    <span class="md-ellipsis">
      rename column lance &para; __version__ = &#39;0.27.3&#39; module &para; str(object='') -&gt; str str(bytes_or_buffer[, encoding[, errors]]) -&gt; str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object.str() (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. BlobColumn &para; A utility to wrap a Pyarrow binary column and iterate over the rows as file-like objects. This can be useful for working with medium-to-small binary objects that need to interface with APIs that expect file-like objects. For very large binary objects (4-8MB or more per value) you might be better off creating a blob column and using meth:lance.Dataset.take_blobs to access the blob data. BlobFile &para; Bases: RawIOBase Represents a blob in a Lance dataset as a file-like object. __init__(inner) &para; Internal only: To obtain a BlobFile use meth:lance.dataset.Dataset.take_blobs. size() &para; Returns the size of the blob in bytes. DataStatistics dataclass &para; Statistics about the data in the dataset FieldStatistics dataclass &para; Statistics about a field in the dataset FragmentMetadata dataclass &para; Metadata for a fragment. Attributes&para; id : int The ID of the fragment. files : List[DataFile] The data files of the fragment. Each data file must have the same number of rows. Each file stores a different subset of the columns. physical_rows : int The number of rows originally in this fragment. This is the number of rows in the data files before deletions. deletion_file : Optional[DeletionFile] The deletion file, if any. row_id_meta : Optional[RowIdMeta] The row id metadata, if any. num_deletions property &para; The number of rows that have been deleted from this fragment. num_rows property &para; The number of rows in this fragment after deletions. to_json() &para; Get this as a simple JSON-serializable dictionary. LanceDataset &para; Bases: Dataset A Lance Dataset in Lance format where the data is stored at the given uri. data_storage_version property &para; The version of the data storage format this dataset is using lance_schema property &para; The LanceSchema for this dataset latest_version property &para; Returns the latest version of the dataset. max_field_id property &para; The max_field_id in manifest partition_expression property &para; Not implemented (just override pyarrow dataset to prevent segfault) schema property &para; The pyarrow Schema for this dataset stats property &para; Experimental API tags property &para; Tag management for the dataset. Similar to Git, tags are a way to add metadata to a specific version of the dataset. .. warning:: Tagged versions are exempted from the :py:meth:`cleanup_old_versions()` process. To remove a version that has been tagged, you must first :py:meth:`~Tags.delete` the associated tag. Examples&para; .. code-block:: python ds = lance.open(&quot;dataset.lance&quot;) ds.tags.create(&quot;v2-prod-20250203&quot;, 10) tags = ds.tags.list() uri property &para; The location of the data version property &para; Returns the currently checked out version of the dataset add_columns(transforms, read_columns=None, reader_schema=None, batch_size=None) &para; Add new columns with defined values. There are several ways to specify the new columns. First, you can provide SQL expressions for each new column. Second you can provide a UDF that takes a batch of existing data and returns a new batch with the new columns. These new columns will be appended to the dataset. You can also provide a RecordBatchReader which will read the new column values from some external source. This is often useful when the new column values have already been staged to files (often by some distributed process) See the :func:lance.add_columns_udf decorator for more information on writing UDFs. Parameters&para; transforms : dict or AddColumnsUDF or ReaderLike If this is a dictionary, then the keys are the names of the new columns and the values are SQL expression strings. These strings can reference existing columns in the dataset. If this is a AddColumnsUDF, then it is a UDF that takes a batch of existing data and returns a new batch with the new columns. If this is :class:pyarrow.Field or :class:pyarrow.Schema, it adds all NULL columns with the given schema, in a metadata-only operation. read_columns : list of str, optional The names of the columns that the UDF will read. If None, then the UDF will read all columns. This is only used when transforms is a UDF. Otherwise, the read columns are inferred from the SQL expressions. reader_schema: pa.Schema, optional Only valid if transforms is a ReaderLike object. This will be used to determine the schema of the reader. batch_size: int, optional The number of rows to read at a time from the source dataset when applying the transform. This is ignored if the dataset is a v1 dataset. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3]}) dataset = lance.write_dataset(table, "my_dataset") @lance.batch_udf() ... def double_a(batch): ... df = batch.to_pandas() ... return pd.DataFrame({'double_a': 2 * df['a']}) dataset.add_columns(double_a) dataset.to_table().to_pandas() a double_a 0 1 2 1 2 4 2 3 6 dataset.add_columns({"triple_a": "a * 3"}) dataset.to_table().to_pandas() a double_a triple_a 0 1 2 3 1 2 4 6 2 3 6 9 See Also&para; LanceDataset.merge : Merge a pre-computed set of columns into the dataset. alter_columns(*alterations) &para; Alter column name, data type, and nullability. Columns that are renamed can keep any indices that are on them. If a column has an IVF_PQ index, it can be kept if the column is casted to another type. However, other index types don't support casting at this time. Column types can be upcasted (such as int32 to int64) or downcasted (such as int64 to int32). However, downcasting will fail if there are any values that cannot be represented in the new type. In general, columns can be casted to same general type: integers to integers, floats to floats, and strings to strings. However, strings, binary, and list columns can be casted between their size variants. For example, string to large string, binary to large binary, and list to large list. Columns that are renamed can keep any indices that are on them. However, if the column is casted to a different type, its indices will be dropped. Parameters&para; alterations : Iterable[Dict[str, Any]] A sequence of dictionaries, each with the following keys: - &quot;path&quot;: str The column path to alter. For a top-level column, this is the name. For a nested column, this is the dot-separated path, e.g. &quot;a.b.c&quot;. - &quot;name&quot;: str, optional The new name of the column. If not specified, the column name is not changed. - &quot;nullable&quot;: bool, optional Whether the column should be nullable. If not specified, the column nullability is not changed. Only non-nullable columns can be changed to nullable. Currently, you cannot change a nullable column to non-nullable. - &quot;data_type&quot;: pyarrow.DataType, optional The new data type to cast the column to. If not specified, the column data type is not changed. Examples&para; import lance import pyarrow as pa schema = pa.schema([pa.field('a', pa.int64()), ... pa.field('b', pa.string(), nullable=False)]) table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.alter_columns({"path": "a", "name": "x"}, ... {"path": "b", "nullable": True}) dataset.to_table().to_pandas() x b 0 1 a 1 2 b 2 3 c dataset.alter_columns({"path": "x", "data_type": pa.int32()}) dataset.schema x: int32 b: string checkout_version(version) &para; Load the given version of the dataset. Unlike the :func:dataset constructor, this will re-use the current cache. This is a no-op if the dataset is already at the given version. Parameters&para; version: int | str, The version to check out. A version number (int) or a tag (str) can be provided. Returns&para; LanceDataset cleanup_old_versions(older_than=None, *, delete_unverified=False, error_if_tagged_old_versions=True) &para; Cleans up old versions of the dataset. Some dataset changes, such as overwriting, leave behind data that is not referenced by the latest dataset version. The old data is left in place to allow the dataset to be restored back to an older version. This method will remove older versions and any data files they reference. Once this cleanup task has run you will not be able to checkout or restore these older versions. Parameters&para; timedelta, optional Only versions older than this will be removed. If not specified, this will default to two weeks. bool, default False Files leftover from a failed transaction may appear to be part of an in-progress operation (e.g. appending new data) and these files will not be deleted unless they are at least 7 days old. If delete_unverified is True then these files will be deleted regardless of their age. This should only be set to True if you can guarantee that no other process is currently working on this dataset. Otherwise the dataset could be put into a corrupted state. bool, default True Some versions may have tags associated with them. Tagged versions will not be cleaned up, regardless of how old they are. If this argument is set to True (the default), an exception will be raised if any tagged versions match the parameters. Otherwise, tagged versions will be ignored without any error and only untagged versions will be cleaned up. commit(base_uri, operation, blobs_op=None, read_version=None, commit_lock=None, storage_options=None, enable_v2_manifest_paths=None, detached=False, max_retries=20) staticmethod &para; Create a new version of dataset This method is an advanced method which allows users to describe a change that has been made to the data files. This method is not needed when using Lance to apply changes (e.g. when using class:LanceDataset or func:write_dataset.) It's current purpose is to allow for changes being made in a distributed environment where no single process is doing all of the work. For example, a distributed bulk update or a distributed bulk modify operation. Once all of the changes have been made, this method can be called to make the changes visible by updating the dataset manifest. Warnings&para; This is an advanced API and doesn't provide the same level of validation as the other APIs. For example, it's the responsibility of the caller to ensure that the fragments are valid for the schema. Parameters&para; base_uri: str, Path, or LanceDataset The base uri of the dataset, or the dataset object itself. Using the dataset object can be more efficient because it can re-use the file metadata cache. operation: BaseOperation The operation to apply to the dataset. This describes what changes have been made. See available operations under :class:LanceOperation. read_version: int, optional The version of the dataset that was used as the base for the changes. This is not needed for overwrite or restore operations. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:migrate_manifest_paths_v2 method. Default is False. WARNING: turning this on will make the dataset unreadable for older versions of Lance (prior to 0.17.0). detached : bool, optional If True, then the commit will not be part of the dataset lineage. It will never show up as the latest dataset and the only way to check it out in the future will be to specifically check it out by version. The version will be a random version that is only unique amongst detached commits. The caller should store this somewhere as there will be no other way to obtain it in the future. max_retries : int The maximum number of retries to perform when committing the dataset. Returns&para; LanceDataset A new version of Lance Dataset. Examples&para; Creating a new dataset with the :class:LanceOperation.Overwrite operation: import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment1 = lance.fragment.LanceFragment.create("example", tab1) fragment2 = lance.fragment.LanceFragment.create("example", tab2) fragments = [fragment1, fragment2] operation = lance.LanceOperation.Overwrite(tab1.schema, fragments) dataset = lance.LanceDataset.commit("example", operation) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d commit_batch(dest, transactions, commit_lock=None, storage_options=None, enable_v2_manifest_paths=None, detached=False, max_retries=20) staticmethod &para; Create a new version of dataset with multiple transactions. This method is an advanced method which allows users to describe a change that has been made to the data files. This method is not needed when using Lance to apply changes (e.g. when using class:LanceDataset or func:write_dataset.) Parameters&para; dest: str, Path, or LanceDataset The base uri of the dataset, or the dataset object itself. Using the dataset object can be more efficient because it can re-use the file metadata cache. transactions: Iterable[Transaction] The transactions to apply to the dataset. These will be merged into a single transaction and applied to the dataset. Note: Only append transactions are currently supported. Other transaction types will be supported in the future. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:migrate_manifest_paths_v2 method. Default is False. WARNING: turning this on will make the dataset unreadable for older versions of Lance (prior to 0.17.0). detached : bool, optional If True, then the commit will not be part of the dataset lineage. It will never show up as the latest dataset and the only way to check it out in the future will be to specifically check it out by version. The version will be a random version that is only unique amongst detached commits. The caller should store this somewhere as there will be no other way to obtain it in the future. max_retries : int The maximum number of retries to perform when committing the dataset. Returns&para; dict with keys: dataset: LanceDataset A new version of Lance Dataset. merged: Transaction The merged transaction that was applied to the dataset. count_rows(filter=None, **kwargs) &para; Count rows matching the scanner filter. Parameters&para; **kwargs : dict, optional See py:method:scanner method for full parameter description. Returns&para; count : int The total number of rows in the dataset. create_index(column, index_type, name=None, metric=&#39;L2&#39;, replace=False, num_partitions=None, ivf_centroids=None, pq_codebook=None, num_sub_vectors=None, accelerator=None, index_cache_size=None, shuffle_partition_batches=None, shuffle_partition_concurrency=None, ivf_centroids_file=None, precomputed_partition_dataset=None, storage_options=None, filter_nan=True, one_pass_ivfpq=False, **kwargs) &para; Create index on column. Experimental API Parameters&para; column : str The column to be indexed. index_type : str The type of the index. "IVF_PQ, IVF_HNSW_PQ and IVF_HNSW_SQ" are supported now. name : str, optional The index name. If not provided, it will be generated from the column name. metric : str The distance metric type, i.e., "L2" (alias to "euclidean"), "cosine" or "dot" (dot product). Default is "L2". replace : bool Replace the existing index if it exists. num_partitions : int, optional The number of partitions of IVF (Inverted File Index). ivf_centroids : optional It can be either class:np.ndarray, class:pyarrow.FixedSizeListArray or class:pyarrow.FixedShapeTensorArray. A num_partitions x dimension array of existing K-mean centroids for IVF clustering. If not provided, a new KMeans model will be trained. pq_codebook : optional, It can be class:np.ndarray, class:pyarrow.FixedSizeListArray, or class:pyarrow.FixedShapeTensorArray. A num_sub_vectors x (2 ^ nbits * dimensions // num_sub_vectors) array of K-mean centroids for PQ codebook. Note: ``nbits`` is always 8 for now. If not provided, a new PQ model will be trained. num_sub_vectors : int, optional The number of sub-vectors for PQ (Product Quantization). accelerator : str or torch.Device, optional If set, use an accelerator to speed up the training process. Accepted accelerator: "cuda" (Nvidia GPU) and "mps" (Apple Silicon GPU). If not set, use the CPU. index_cache_size : int, optional The size of the index cache in number of entries. Default value is 256. shuffle_partition_batches : int, optional The number of batches, using the row group size of the dataset, to include in each shuffle partition. Default value is 10240. Assuming the row group size is 1024, each shuffle partition will hold 10240 * 1024 = 10,485,760 rows. By making this value smaller, this shuffle will consume less memory but will take longer to complete, and vice versa. shuffle_partition_concurrency : int, optional The number of shuffle partitions to process concurrently. Default value is 2 By making this value smaller, this shuffle will consume less memory but will take longer to complete, and vice versa. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. filter_nan: bool Defaults to True. False is UNSAFE, and will cause a crash if any null/nan values are present (and otherwise will not). Disables the null filter used for nullable columns. Obtains a small speed boost. one_pass_ivfpq: bool Defaults to False. If enabled, index type must be "IVF_PQ". Reduces disk IO. kwargs : Parameters passed to the index building process. The SQ (Scalar Quantization) is available for only IVF_HNSW_SQ index type, this quantization method is used to reduce the memory usage of the index, it maps the float vectors to integer vectors, each integer is of num_bits, now only 8 bits are supported. If index_type is "IVF_*", then the following parameters are required: num_partitions If index_type is with "PQ", then the following parameters are required: num_sub_vectors Optional parameters for IVF_PQ: - ivf_centroids Existing K-mean centroids for IVF clustering. - num_bits The number of bits for PQ (Product Quantization). Default is 8. Only 4, 8 are supported. Optional parameters for IVF_HNSW_*: max_level Int, the maximum number of levels in the graph. m Int, the number of edges per node in the graph. ef_construction Int, the number of nodes to examine during the construction. Examples&para; .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_PQ&quot;, num_partitions=256, num_sub_vectors=16 ) .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_HNSW_SQ&quot;, num_partitions=256, ) Experimental Accelerator (GPU) support: accelerate: use GPU to train IVF partitions. Only supports CUDA (Nvidia) or MPS (Apple) currently. Requires PyTorch being installed. .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_PQ&quot;, num_partitions=256, num_sub_vectors=16, accelerator=&quot;cuda&quot; ) References&para; Faiss Index &lt;https://github.com/facebookresearch/faiss/wiki/Faiss-indexes&gt;_ IVF introduced in Video Google: a text retrieval approach to object matching in videos &lt;https://ieeexplore.ieee.org/abstract/document/1238663&gt;_ Product quantization for nearest neighbor search &lt;https://hal.inria.fr/inria-00514462v2/document&gt;_ create_scalar_index(column, index_type, name=None, *, replace=True, **kwargs) &para; Create a scalar index on a column. Scalar indices, like vector indices, can be used to speed up scans. A scalar index can speed up scans that contain filter expressions on the indexed column. For example, the following scan will be faster if the column my_col has a scalar index: .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) my_table = dataset.scanner(filter=&quot;my_col != 7&quot;).to_table() Vector search with pre-filers can also benefit from scalar indices. For example, .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) my_table = dataset.scanner( nearest=dict( column=&quot;vector&quot;, q=[1, 2, 3, 4], k=10, ) filter=&quot;my_col != 7&quot;, prefilter=True ) There are 5 types of scalar indices available today. BTREE. The most common type is BTREE. This index is inspired by the btree data structure although only the first few layers of the btree are cached in memory. It will perform well on columns with a large number of unique values and few rows per value. BITMAP. This index stores a bitmap for each unique value in the column. This index is useful for columns with a small number of unique values and many rows per value. LABEL_LIST. A special index that is used to index list columns whose values have small cardinality. For example, a column that contains lists of tags (e.g. ["tag1", "tag2", "tag3"]) can be indexed with a LABEL_LIST index. This index can only speedup queries with array_has_any or array_has_all filters. NGRAM. A special index that is used to index string columns. This index creates a bitmap for each ngram in the string. By default we use trigrams. This index can currently speed up queries using the contains function in filters. FTS/INVERTED. It is used to index document columns. This index can conduct full-text searches. For example, a column that contains any word of query string "hello world". The results will be ranked by BM25. Note that the LANCE_BYPASS_SPILLING environment variable can be used to bypass spilling to disk. Setting this to true can avoid memory exhaustion issues (see https://github.com/apache/datafusion/issues/10073 for more info). Experimental API Parameters&para; column : str The column to be indexed. Must be a boolean, integer, float, or string column. index_type : str The type of the index. One of "BTREE", "BITMAP", "LABEL_LIST", "NGRAM", "FTS" or "INVERTED". name : str, optional The index name. If not provided, it will be generated from the column name. replace : bool, default True Replace the existing index if it exists. bool, default True This is for the INVERTED index. If True, the index will store the positions of the words in the document, so that you can conduct phrase query. This will significantly increase the index size. It won't impact the performance of non-phrase queries even if it is set to True. base_tokenizer: str, default "simple" This is for the INVERTED index. The base tokenizer to use. The value can be: * "simple": splits tokens on whitespace and punctuation. * "whitespace": splits tokens on whitespace. * "raw": no tokenization. language: str, default "English" This is for the INVERTED index. The language for stemming and stop words. This is only used when stem or remove_stop_words is true max_token_length: Optional[int], default 40 This is for the INVERTED index. The maximum token length. Any token longer than this will be removed. lower_case: bool, default True This is for the INVERTED index. If True, the index will convert all text to lowercase. stem: bool, default False This is for the INVERTED index. If True, the index will stem the tokens. remove_stop_words: bool, default False This is for the INVERTED index. If True, the index will remove stop words. ascii_folding: bool, default False This is for the INVERTED index. If True, the index will convert non-ascii characters to ascii characters if possible. This would remove accents like "é" -&gt; "e". Examples&para; .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) dataset.create_index( &quot;category&quot;, &quot;BTREE&quot;, ) Scalar indices can only speed up scans for basic filters using equality, comparison, range (e.g. my_col BETWEEN 0 AND 100), and set membership (e.g. my_col IN (0, 1, 2)) Scalar indices can be used if the filter contains multiple indexed columns and the filter criteria are AND'd or OR'd together (e.g. my_col &lt; 0 AND other_col&gt; 100) Scalar indices may be used if the filter contains non-indexed columns but, depending on the structure of the filter, they may not be usable. For example, if the column not_indexed does not have a scalar index then the filter my_col = 0 OR not_indexed = 1 will not be able to use any scalar index on my_col. To determine if a scan is making use of a scalar index you can use explain_plan to look at the query plan that lance has created. Queries that use scalar indices will either have a ScalarIndexQuery relation or a MaterializeIndex operator. delete(predicate) &para; Delete rows from the dataset. This marks rows as deleted, but does not physically remove them from the files. This keeps the existing indexes still valid. Parameters&para; predicate : str or pa.compute.Expression The predicate to use to select rows to delete. May either be a SQL string or a pyarrow Expression. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.delete("a = 1 or b in ('a', 'b')") dataset.to_table() pyarrow.Table a: int64 b: string a: [[3]] b: [["c"]] drop_columns(columns) &para; Drop one or more columns from the dataset This is a metadata-only operation and does not remove the data from the underlying storage. In order to remove the data, you must subsequently call compact_files to rewrite the data without the removed columns and then call cleanup_old_versions to remove the old files. Parameters&para; columns : list of str The names of the columns to drop. These can be nested column references (e.g. "a.b.c") or top-level column names (e.g. "a"). Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.drop_columns(["a"]) dataset.to_table().to_pandas() b 0 a 1 b 2 c drop_index(name) &para; Drops an index from the dataset Note: Indices are dropped by "index name". This is not the same as the field name. If you did not specify a name when you created the index then a name was generated for you. You can use the list_indices method to get the names of the indices. get_fragment(fragment_id) &para; Get the fragment with fragment id. get_fragments(filter=None) &para; Get all fragments from the dataset. Note: filter is not supported yet. head(num_rows, **kwargs) &para; Load the first N rows of the dataset. Parameters&para; num_rows : int The number of rows to load. **kwargs : dict, optional See scanner() method for full parameter description. Returns&para; table : Table insert(data, *, mode=&#39;append&#39;, **kwargs) &para; Insert data into the dataset. Parameters&para; data_obj: Reader-like The data to be written. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader - Huggingface dataset mode: str, default 'append' The mode to use when writing the data. Options are: create - create a new dataset (raises if uri already exists). overwrite - create a new snapshot version append - create a new version that is the concat of the input the latest version (raises if uri does not exist) **kwargs : dict, optional Additional keyword arguments to pass to :func:write_dataset. join(right_dataset, keys, right_keys=None, join_type=&#39;left outer&#39;, left_suffix=None, right_suffix=None, coalesce_keys=True, use_threads=True) &para; Not implemented (just override pyarrow dataset to prevent segfault) merge(data_obj, left_on, right_on=None, schema=None) &para; Merge another dataset into this one. Performs a left join, where the dataset is the left side and data_obj is the right side. Rows existing in the dataset but not on the left will be filled with null values, unless Lance doesn't support null values for some types, in which case an error will be raised. Parameters&para; data_obj: Reader-like The data to be merged. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, Iterator[RecordBatch], or RecordBatchReader left_on: str The name of the column in the dataset to join on. right_on: str or None The name of the column in data_obj to join on. If None, defaults to left_on. Examples&para; import lance import pyarrow as pa df = pa.table({'x': [1, 2, 3], 'y': ['a', 'b', 'c']}) dataset = lance.write_dataset(df, "dataset") dataset.to_table().to_pandas() x y 0 1 a 1 2 b 2 3 c new_df = pa.table({'x': [1, 2, 3], 'z': ['d', 'e', 'f']}) dataset.merge(new_df, 'x') dataset.to_table().to_pandas() x y z 0 1 a d 1 2 b e 2 3 c f See Also&para; LanceDataset.add_columns : Add new columns by computing batch-by-batch. merge_insert(on) &para; Returns a builder that can be used to create a "merge insert" operation This operation can add rows, update rows, and remove rows in a single transaction. It is a very generic tool that can be used to create behaviors like "insert if not exists", "update or insert (i.e. upsert)", or even replace a portion of existing data with new data (e.g. replace all data where month="january") The merge insert operation works by combining new data from a source table with existing data in a target table by using a join. There are three categories of records. "Matched" records are records that exist in both the source table and the target table. "Not matched" records exist only in the source table (e.g. these are new data). "Not matched by source" records exist only in the target table (this is old data). The builder returned by this method can be used to customize what should happen for each category of data. Please note that the data will be reordered as part of this operation. This is because updated rows will be deleted from the dataset and then reinserted at the end with the new values. The order of the newly inserted rows may fluctuate randomly because a hash-join operation is used internally. Parameters&para; Union[str, Iterable[str]] A column (or columns) to join on. This is how records from the source table and target table are matched. Typically this is some kind of key or id column. Examples&para; Use when_matched_update_all() and when_not_matched_insert_all() to perform an "upsert" operation. This will update rows that already exist in the dataset and insert rows that do not exist. import lance import pyarrow as pa table = pa.table({"a": [2, 1, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform a "upsert" operation&para; dataset.merge_insert("a") \ ... .when_matched_update_all() \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b 0 1 b 1 2 x 2 3 y 3 4 z Use when_not_matched_insert_all() to perform an "insert if not exists" operation. This will only insert rows that do not already exist in the dataset. import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example2") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform an "insert if not exists" operation&para; dataset.merge_insert("a") \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 0, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 z You are not required to provide all the columns. If you only want to update a subset of columns, you can omit columns you don't want to update. Omitted columns will keep their existing values if they are updated, or will be null if they are inserted. import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"], \ ... "c": ["x", "y", "z"]}) dataset = lance.write_dataset(table, "example3") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform an "upsert" operation, only updating column "a"&para; dataset.merge_insert("a") \ ... .when_matched_update_all() \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b c 0 1 a x 1 2 x y 2 3 y z 3 4 z None migrate_manifest_paths_v2() &para; Migrate the manifest paths to the new format. This will update the manifest to use the new v2 format for paths. This function is idempotent, and can be run multiple times without changing the state of the object store. DANGER: this should not be run while other concurrent operations are happening. And it should also run until completion before resuming other operations. prewarm_index(name) &para; Prewarm an index This will load the entire index into memory. This can help avoid cold start issues with index queries. If the index does not fit in the index cache, then this will result in wasted I/O. Parameters&para; name: str The name of the index to prewarm. replace_field_metadata(field_name, new_metadata) &para; Replace the metadata of a field in the schema Parameters&para; field_name: str The name of the field to replace the metadata for new_metadata: dict The new metadata to set replace_schema(schema) &para; Not implemented (just override pyarrow dataset to prevent segfault) See method:replace_schema_metadata or method:replace_field_metadata replace_schema_metadata(new_metadata) &para; Replace the schema metadata of the dataset Parameters&para; new_metadata: dict The new metadata to set restore() &para; Restore the currently checked out version as the latest version of the dataset. This creates a new commit. sample(num_rows, columns=None, randomize_order=True, **kwargs) &para; Select a random sample of data Parameters&para; num_rows: int number of rows to retrieve columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. **kwargs : dict, optional see scanner() method for full parameter description. Returns&para; table : Table scanner(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, fragments=None, full_text_query=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, fast_search=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, include_deleted_rows=None, scan_stats_callback=None, strict_batch_size=None) &para; Return a Scanner that can support various pushdowns. Parameters&para; columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. filter: pa.compute.Expression or str Expression or str that is a valid SQL where clause. See Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;_ for valid SQL expressions. limit: int, default None Fetch up to this many rows. All rows if None or unspecified. offset: int, default None Fetch starting with this row. 0 if None or unspecified. nearest: dict, default None Get the rows corresponding to the K most similar vectors. Example: .. code-block:: python { &quot;column&quot;: &lt;embedding col name&gt;, &quot;q&quot;: &lt;query vector as pa.Float32Array&gt;, &quot;k&quot;: 10, &quot;nprobes&quot;: 1, &quot;refine_factor&quot;: 1 } int, default None The target size of batches returned. In some cases batches can be up to twice this size (but never larger than this). In some cases batches can be smaller than this size. io_buffer_size: int, default None The size of the IO buffer. See ScannerBuilder.io_buffer_size for more information. batch_readahead: int, optional The number of batches to read ahead. fragment_readahead: int, optional The number of fragments to read ahead. scan_in_order: bool, default True Whether to read the fragments and batches in order. If false, throughput may be higher, but batches will be returned out of order and memory use might increase. fragments: iterable of LanceFragment, default None If specified, only scan these fragments. If scan_in_order is True, then the fragments will be scanned in the order given. prefilter: bool, default False If True then the filter will be applied before the vector query is run. This will generate more correct results but it may be a more costly query. It's generally good when the filter is highly selective. If False then the filter will be applied after the vector query is run. This will perform well but the results may have fewer than the requested number of rows (or be empty) if the rows closest to the query do not match the filter. It&#39;s generally good when the filter is not very selective. use_scalar_index: bool, default True Lance will automatically use scalar indices to optimize a query. In some corner cases this can make query performance worse and this parameter can be used to disable scalar indices in these cases. late_materialization: bool or List[str], default None Allows custom control over late materialization. Late materialization fetches non-query columns using a take operation after the filter. This is useful when there are few results or columns are very large. Early materialization can be better when there are many results or the columns are very narrow. If True, then all columns are late materialized. If False, then all columns are early materialized. If a list of strings, then only the columns in the list are late materialized. The default uses a heuristic that assumes filters will select about 0.1% of the rows. If your filter is more selective (e.g. find by id) you may want to set this to True. If your filter is not very selective (e.g. matches 20% of the rows) you may want to set this to False. full_text_query: str or dict, optional query string to search for, the results will be ranked by BM25. e.g. "hello world", would match documents containing "hello" or "world". or a dictionary with the following keys: - columns: list[str] The columns to search, currently only supports a single column in the columns list. - query: str The query string to search for. fast_search: bool, default False If True, then the search will only be performed on the indexed data, which yields faster search time. scan_stats_callback: Callable[[ScanStatistics], None], default None A callback function that will be called with the scan statistics after the scan is complete. Errors raised by the callback will be logged but not re-raised. include_deleted_rows: bool, default False If True, then rows that have been deleted, but are still present in the fragment, will be returned. These rows will have the _rowid column set to null. All other columns will reflect the value stored on disk and may not be null. Note: if this is a search operation, or a take operation (including scalar indexed scans) then deleted rows cannot be returned. .. note:: For now, if BOTH filter and nearest is specified, then: 1. nearest is executed first. 2. The results are filtered afterwards. For debugging ANN results, you can choose to not use the index even if present by specifying use_index=False. For example, the following will always return exact KNN results: .. code-block:: python dataset.to_table(nearest={ &quot;column&quot;: &quot;vector&quot;, &quot;k&quot;: 10, &quot;q&quot;: &lt;query vector&gt;, &quot;use_index&quot;: False } session() &para; Return the dataset session, which holds the dataset's state. take(indices, columns=None) &para; Select rows of data by index. Parameters&para; indices : Array or array-like indices of rows to select in the dataset. columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. Returns&para; table : pyarrow.Table take_blobs(blob_column, ids=None, addresses=None, indices=None) &para; Select blobs by row IDs. Instead of loading large binary blob data into memory before processing it, this API allows you to open binary blob data as a regular Python file-like object. For more details, see class:lance.BlobFile. Exactly one of ids, addresses, or indices must be specified. Parameters blob_column : str The name of the blob column to select. ids : Integer Array or array-like row IDs to select in the dataset. addresses: Integer Array or array-like The (unstable) row addresses to select in the dataset. indices : Integer Array or array-like The offset / indices of the row in the dataset. Returns&para; blob_files : List[BlobFile] to_batches(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, full_text_query=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, strict_batch_size=None, **kwargs) &para; Read the dataset as materialized record batches. Parameters&para; **kwargs : dict, optional Arguments for meth:~LanceDataset.scanner. Returns&para; record_batches : Iterator of class:~pyarrow.RecordBatch to_table(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, fast_search=None, full_text_query=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, include_deleted_rows=None) &para; Read the data into memory as a class:pyarrow.Table Parameters&para; columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. filter : pa.compute.Expression or str Expression or str that is a valid SQL where clause. See Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;_ for valid SQL expressions. limit: int, default None Fetch up to this many rows. All rows if None or unspecified. offset: int, default None Fetch starting with this row. 0 if None or unspecified. nearest: dict, default None Get the rows corresponding to the K most similar vectors. Example: .. code-block:: python { &quot;column&quot;: &lt;embedding col name&gt;, &quot;q&quot;: &lt;query vector as pa.Float32Array&gt;, &quot;k&quot;: 10, &quot;metric&quot;: &quot;cosine&quot;, &quot;nprobes&quot;: 1, &quot;refine_factor&quot;: 1 } int, optional The number of rows to read at a time. io_buffer_size: int, default None The size of the IO buffer. See ScannerBuilder.io_buffer_size for more information. batch_readahead: int, optional The number of batches to read ahead. fragment_readahead: int, optional The number of fragments to read ahead. scan_in_order: bool, optional, default True Whether to read the fragments and batches in order. If false, throughput may be higher, but batches will be returned out of order and memory use might increase. prefilter: bool, optional, default False Run filter before the vector search. late_materialization: bool or List[str], default None Allows custom control over late materialization. See ScannerBuilder.late_materialization for more information. use_scalar_index: bool, default True Allows custom control over scalar index usage. See ScannerBuilder.use_scalar_index for more information. with_row_id: bool, optional, default False Return row ID. with_row_address: bool, optional, default False Return row address use_stats: bool, optional, default True Use stats pushdown during filters. fast_search: bool, optional, default False full_text_query: str or dict, optional query string to search for, the results will be ranked by BM25. e.g. "hello world", would match documents contains "hello" or "world". or a dictionary with the following keys: - columns: list[str] The columns to search, currently only supports a single column in the columns list. - query: str The query string to search for. include_deleted_rows: bool, optional, default False If True, then rows that have been deleted, but are still present in the fragment, will be returned. These rows will have the _rowid column set to null. All other columns will reflect the value stored on disk and may not be null. Note: if this is a search operation, or a take operation (including scalar indexed scans) then deleted rows cannot be returned. Notes&para; If BOTH filter and nearest is specified, then: nearest is executed first. The results are filtered afterward, unless pre-filter sets to True. update(updates, where=None) &para; Update column values for rows matching where. Parameters&para; updates : dict of str to str A mapping of column names to a SQL expression. where : str, optional A SQL predicate indicating which rows should be updated. Returns&para; updates : dict A dictionary containing the number of rows updated. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") update_stats = dataset.update(dict(a = 'a + 2'), where="b != 'a'") update_stats["num_updated_rows"] = 2 dataset.to_table().to_pandas() a b 0 1 a 1 4 b 2 5 c validate() &para; Validate the dataset. This checks the integrity of the dataset and will raise an exception if the dataset is corrupted. versions() &para; Return all versions in this dataset. LanceFragment &para; Bases: Fragment metadata property &para; Return the metadata of this fragment. Returns&para; FragmentMetadata num_deletions property &para; Return the number of deleted rows in this fragment. physical_rows property &para; Return the number of rows originally in this fragment. To get the number of rows after deletions, use :meth:count_rows instead. schema property &para; Return the schema of this fragment. create(dataset_uri, data, fragment_id=None, schema=None, max_rows_per_group=1024, progress=None, mode=&#39;append&#39;, *, data_storage_version=None, use_legacy_format=None, storage_options=None) staticmethod &para; Create a :class:FragmentMetadata from the given data. This can be used if the dataset is not yet created. .. warning:: Internal API. This method is not intended to be used by end users. Parameters&para; dataset_uri: str The URI of the dataset. fragment_id: int The ID of the fragment. data: pa.Table or pa.RecordBatchReader The data to be written to the fragment. schema: pa.Schema, optional The schema of the data. If not specified, the schema will be inferred from the data. max_rows_per_group: int, default 1024 The maximum number of rows per group in the data file. progress: FragmentWriteProgress, optional Experimental API. Progress tracking for writing the fragment. Pass a custom class that defines hooks to be called when each fragment is starting to write and finishing writing. mode: str, default "append" The write mode. If "append" is specified, the data will be checked against the existing dataset's schema. Otherwise, pass "create" or "overwrite" to assign new field ids to the schema. data_storage_version: optional, str, default None The version of the data storage format to use. Newer versions are more efficient but require newer versions of lance to read. The default (None) will use the latest stable version. See the user guide for more details. use_legacy_format: bool, default None Deprecated parameter. Use data_storage_version instead. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. See Also&para; lance.dataset.LanceOperation.Overwrite : The operation used to create a new dataset or overwrite one using fragments created with this API. See the doc page for an example of using this API. lance.dataset.LanceOperation.Append : The operation used to append fragments created with this API to an existing dataset. See the doc page for an example of using this API. Returns&para; FragmentMetadata create_from_file(filename, dataset, fragment_id) staticmethod &para; Create a fragment from the given datafile uri. This can be used if the datafile is loss from dataset. .. warning:: Internal API. This method is not intended to be used by end users. Parameters&para; filename: str The filename of the datafile. dataset: LanceDataset The dataset that the fragment belongs to. fragment_id: int The ID of the fragment. data_files() &para; Return the data files of this fragment. delete(predicate) &para; Delete rows from this Fragment. This will add or update the deletion file of this fragment. It does not modify or delete the data files of this fragment. If no rows are left after the deletion, this method will return None. .. warning:: Internal API. This method is not intended to be used by end users. Parameters&para; predicate: str A SQL predicate that specifies the rows to delete. Returns&para; FragmentMetadata or None A new fragment containing the new deletion file, or None if no rows left. Examples&para; import lance import pyarrow as pa tab = pa.table({"a": [1, 2, 3], "b": [4, 5, 6]}) dataset = lance.write_dataset(tab, "dataset") frag = dataset.get_fragment(0) frag.delete("a &gt; 1") FragmentMetadata(id=0, files=[DataFile(path='...', fields=[0, 1], ...), ...) frag.delete("a &gt; 0") is None True See Also&para; lance.dataset.LanceOperation.Delete : The operation used to commit these changes to a dataset. See the doc page for an example of using this API. deletion_file() &para; Return the deletion file, if any merge(data_obj, left_on, right_on=None, schema=None) &para; Merge another dataset into this fragment. Performs a left join, where the fragment is the left side and data_obj is the right side. Rows existing in the dataset but not on the left will be filled with null values, unless Lance doesn't support null values for some types, in which case an error will be raised. Parameters&para; data_obj: Reader-like The data to be merged. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, Iterator[RecordBatch], or RecordBatchReader left_on: str The name of the column in the dataset to join on. right_on: str or None The name of the column in data_obj to join on. If None, defaults to left_on. Examples&para; import lance import pyarrow as pa df = pa.table({'x': [1, 2, 3], 'y': ['a', 'b', 'c']}) dataset = lance.write_dataset(df, "dataset") dataset.to_table().to_pandas() x y 0 1 a 1 2 b 2 3 c fragments = dataset.get_fragments() new_df = pa.table({'x': [1, 2, 3], 'z': ['d', 'e', 'f']}) merged = [] schema = None for f in fragments: ... f, schema = f.merge(new_df, 'x') ... merged.append(f) merge = lance.LanceOperation.Merge(merged, schema) dataset = lance.LanceDataset.commit("dataset", merge, read_version=1) dataset.to_table().to_pandas() x y z 0 1 a d 1 2 b e 2 3 c f See Also&para; LanceDataset.merge_columns : Add columns to this Fragment. Returns&para; Tuple[FragmentMetadata, LanceSchema] A new fragment with the merged column(s) and the final schema. merge_columns(value_func, columns=None, batch_size=None, reader_schema=None) &para; Add columns to this Fragment. .. warning:: Internal API. This method is not intended to be used by end users. The parameters and their interpretation are the same as in the :meth:lance.dataset.LanceDataset.add_columns operation. The only difference is that, instead of modifying the dataset, a new fragment is created. The new schema of the fragment is returned as well. These can be used in a later operation to commit the changes to the dataset. See Also&para; lance.dataset.LanceOperation.Merge : The operation used to commit these changes to the dataset. See the doc page for an example of using this API. Returns&para; Tuple[FragmentMetadata, LanceSchema] A new fragment with the added column(s) and the final schema. scanner(*, columns=None, batch_size=None, filter=None, limit=None, offset=None, with_row_id=False, with_row_address=False, batch_readahead=16) &para; See Dataset::scanner for details LanceOperation &para; Append dataclass &para; Bases: BaseOperation Append new rows to the dataset. Attributes&para; fragments: list[FragmentMetadata] The fragments that contain the new rows. Warning&para; This is an advanced API for distributed operations. To append to a dataset on a single machine, use :func:lance.write_dataset. Examples&para; To append new rows to a dataset, first use :meth:lance.fragment.LanceFragment.create to create fragments. Then collect the fragment metadata into a list and pass it to this class. Finally, pass the operation to the :meth:LanceDataset.commit method to create the new dataset. import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) dataset = lance.write_dataset(tab1, "example") tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment = lance.fragment.LanceFragment.create("example", tab2) operation = lance.LanceOperation.Append([fragment]) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d BaseOperation &para; Bases: ABC Base class for operations that can be applied to a dataset. See available operations under :class:LanceOperation. CreateIndex dataclass &para; Bases: BaseOperation Operation that creates an index on the dataset. DataReplacement dataclass &para; Bases: BaseOperation Operation that replaces existing datafiles in the dataset. DataReplacementGroup dataclass &para; Group of data replacements Delete dataclass &para; Bases: BaseOperation Remove fragments or rows from the dataset. Attributes&para; updated_fragments: list[FragmentMetadata] The fragments that have been updated with new deletion vectors. deleted_fragment_ids: list[int] The ids of the fragments that have been deleted entirely. These are the fragments where :meth:LanceFragment.delete() returned None. predicate: str The original SQL predicate used to select the rows to delete. Warning&para; This is an advanced API for distributed operations. To delete rows from dataset on a single machine, use :meth:lance.LanceDataset.delete. Examples&para; To delete rows from a dataset, call :meth:lance.fragment.LanceFragment.delete on each of the fragments. If that returns a new fragment, add that to the updated_fragments list. If it returns None, that means the whole fragment was deleted, so add the fragment id to the deleted_fragment_ids. Finally, pass the operation to the :meth:LanceDataset.commit method to complete the deletion operation. import lance import pyarrow as pa table = pa.table({"a": [1, 2], "b": ["a", "b"]}) dataset = lance.write_dataset(table, "example") table = pa.table({"a": [3, 4], "b": ["c", "d"]}) dataset = lance.write_dataset(table, "example", mode="append") dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d predicate = "a &gt;= 2" updated_fragments = [] deleted_fragment_ids = [] for fragment in dataset.get_fragments(): ... new_fragment = fragment.delete(predicate) ... if new_fragment is not None: ... updated_fragments.append(new_fragment) ... else: ... deleted_fragment_ids.append(fragment.fragment_id) operation = lance.LanceOperation.Delete(updated_fragments, ... deleted_fragment_ids, ... predicate) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b 0 1 a Merge dataclass &para; Bases: BaseOperation Operation that adds columns. Unlike Overwrite, this should not change the structure of the fragments, allowing existing indices to be kept. Attributes&para; fragments: iterable of FragmentMetadata The fragments that make up the new dataset. schema: LanceSchema or pyarrow.Schema The schema of the new dataset. Passing a LanceSchema is preferred, and passing a pyarrow.Schema is deprecated. Warning&para; This is an advanced API for distributed operations. To overwrite or create new dataset on a single machine, use :func:lance.write_dataset. Examples&para; To add new columns to a dataset, first define a method that will create the new columns based on the existing columns. Then use :meth:lance.fragment.LanceFragment.add_columns import lance import pyarrow as pa import pyarrow.compute as pc table = pa.table({"a": [1, 2, 3, 4], "b": ["a", "b", "c", "d"]}) dataset = lance.write_dataset(table, "example") dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d def double_a(batch: pa.RecordBatch) -&gt; pa.RecordBatch: ... doubled = pc.multiply(batch["a"], 2) ... return pa.record_batch([doubled], ["a_doubled"]) fragments = [] for fragment in dataset.get_fragments(): ... new_fragment, new_schema = fragment.merge_columns(double_a, ... columns=['a']) ... fragments.append(new_fragment) operation = lance.LanceOperation.Merge(fragments, new_schema) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b a_doubled 0 1 a 2 1 2 b 4 2 3 c 6 3 4 d 8 Overwrite dataclass &para; Bases: BaseOperation Overwrite or create a new dataset. Attributes&para; new_schema: pyarrow.Schema The schema of the new dataset. fragments: list[FragmentMetadata] The fragments that make up the new dataset. Warning&para; This is an advanced API for distributed operations. To overwrite or create new dataset on a single machine, use :func:lance.write_dataset. Examples&para; To create or overwrite a dataset, first use :meth:lance.fragment.LanceFragment.create to create fragments. Then collect the fragment metadata into a list and pass it along with the schema to this class. Finally, pass the operation to the :meth:LanceDataset.commit method to create the new dataset. import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment1 = lance.fragment.LanceFragment.create("example", tab1) fragment2 = lance.fragment.LanceFragment.create("example", tab2) fragments = [fragment1, fragment2] operation = lance.LanceOperation.Overwrite(tab1.schema, fragments) dataset = lance.LanceDataset.commit("example", operation) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d Project dataclass &para; Bases: BaseOperation Operation that project columns. Use this operator for drop column or rename/swap column. Attributes&para; schema: LanceSchema The lance schema of the new dataset. Examples&para; Use the projece operator to swap column: import lance import pyarrow as pa import pyarrow.compute as pc from lance.schema import LanceSchema table = pa.table({"a": [1, 2], "b": ["a", "b"], "b1": ["c", "d"]}) dataset = lance.write_dataset(table, "example") dataset.to_table().to_pandas() a b b1 0 1 a c 1 2 b d rename column b into b0 and rename b1 into b&para; table = pa.table({"a": [3, 4], "b0": ["a", "b"], "b": ["c", "d"]}) lance_schema = LanceSchema.from_pyarrow(table.schema) operation = lance.LanceOperation.Project(lance_schema) dataset = lance.LanceDataset.commit("example", operation, read_version=1) dataset.to_table().to_pandas() a b0 b 0 1 a c 1 2 b d Restore dataclass &para; Bases: BaseOperation Operation that restores a previous version of the dataset. Rewrite dataclass &para; Bases: BaseOperation Operation that rewrites one or more files and indices into one or more files and indices. Attributes&para; groups: list[RewriteGroup] Groups of files that have been rewritten. rewritten_indices: list[RewrittenIndex] Indices that have been rewritten. Warning&para; This is an advanced API not intended for general use. RewriteGroup dataclass &para; Collection of rewritten files RewrittenIndex dataclass &para; An index that has been rewritten Update dataclass &para; Bases: BaseOperation Operation that updates rows in the dataset. Attributes&para; removed_fragment_ids: list[int] The ids of the fragments that have been removed entirely. updated_fragments: list[FragmentMetadata] The fragments that have been updated with new deletion vectors. new_fragments: list[FragmentMetadata] The fragments that contain the new rows. LanceScanner &para; Bases: Scanner dataset_schema property &para; The schema with which batches will be read from fragments. analyze_plan() &para; Execute the plan for this scanner and display with runtime metrics. Parameters&para; verbose : bool, default False Use a verbose output format. Returns&para; plan : str count_rows() &para; Count rows matching the scanner filter. Returns&para; count : int explain_plan(verbose=False) &para; Return the execution plan for this scanner. Parameters&para; verbose : bool, default False Use a verbose output format. Returns&para; plan : str from_batches(*args, **kwargs) staticmethod &para; Not implemented from_dataset(*args, **kwargs) staticmethod &para; Not implemented from_fragment(*args, **kwargs) staticmethod &para; Not implemented head(num_rows) &para; Load the first N rows of the dataset. Parameters&para; num_rows : int The number of rows to load. Returns&para; Table scan_batches() &para; Consume a Scanner in record batches with corresponding fragments. Returns&para; record_batches : iterator of TaggedRecordBatch take(indices) &para; Not implemented to_table() &para; Read the data into memory and return a pyarrow Table. MergeInsertBuilder &para; Bases: _MergeInsertBuilder conflict_retries(max_retries) &para; Set number of times to retry the operation if there is contention. If this is set &gt; 0, then the operation will keep a copy of the input data either in memory or on disk (depending on the size of the data) and will retry the operation if there is contention. Default is 10. execute(data_obj, *, schema=None) &para; Executes the merge insert operation This function updates the original dataset and returns a dictionary with information about merge statistics - i.e. the number of inserted, updated, and deleted rows. Parameters&para; ReaderLike The new data to use as the source table for the operation. This parameter can be any source of data (e.g. table / dataset) that :func:~lance.write_dataset accepts. schema: Optional[pa.Schema] The schema of the data. This only needs to be supplied whenever the data source is some kind of generator. execute_uncommitted(data_obj, *, schema=None) &para; Executes the merge insert operation without committing This function updates the original dataset and returns a dictionary with information about merge statistics - i.e. the number of inserted, updated, and deleted rows. Parameters&para; ReaderLike The new data to use as the source table for the operation. This parameter can be any source of data (e.g. table / dataset) that :func:~lance.write_dataset accepts. schema: Optional[pa.Schema] The schema of the data. This only needs to be supplied whenever the data source is some kind of generator. retry_timeout(timeout) &para; Set the timeout used to limit retries. This is the maximum time to spend on the operation before giving up. At least one attempt will be made, regardless of how long it takes to complete. Subsequent attempts will be cancelled once this timeout is reached. If the timeout has been reached during the first attempt, the operation will be cancelled immediately before making a second attempt. The default is 30 seconds. when_matched_update_all(condition=None) &para; Configure the operation to update matched rows After this method is called, when the merge insert operation executes, any rows that match both the source table and the target table will be updated. The rows from the target table will be removed and the rows from the source table will be added. An optional condition may be specified. This should be an SQL filter and, if present, then only matched rows that also satisfy this filter will be updated. The SQL filter should use the prefix target. to refer to columns in the target table and the prefix source. to refer to columns in the source table. For example, source.last_update &lt; target.last_update. If a condition is specified and rows do not satisfy the condition then these rows will not be updated. Failure to satisfy the filter does not cause a "matched" row to become a "not matched" row. when_not_matched_by_source_delete(expr=None) &para; Configure the operation to delete source rows that do not match After this method is called, when the merge insert operation executes, any rows that exist only in the target table will be deleted. An optional filter can be specified to limit the scope of the delete operation. If given (as an SQL filter) then only rows which match the filter will be deleted. when_not_matched_insert_all() &para; Configure the operation to insert not matched rows After this method is called, when the merge insert operation executes, any rows that exist only in the source table will be inserted into the target table. batch_udf(output_schema=None, checkpoint_file=None) &para; Create a user defined function (UDF) that adds columns to a dataset. This function is used to add columns to a dataset. It takes a function that takes a single argument, a RecordBatch, and returns a RecordBatch. The function is called once for each batch in the dataset. The function should not modify the input batch, but instead create a new batch with the new columns added. Parameters&para; output_schema : Schema, optional The schema of the output RecordBatch. This is used to validate the output of the function. If not provided, the schema of the first output RecordBatch will be used. checkpoint_file : str or Path, optional If specified, this file will be used as a cache for unsaved results of this UDF. If the process fails, and you call add_columns again with this same file, it will resume from the last saved state. This is useful for long running processes that may fail and need to be resumed. This file may get very large. It will hold up to an entire data files' worth of results on disk, which can be multiple gigabytes of data. Returns&para; AddColumnsUDF json_to_schema(schema_json) &para; Converts a JSON string to a PyArrow schema. Parameters&para; schema_json: Dict[str, Any] The JSON payload to convert to a PyArrow Schema. schema_to_json(schema) &para; Converts a pyarrow schema to a JSON string. Parameters&para; write_dataset(data_obj, uri, schema=None, mode=&#39;create&#39;, *, max_rows_per_file=1024 * 1024, max_rows_per_group=1024, max_bytes_per_file=90 * 1024 * 1024 * 1024, commit_lock=None, progress=None, storage_options=None, data_storage_version=None, use_legacy_format=None, enable_v2_manifest_paths=False, enable_move_stable_row_ids=False) &para; Write a given data_obj to the given uri Parameters&para; data_obj: Reader-like The data to be written. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader - Huggingface dataset uri: str, Path, or LanceDataset Where to write the dataset to (directory). If a LanceDataset is passed, the session will be reused. schema: Schema, optional If specified and the input is a pandas DataFrame, use this schema instead of the default pandas to arrow table conversion. mode: str create - create a new dataset (raises if uri already exists). overwrite - create a new snapshot version append - create a new version that is the concat of the input the latest version (raises if uri does not exist) max_rows_per_file: int, default 1024 * 1024 The max number of rows to write before starting a new file max_rows_per_group: int, default 1024 The max number of rows before starting a new group (in the same file) max_bytes_per_file: int, default 90 * 1024 * 1024 * 1024 The max number of bytes to write before starting a new file. This is a soft limit. This limit is checked after each group is written, which means larger groups may cause this to be overshot meaningfully. This defaults to 90 GB, since we have a hard limit of 100 GB per file on object stores. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. progress: FragmentWriteProgress, optional Experimental API. Progress tracking for writing the fragment. Pass a custom class that defines hooks to be called when each fragment is starting to write and finishing writing. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. data_storage_version: optional, str, default None The version of the data storage format to use. Newer versions are more efficient but require newer versions of lance to read. The default (None) will use the latest stable version. See the user guide for more details. use_legacy_format : optional, bool, default None Deprecated method for setting the data storage version. Use the data_storage_version parameter instead. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:LanceDataset.migrate_manifest_paths_v2 method. Default is False. enable_move_stable_row_ids : bool, optional Experimental parameter: if set to true, the writer will use move-stable row ids. These row ids are stable after compaction operations, but not after updates. This makes compaction more efficient, since with stable row ids no secondary indices need to be updated to point to new row ids. into lance.dataset &para; DataStatistics dataclass &para; Statistics about the data in the dataset DatasetOptimizer &para; compact_files(*, target_rows_per_fragment=1024 * 1024, max_rows_per_group=1024, max_bytes_per_file=None, materialize_deletions=True, materialize_deletions_threshold=0.1, num_threads=None, batch_size=None) &para; Compacts small files in the dataset, reducing total number of files. This does a few things Removes deleted rows from fragments Removes dropped columns from fragments Merges small fragments into larger ones This method preserves the insertion order of the dataset. This may mean it leaves small fragments in the dataset if they are not adjacent to other fragments that need compaction. For example, if you have fragments with row counts 5 million, 100, and 5 million, the middle fragment will not be compacted because the fragments it is adjacent to do not need compaction. Parameters&para; target_rows_per_fragment: int, default 1024*1024 The target number of rows per fragment. This is the number of rows that will be in each fragment after compaction. max_rows_per_group: int, default 1024 Max number of rows per group. This does not affect which fragments need compaction, but does affect how they are re-written if selected. This setting only affects datasets using the legacy storage format. The newer format does not require row groups. max_bytes_per_file: Optional[int], default None Max number of bytes in a single file. This does not affect which fragments need compaction, but does affect how they are re-written if selected. If this value is too small you may end up with fragments that are smaller than target_rows_per_fragment. The default will use the default from ``write_dataset``. materialize_deletions: bool, default True Whether to compact fragments with soft deleted rows so they are no longer present in the file. materialize_deletions_threshold: float, default 0.1 The fraction of original rows that are soft deleted in a fragment before the fragment is a candidate for compaction. num_threads: int, optional The number of threads to use when performing compaction. If not specified, defaults to the number of cores on the machine. batch_size: int, optional The batch size to use when scanning input fragments. You may want to reduce this if you are running out of memory during compaction. The default will use the same default from ``scanner``. Returns&para; CompactionMetrics Metrics about the compaction process See Also&para; lance.optimize.Compaction optimize_indices(**kwargs) &para; Optimizes index performance. As new data arrives it is not added to existing indexes automatically. When searching we need to perform an indexed search of the old data plus an expensive unindexed search on the new data. As the amount of new unindexed data grows this can have an impact on search latency. This function will add the new data to existing indexes, restoring the performance. This function does not retrain the index, it only assigns the new data to existing partitions. This means an update is much quicker than retraining the entire index but may have less accuracy (especially if the new data exhibits new patterns, concepts, or trends) Parameters&para; num_indices_to_merge: int, default 1 The number of indices to merge. If set to 0, new delta index will be created. index_names: List[str], default None The names of the indices to optimize. If None, all indices will be optimized. retrain: bool, default False Whether to retrain the whole index. If true, the index will be retrained based on the current data, num_indices_to_merge will be ignored, and all indices will be merged into one. This is useful when the data distribution has changed significantly, and we want to retrain the index to improve the search quality. This would be faster than re-create the index from scratch. FieldStatistics dataclass &para; Statistics about a field in the dataset LanceDataset &para; Bases: Dataset A Lance Dataset in Lance format where the data is stored at the given uri. data_storage_version property &para; The version of the data storage format this dataset is using lance_schema property &para; The LanceSchema for this dataset latest_version property &para; Returns the latest version of the dataset. max_field_id property &para; The max_field_id in manifest partition_expression property &para; Not implemented (just override pyarrow dataset to prevent segfault) schema property &para; The pyarrow Schema for this dataset stats property &para; Experimental API tags property &para; Tag management for the dataset. Similar to Git, tags are a way to add metadata to a specific version of the dataset. .. warning:: Tagged versions are exempted from the :py:meth:`cleanup_old_versions()` process. To remove a version that has been tagged, you must first :py:meth:`~Tags.delete` the associated tag. Examples&para; .. code-block:: python ds = lance.open(&quot;dataset.lance&quot;) ds.tags.create(&quot;v2-prod-20250203&quot;, 10) tags = ds.tags.list() uri property &para; The location of the data version property &para; Returns the currently checked out version of the dataset add_columns(transforms, read_columns=None, reader_schema=None, batch_size=None) &para; Add new columns with defined values. There are several ways to specify the new columns. First, you can provide SQL expressions for each new column. Second you can provide a UDF that takes a batch of existing data and returns a new batch with the new columns. These new columns will be appended to the dataset. You can also provide a RecordBatchReader which will read the new column values from some external source. This is often useful when the new column values have already been staged to files (often by some distributed process) See the :func:lance.add_columns_udf decorator for more information on writing UDFs. Parameters&para; transforms : dict or AddColumnsUDF or ReaderLike If this is a dictionary, then the keys are the names of the new columns and the values are SQL expression strings. These strings can reference existing columns in the dataset. If this is a AddColumnsUDF, then it is a UDF that takes a batch of existing data and returns a new batch with the new columns. If this is :class:pyarrow.Field or :class:pyarrow.Schema, it adds all NULL columns with the given schema, in a metadata-only operation. read_columns : list of str, optional The names of the columns that the UDF will read. If None, then the UDF will read all columns. This is only used when transforms is a UDF. Otherwise, the read columns are inferred from the SQL expressions. reader_schema: pa.Schema, optional Only valid if transforms is a ReaderLike object. This will be used to determine the schema of the reader. batch_size: int, optional The number of rows to read at a time from the source dataset when applying the transform. This is ignored if the dataset is a v1 dataset. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3]}) dataset = lance.write_dataset(table, "my_dataset") @lance.batch_udf() ... def double_a(batch): ... df = batch.to_pandas() ... return pd.DataFrame({'double_a': 2 * df['a']}) dataset.add_columns(double_a) dataset.to_table().to_pandas() a double_a 0 1 2 1 2 4 2 3 6 dataset.add_columns({"triple_a": "a * 3"}) dataset.to_table().to_pandas() a double_a triple_a 0 1 2 3 1 2 4 6 2 3 6 9 See Also&para; LanceDataset.merge : Merge a pre-computed set of columns into the dataset. alter_columns(*alterations) &para; Alter column name, data type, and nullability. Columns that are renamed can keep any indices that are on them. If a column has an IVF_PQ index, it can be kept if the column is casted to another type. However, other index types don't support casting at this time. Column types can be upcasted (such as int32 to int64) or downcasted (such as int64 to int32). However, downcasting will fail if there are any values that cannot be represented in the new type. In general, columns can be casted to same general type: integers to integers, floats to floats, and strings to strings. However, strings, binary, and list columns can be casted between their size variants. For example, string to large string, binary to large binary, and list to large list. Columns that are renamed can keep any indices that are on them. However, if the column is casted to a different type, its indices will be dropped. Parameters&para; alterations : Iterable[Dict[str, Any]] A sequence of dictionaries, each with the following keys: - &quot;path&quot;: str The column path to alter. For a top-level column, this is the name. For a nested column, this is the dot-separated path, e.g. &quot;a.b.c&quot;. - &quot;name&quot;: str, optional The new name of the column. If not specified, the column name is not changed. - &quot;nullable&quot;: bool, optional Whether the column should be nullable. If not specified, the column nullability is not changed. Only non-nullable columns can be changed to nullable. Currently, you cannot change a nullable column to non-nullable. - &quot;data_type&quot;: pyarrow.DataType, optional The new data type to cast the column to. If not specified, the column data type is not changed. Examples&para; import lance import pyarrow as pa schema = pa.schema([pa.field('a', pa.int64()), ... pa.field('b', pa.string(), nullable=False)]) table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.alter_columns({"path": "a", "name": "x"}, ... {"path": "b", "nullable": True}) dataset.to_table().to_pandas() x b 0 1 a 1 2 b 2 3 c dataset.alter_columns({"path": "x", "data_type": pa.int32()}) dataset.schema x: int32 b: string checkout_version(version) &para; Load the given version of the dataset. Unlike the :func:dataset constructor, this will re-use the current cache. This is a no-op if the dataset is already at the given version. Parameters&para; version: int | str, The version to check out. A version number (int) or a tag (str) can be provided. Returns&para; LanceDataset cleanup_old_versions(older_than=None, *, delete_unverified=False, error_if_tagged_old_versions=True) &para; Cleans up old versions of the dataset. Some dataset changes, such as overwriting, leave behind data that is not referenced by the latest dataset version. The old data is left in place to allow the dataset to be restored back to an older version. This method will remove older versions and any data files they reference. Once this cleanup task has run you will not be able to checkout or restore these older versions. Parameters&para; timedelta, optional Only versions older than this will be removed. If not specified, this will default to two weeks. bool, default False Files leftover from a failed transaction may appear to be part of an in-progress operation (e.g. appending new data) and these files will not be deleted unless they are at least 7 days old. If delete_unverified is True then these files will be deleted regardless of their age. This should only be set to True if you can guarantee that no other process is currently working on this dataset. Otherwise the dataset could be put into a corrupted state. bool, default True Some versions may have tags associated with them. Tagged versions will not be cleaned up, regardless of how old they are. If this argument is set to True (the default), an exception will be raised if any tagged versions match the parameters. Otherwise, tagged versions will be ignored without any error and only untagged versions will be cleaned up. commit(base_uri, operation, blobs_op=None, read_version=None, commit_lock=None, storage_options=None, enable_v2_manifest_paths=None, detached=False, max_retries=20) staticmethod &para; Create a new version of dataset This method is an advanced method which allows users to describe a change that has been made to the data files. This method is not needed when using Lance to apply changes (e.g. when using class:LanceDataset or func:write_dataset.) It's current purpose is to allow for changes being made in a distributed environment where no single process is doing all of the work. For example, a distributed bulk update or a distributed bulk modify operation. Once all of the changes have been made, this method can be called to make the changes visible by updating the dataset manifest. Warnings&para; This is an advanced API and doesn't provide the same level of validation as the other APIs. For example, it's the responsibility of the caller to ensure that the fragments are valid for the schema. Parameters&para; base_uri: str, Path, or LanceDataset The base uri of the dataset, or the dataset object itself. Using the dataset object can be more efficient because it can re-use the file metadata cache. operation: BaseOperation The operation to apply to the dataset. This describes what changes have been made. See available operations under :class:LanceOperation. read_version: int, optional The version of the dataset that was used as the base for the changes. This is not needed for overwrite or restore operations. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:migrate_manifest_paths_v2 method. Default is False. WARNING: turning this on will make the dataset unreadable for older versions of Lance (prior to 0.17.0). detached : bool, optional If True, then the commit will not be part of the dataset lineage. It will never show up as the latest dataset and the only way to check it out in the future will be to specifically check it out by version. The version will be a random version that is only unique amongst detached commits. The caller should store this somewhere as there will be no other way to obtain it in the future. max_retries : int The maximum number of retries to perform when committing the dataset. Returns&para; LanceDataset A new version of Lance Dataset. Examples&para; Creating a new dataset with the :class:LanceOperation.Overwrite operation: import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment1 = lance.fragment.LanceFragment.create("example", tab1) fragment2 = lance.fragment.LanceFragment.create("example", tab2) fragments = [fragment1, fragment2] operation = lance.LanceOperation.Overwrite(tab1.schema, fragments) dataset = lance.LanceDataset.commit("example", operation) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d commit_batch(dest, transactions, commit_lock=None, storage_options=None, enable_v2_manifest_paths=None, detached=False, max_retries=20) staticmethod &para; Create a new version of dataset with multiple transactions. This method is an advanced method which allows users to describe a change that has been made to the data files. This method is not needed when using Lance to apply changes (e.g. when using class:LanceDataset or func:write_dataset.) Parameters&para; dest: str, Path, or LanceDataset The base uri of the dataset, or the dataset object itself. Using the dataset object can be more efficient because it can re-use the file metadata cache. transactions: Iterable[Transaction] The transactions to apply to the dataset. These will be merged into a single transaction and applied to the dataset. Note: Only append transactions are currently supported. Other transaction types will be supported in the future. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:migrate_manifest_paths_v2 method. Default is False. WARNING: turning this on will make the dataset unreadable for older versions of Lance (prior to 0.17.0). detached : bool, optional If True, then the commit will not be part of the dataset lineage. It will never show up as the latest dataset and the only way to check it out in the future will be to specifically check it out by version. The version will be a random version that is only unique amongst detached commits. The caller should store this somewhere as there will be no other way to obtain it in the future. max_retries : int The maximum number of retries to perform when committing the dataset. Returns&para; dict with keys: dataset: LanceDataset A new version of Lance Dataset. merged: Transaction The merged transaction that was applied to the dataset. count_rows(filter=None, **kwargs) &para; Count rows matching the scanner filter. Parameters&para; **kwargs : dict, optional See py:method:scanner method for full parameter description. Returns&para; count : int The total number of rows in the dataset. create_index(column, index_type, name=None, metric=&#39;L2&#39;, replace=False, num_partitions=None, ivf_centroids=None, pq_codebook=None, num_sub_vectors=None, accelerator=None, index_cache_size=None, shuffle_partition_batches=None, shuffle_partition_concurrency=None, ivf_centroids_file=None, precomputed_partition_dataset=None, storage_options=None, filter_nan=True, one_pass_ivfpq=False, **kwargs) &para; Create index on column. Experimental API Parameters&para; column : str The column to be indexed. index_type : str The type of the index. "IVF_PQ, IVF_HNSW_PQ and IVF_HNSW_SQ" are supported now. name : str, optional The index name. If not provided, it will be generated from the column name. metric : str The distance metric type, i.e., "L2" (alias to "euclidean"), "cosine" or "dot" (dot product). Default is "L2". replace : bool Replace the existing index if it exists. num_partitions : int, optional The number of partitions of IVF (Inverted File Index). ivf_centroids : optional It can be either class:np.ndarray, class:pyarrow.FixedSizeListArray or class:pyarrow.FixedShapeTensorArray. A num_partitions x dimension array of existing K-mean centroids for IVF clustering. If not provided, a new KMeans model will be trained. pq_codebook : optional, It can be class:np.ndarray, class:pyarrow.FixedSizeListArray, or class:pyarrow.FixedShapeTensorArray. A num_sub_vectors x (2 ^ nbits * dimensions // num_sub_vectors) array of K-mean centroids for PQ codebook. Note: ``nbits`` is always 8 for now. If not provided, a new PQ model will be trained. num_sub_vectors : int, optional The number of sub-vectors for PQ (Product Quantization). accelerator : str or torch.Device, optional If set, use an accelerator to speed up the training process. Accepted accelerator: "cuda" (Nvidia GPU) and "mps" (Apple Silicon GPU). If not set, use the CPU. index_cache_size : int, optional The size of the index cache in number of entries. Default value is 256. shuffle_partition_batches : int, optional The number of batches, using the row group size of the dataset, to include in each shuffle partition. Default value is 10240. Assuming the row group size is 1024, each shuffle partition will hold 10240 * 1024 = 10,485,760 rows. By making this value smaller, this shuffle will consume less memory but will take longer to complete, and vice versa. shuffle_partition_concurrency : int, optional The number of shuffle partitions to process concurrently. Default value is 2 By making this value smaller, this shuffle will consume less memory but will take longer to complete, and vice versa. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. filter_nan: bool Defaults to True. False is UNSAFE, and will cause a crash if any null/nan values are present (and otherwise will not). Disables the null filter used for nullable columns. Obtains a small speed boost. one_pass_ivfpq: bool Defaults to False. If enabled, index type must be "IVF_PQ". Reduces disk IO. kwargs : Parameters passed to the index building process. The SQ (Scalar Quantization) is available for only IVF_HNSW_SQ index type, this quantization method is used to reduce the memory usage of the index, it maps the float vectors to integer vectors, each integer is of num_bits, now only 8 bits are supported. If index_type is "IVF_*", then the following parameters are required: num_partitions If index_type is with "PQ", then the following parameters are required: num_sub_vectors Optional parameters for IVF_PQ: - ivf_centroids Existing K-mean centroids for IVF clustering. - num_bits The number of bits for PQ (Product Quantization). Default is 8. Only 4, 8 are supported. Optional parameters for IVF_HNSW_*: max_level Int, the maximum number of levels in the graph. m Int, the number of edges per node in the graph. ef_construction Int, the number of nodes to examine during the construction. Examples&para; .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_PQ&quot;, num_partitions=256, num_sub_vectors=16 ) .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_HNSW_SQ&quot;, num_partitions=256, ) Experimental Accelerator (GPU) support: accelerate: use GPU to train IVF partitions. Only supports CUDA (Nvidia) or MPS (Apple) currently. Requires PyTorch being installed. .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/sift.lance&quot;) dataset.create_index( &quot;vector&quot;, &quot;IVF_PQ&quot;, num_partitions=256, num_sub_vectors=16, accelerator=&quot;cuda&quot; ) References&para; Faiss Index &lt;https://github.com/facebookresearch/faiss/wiki/Faiss-indexes&gt;_ IVF introduced in Video Google: a text retrieval approach to object matching in videos &lt;https://ieeexplore.ieee.org/abstract/document/1238663&gt;_ Product quantization for nearest neighbor search &lt;https://hal.inria.fr/inria-00514462v2/document&gt;_ create_scalar_index(column, index_type, name=None, *, replace=True, **kwargs) &para; Create a scalar index on a column. Scalar indices, like vector indices, can be used to speed up scans. A scalar index can speed up scans that contain filter expressions on the indexed column. For example, the following scan will be faster if the column my_col has a scalar index: .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) my_table = dataset.scanner(filter=&quot;my_col != 7&quot;).to_table() Vector search with pre-filers can also benefit from scalar indices. For example, .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) my_table = dataset.scanner( nearest=dict( column=&quot;vector&quot;, q=[1, 2, 3, 4], k=10, ) filter=&quot;my_col != 7&quot;, prefilter=True ) There are 5 types of scalar indices available today. BTREE. The most common type is BTREE. This index is inspired by the btree data structure although only the first few layers of the btree are cached in memory. It will perform well on columns with a large number of unique values and few rows per value. BITMAP. This index stores a bitmap for each unique value in the column. This index is useful for columns with a small number of unique values and many rows per value. LABEL_LIST. A special index that is used to index list columns whose values have small cardinality. For example, a column that contains lists of tags (e.g. ["tag1", "tag2", "tag3"]) can be indexed with a LABEL_LIST index. This index can only speedup queries with array_has_any or array_has_all filters. NGRAM. A special index that is used to index string columns. This index creates a bitmap for each ngram in the string. By default we use trigrams. This index can currently speed up queries using the contains function in filters. FTS/INVERTED. It is used to index document columns. This index can conduct full-text searches. For example, a column that contains any word of query string "hello world". The results will be ranked by BM25. Note that the LANCE_BYPASS_SPILLING environment variable can be used to bypass spilling to disk. Setting this to true can avoid memory exhaustion issues (see https://github.com/apache/datafusion/issues/10073 for more info). Experimental API Parameters&para; column : str The column to be indexed. Must be a boolean, integer, float, or string column. index_type : str The type of the index. One of "BTREE", "BITMAP", "LABEL_LIST", "NGRAM", "FTS" or "INVERTED". name : str, optional The index name. If not provided, it will be generated from the column name. replace : bool, default True Replace the existing index if it exists. bool, default True This is for the INVERTED index. If True, the index will store the positions of the words in the document, so that you can conduct phrase query. This will significantly increase the index size. It won't impact the performance of non-phrase queries even if it is set to True. base_tokenizer: str, default "simple" This is for the INVERTED index. The base tokenizer to use. The value can be: * "simple": splits tokens on whitespace and punctuation. * "whitespace": splits tokens on whitespace. * "raw": no tokenization. language: str, default "English" This is for the INVERTED index. The language for stemming and stop words. This is only used when stem or remove_stop_words is true max_token_length: Optional[int], default 40 This is for the INVERTED index. The maximum token length. Any token longer than this will be removed. lower_case: bool, default True This is for the INVERTED index. If True, the index will convert all text to lowercase. stem: bool, default False This is for the INVERTED index. If True, the index will stem the tokens. remove_stop_words: bool, default False This is for the INVERTED index. If True, the index will remove stop words. ascii_folding: bool, default False This is for the INVERTED index. If True, the index will convert non-ascii characters to ascii characters if possible. This would remove accents like "é" -&gt; "e". Examples&para; .. code-block:: python import lance dataset = lance.dataset(&quot;/tmp/images.lance&quot;) dataset.create_index( &quot;category&quot;, &quot;BTREE&quot;, ) Scalar indices can only speed up scans for basic filters using equality, comparison, range (e.g. my_col BETWEEN 0 AND 100), and set membership (e.g. my_col IN (0, 1, 2)) Scalar indices can be used if the filter contains multiple indexed columns and the filter criteria are AND'd or OR'd together (e.g. my_col &lt; 0 AND other_col&gt; 100) Scalar indices may be used if the filter contains non-indexed columns but, depending on the structure of the filter, they may not be usable. For example, if the column not_indexed does not have a scalar index then the filter my_col = 0 OR not_indexed = 1 will not be able to use any scalar index on my_col. To determine if a scan is making use of a scalar index you can use explain_plan to look at the query plan that lance has created. Queries that use scalar indices will either have a ScalarIndexQuery relation or a MaterializeIndex operator. delete(predicate) &para; Delete rows from the dataset. This marks rows as deleted, but does not physically remove them from the files. This keeps the existing indexes still valid. Parameters&para; predicate : str or pa.compute.Expression The predicate to use to select rows to delete. May either be a SQL string or a pyarrow Expression. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.delete("a = 1 or b in ('a', 'b')") dataset.to_table() pyarrow.Table a: int64 b: string a: [[3]] b: [["c"]] drop_columns(columns) &para; Drop one or more columns from the dataset This is a metadata-only operation and does not remove the data from the underlying storage. In order to remove the data, you must subsequently call compact_files to rewrite the data without the removed columns and then call cleanup_old_versions to remove the old files. Parameters&para; columns : list of str The names of the columns to drop. These can be nested column references (e.g. "a.b.c") or top-level column names (e.g. "a"). Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") dataset.drop_columns(["a"]) dataset.to_table().to_pandas() b 0 a 1 b 2 c drop_index(name) &para; Drops an index from the dataset Note: Indices are dropped by "index name". This is not the same as the field name. If you did not specify a name when you created the index then a name was generated for you. You can use the list_indices method to get the names of the indices. get_fragment(fragment_id) &para; Get the fragment with fragment id. get_fragments(filter=None) &para; Get all fragments from the dataset. Note: filter is not supported yet. head(num_rows, **kwargs) &para; Load the first N rows of the dataset. Parameters&para; num_rows : int The number of rows to load. **kwargs : dict, optional See scanner() method for full parameter description. Returns&para; table : Table insert(data, *, mode=&#39;append&#39;, **kwargs) &para; Insert data into the dataset. Parameters&para; data_obj: Reader-like The data to be written. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader - Huggingface dataset mode: str, default 'append' The mode to use when writing the data. Options are: create - create a new dataset (raises if uri already exists). overwrite - create a new snapshot version append - create a new version that is the concat of the input the latest version (raises if uri does not exist) **kwargs : dict, optional Additional keyword arguments to pass to :func:write_dataset. join(right_dataset, keys, right_keys=None, join_type=&#39;left outer&#39;, left_suffix=None, right_suffix=None, coalesce_keys=True, use_threads=True) &para; Not implemented (just override pyarrow dataset to prevent segfault) merge(data_obj, left_on, right_on=None, schema=None) &para; Merge another dataset into this one. Performs a left join, where the dataset is the left side and data_obj is the right side. Rows existing in the dataset but not on the left will be filled with null values, unless Lance doesn't support null values for some types, in which case an error will be raised. Parameters&para; data_obj: Reader-like The data to be merged. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, Iterator[RecordBatch], or RecordBatchReader left_on: str The name of the column in the dataset to join on. right_on: str or None The name of the column in data_obj to join on. If None, defaults to left_on. Examples&para; import lance import pyarrow as pa df = pa.table({'x': [1, 2, 3], 'y': ['a', 'b', 'c']}) dataset = lance.write_dataset(df, "dataset") dataset.to_table().to_pandas() x y 0 1 a 1 2 b 2 3 c new_df = pa.table({'x': [1, 2, 3], 'z': ['d', 'e', 'f']}) dataset.merge(new_df, 'x') dataset.to_table().to_pandas() x y z 0 1 a d 1 2 b e 2 3 c f See Also&para; LanceDataset.add_columns : Add new columns by computing batch-by-batch. merge_insert(on) &para; Returns a builder that can be used to create a "merge insert" operation This operation can add rows, update rows, and remove rows in a single transaction. It is a very generic tool that can be used to create behaviors like "insert if not exists", "update or insert (i.e. upsert)", or even replace a portion of existing data with new data (e.g. replace all data where month="january") The merge insert operation works by combining new data from a source table with existing data in a target table by using a join. There are three categories of records. "Matched" records are records that exist in both the source table and the target table. "Not matched" records exist only in the source table (e.g. these are new data). "Not matched by source" records exist only in the target table (this is old data). The builder returned by this method can be used to customize what should happen for each category of data. Please note that the data will be reordered as part of this operation. This is because updated rows will be deleted from the dataset and then reinserted at the end with the new values. The order of the newly inserted rows may fluctuate randomly because a hash-join operation is used internally. Parameters&para; Union[str, Iterable[str]] A column (or columns) to join on. This is how records from the source table and target table are matched. Typically this is some kind of key or id column. Examples&para; Use when_matched_update_all() and when_not_matched_insert_all() to perform an "upsert" operation. This will update rows that already exist in the dataset and insert rows that do not exist. import lance import pyarrow as pa table = pa.table({"a": [2, 1, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform a "upsert" operation&para; dataset.merge_insert("a") \ ... .when_matched_update_all() \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b 0 1 b 1 2 x 2 3 y 3 4 z Use when_not_matched_insert_all() to perform an "insert if not exists" operation. This will only insert rows that do not already exist in the dataset. import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example2") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform an "insert if not exists" operation&para; dataset.merge_insert("a") \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 0, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 z You are not required to provide all the columns. If you only want to update a subset of columns, you can omit columns you don't want to update. Omitted columns will keep their existing values if they are updated, or will be null if they are inserted. import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"], \ ... "c": ["x", "y", "z"]}) dataset = lance.write_dataset(table, "example3") new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]}) Perform an "upsert" operation, only updating column "a"&para; dataset.merge_insert("a") \ ... .when_matched_update_all() \ ... .when_not_matched_insert_all() \ ... .execute(new_table) {'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0} dataset.to_table().sort_by("a").to_pandas() a b c 0 1 a x 1 2 x y 2 3 y z 3 4 z None migrate_manifest_paths_v2() &para; Migrate the manifest paths to the new format. This will update the manifest to use the new v2 format for paths. This function is idempotent, and can be run multiple times without changing the state of the object store. DANGER: this should not be run while other concurrent operations are happening. And it should also run until completion before resuming other operations. prewarm_index(name) &para; Prewarm an index This will load the entire index into memory. This can help avoid cold start issues with index queries. If the index does not fit in the index cache, then this will result in wasted I/O. Parameters&para; name: str The name of the index to prewarm. replace_field_metadata(field_name, new_metadata) &para; Replace the metadata of a field in the schema Parameters&para; field_name: str The name of the field to replace the metadata for new_metadata: dict The new metadata to set replace_schema(schema) &para; Not implemented (just override pyarrow dataset to prevent segfault) See method:replace_schema_metadata or method:replace_field_metadata replace_schema_metadata(new_metadata) &para; Replace the schema metadata of the dataset Parameters&para; new_metadata: dict The new metadata to set restore() &para; Restore the currently checked out version as the latest version of the dataset. This creates a new commit. sample(num_rows, columns=None, randomize_order=True, **kwargs) &para; Select a random sample of data Parameters&para; num_rows: int number of rows to retrieve columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. **kwargs : dict, optional see scanner() method for full parameter description. Returns&para; table : Table scanner(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, fragments=None, full_text_query=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, fast_search=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, include_deleted_rows=None, scan_stats_callback=None, strict_batch_size=None) &para; Return a Scanner that can support various pushdowns. Parameters&para; columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. filter: pa.compute.Expression or str Expression or str that is a valid SQL where clause. See Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;_ for valid SQL expressions. limit: int, default None Fetch up to this many rows. All rows if None or unspecified. offset: int, default None Fetch starting with this row. 0 if None or unspecified. nearest: dict, default None Get the rows corresponding to the K most similar vectors. Example: .. code-block:: python { &quot;column&quot;: &lt;embedding col name&gt;, &quot;q&quot;: &lt;query vector as pa.Float32Array&gt;, &quot;k&quot;: 10, &quot;nprobes&quot;: 1, &quot;refine_factor&quot;: 1 } int, default None The target size of batches returned. In some cases batches can be up to twice this size (but never larger than this). In some cases batches can be smaller than this size. io_buffer_size: int, default None The size of the IO buffer. See ScannerBuilder.io_buffer_size for more information. batch_readahead: int, optional The number of batches to read ahead. fragment_readahead: int, optional The number of fragments to read ahead. scan_in_order: bool, default True Whether to read the fragments and batches in order. If false, throughput may be higher, but batches will be returned out of order and memory use might increase. fragments: iterable of LanceFragment, default None If specified, only scan these fragments. If scan_in_order is True, then the fragments will be scanned in the order given. prefilter: bool, default False If True then the filter will be applied before the vector query is run. This will generate more correct results but it may be a more costly query. It's generally good when the filter is highly selective. If False then the filter will be applied after the vector query is run. This will perform well but the results may have fewer than the requested number of rows (or be empty) if the rows closest to the query do not match the filter. It&#39;s generally good when the filter is not very selective. use_scalar_index: bool, default True Lance will automatically use scalar indices to optimize a query. In some corner cases this can make query performance worse and this parameter can be used to disable scalar indices in these cases. late_materialization: bool or List[str], default None Allows custom control over late materialization. Late materialization fetches non-query columns using a take operation after the filter. This is useful when there are few results or columns are very large. Early materialization can be better when there are many results or the columns are very narrow. If True, then all columns are late materialized. If False, then all columns are early materialized. If a list of strings, then only the columns in the list are late materialized. The default uses a heuristic that assumes filters will select about 0.1% of the rows. If your filter is more selective (e.g. find by id) you may want to set this to True. If your filter is not very selective (e.g. matches 20% of the rows) you may want to set this to False. full_text_query: str or dict, optional query string to search for, the results will be ranked by BM25. e.g. "hello world", would match documents containing "hello" or "world". or a dictionary with the following keys: - columns: list[str] The columns to search, currently only supports a single column in the columns list. - query: str The query string to search for. fast_search: bool, default False If True, then the search will only be performed on the indexed data, which yields faster search time. scan_stats_callback: Callable[[ScanStatistics], None], default None A callback function that will be called with the scan statistics after the scan is complete. Errors raised by the callback will be logged but not re-raised. include_deleted_rows: bool, default False If True, then rows that have been deleted, but are still present in the fragment, will be returned. These rows will have the _rowid column set to null. All other columns will reflect the value stored on disk and may not be null. Note: if this is a search operation, or a take operation (including scalar indexed scans) then deleted rows cannot be returned. .. note:: For now, if BOTH filter and nearest is specified, then: 1. nearest is executed first. 2. The results are filtered afterwards. For debugging ANN results, you can choose to not use the index even if present by specifying use_index=False. For example, the following will always return exact KNN results: .. code-block:: python dataset.to_table(nearest={ &quot;column&quot;: &quot;vector&quot;, &quot;k&quot;: 10, &quot;q&quot;: &lt;query vector&gt;, &quot;use_index&quot;: False } session() &para; Return the dataset session, which holds the dataset's state. take(indices, columns=None) &para; Select rows of data by index. Parameters&para; indices : Array or array-like indices of rows to select in the dataset. columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. Returns&para; table : pyarrow.Table take_blobs(blob_column, ids=None, addresses=None, indices=None) &para; Select blobs by row IDs. Instead of loading large binary blob data into memory before processing it, this API allows you to open binary blob data as a regular Python file-like object. For more details, see class:lance.BlobFile. Exactly one of ids, addresses, or indices must be specified. Parameters blob_column : str The name of the blob column to select. ids : Integer Array or array-like row IDs to select in the dataset. addresses: Integer Array or array-like The (unstable) row addresses to select in the dataset. indices : Integer Array or array-like The offset / indices of the row in the dataset. Returns&para; blob_files : List[BlobFile] to_batches(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, full_text_query=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, strict_batch_size=None, **kwargs) &para; Read the dataset as materialized record batches. Parameters&para; **kwargs : dict, optional Arguments for meth:~LanceDataset.scanner. Returns&para; record_batches : Iterator of class:~pyarrow.RecordBatch to_table(columns=None, filter=None, limit=None, offset=None, nearest=None, batch_size=None, batch_readahead=None, fragment_readahead=None, scan_in_order=None, *, prefilter=None, with_row_id=None, with_row_address=None, use_stats=None, fast_search=None, full_text_query=None, io_buffer_size=None, late_materialization=None, use_scalar_index=None, include_deleted_rows=None) &para; Read the data into memory as a class:pyarrow.Table Parameters&para; columns: list of str, or dict of str to str default None List of column names to be fetched. Or a dictionary of column names to SQL expressions. All columns are fetched if None or unspecified. filter : pa.compute.Expression or str Expression or str that is a valid SQL where clause. See Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;_ for valid SQL expressions. limit: int, default None Fetch up to this many rows. All rows if None or unspecified. offset: int, default None Fetch starting with this row. 0 if None or unspecified. nearest: dict, default None Get the rows corresponding to the K most similar vectors. Example: .. code-block:: python { &quot;column&quot;: &lt;embedding col name&gt;, &quot;q&quot;: &lt;query vector as pa.Float32Array&gt;, &quot;k&quot;: 10, &quot;metric&quot;: &quot;cosine&quot;, &quot;nprobes&quot;: 1, &quot;refine_factor&quot;: 1 } int, optional The number of rows to read at a time. io_buffer_size: int, default None The size of the IO buffer. See ScannerBuilder.io_buffer_size for more information. batch_readahead: int, optional The number of batches to read ahead. fragment_readahead: int, optional The number of fragments to read ahead. scan_in_order: bool, optional, default True Whether to read the fragments and batches in order. If false, throughput may be higher, but batches will be returned out of order and memory use might increase. prefilter: bool, optional, default False Run filter before the vector search. late_materialization: bool or List[str], default None Allows custom control over late materialization. See ScannerBuilder.late_materialization for more information. use_scalar_index: bool, default True Allows custom control over scalar index usage. See ScannerBuilder.use_scalar_index for more information. with_row_id: bool, optional, default False Return row ID. with_row_address: bool, optional, default False Return row address use_stats: bool, optional, default True Use stats pushdown during filters. fast_search: bool, optional, default False full_text_query: str or dict, optional query string to search for, the results will be ranked by BM25. e.g. "hello world", would match documents contains "hello" or "world". or a dictionary with the following keys: - columns: list[str] The columns to search, currently only supports a single column in the columns list. - query: str The query string to search for. include_deleted_rows: bool, optional, default False If True, then rows that have been deleted, but are still present in the fragment, will be returned. These rows will have the _rowid column set to null. All other columns will reflect the value stored on disk and may not be null. Note: if this is a search operation, or a take operation (including scalar indexed scans) then deleted rows cannot be returned. Notes&para; If BOTH filter and nearest is specified, then: nearest is executed first. The results are filtered afterward, unless pre-filter sets to True. update(updates, where=None) &para; Update column values for rows matching where. Parameters&para; updates : dict of str to str A mapping of column names to a SQL expression. where : str, optional A SQL predicate indicating which rows should be updated. Returns&para; updates : dict A dictionary containing the number of rows updated. Examples&para; import lance import pyarrow as pa table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]}) dataset = lance.write_dataset(table, "example") update_stats = dataset.update(dict(a = 'a + 2'), where="b != 'a'") update_stats["num_updated_rows"] = 2 dataset.to_table().to_pandas() a b 0 1 a 1 4 b 2 5 c validate() &para; Validate the dataset. This checks the integrity of the dataset and will raise an exception if the dataset is corrupted. versions() &para; Return all versions in this dataset. LanceOperation &para; Append dataclass &para; Bases: BaseOperation Append new rows to the dataset. Attributes&para; fragments: list[FragmentMetadata] The fragments that contain the new rows. Warning&para; This is an advanced API for distributed operations. To append to a dataset on a single machine, use :func:lance.write_dataset. Examples&para; To append new rows to a dataset, first use :meth:lance.fragment.LanceFragment.create to create fragments. Then collect the fragment metadata into a list and pass it to this class. Finally, pass the operation to the :meth:LanceDataset.commit method to create the new dataset. import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) dataset = lance.write_dataset(tab1, "example") tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment = lance.fragment.LanceFragment.create("example", tab2) operation = lance.LanceOperation.Append([fragment]) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d BaseOperation &para; Bases: ABC Base class for operations that can be applied to a dataset. See available operations under :class:LanceOperation. CreateIndex dataclass &para; Bases: BaseOperation Operation that creates an index on the dataset. DataReplacement dataclass &para; Bases: BaseOperation Operation that replaces existing datafiles in the dataset. DataReplacementGroup dataclass &para; Group of data replacements Delete dataclass &para; Bases: BaseOperation Remove fragments or rows from the dataset. Attributes&para; updated_fragments: list[FragmentMetadata] The fragments that have been updated with new deletion vectors. deleted_fragment_ids: list[int] The ids of the fragments that have been deleted entirely. These are the fragments where :meth:LanceFragment.delete() returned None. predicate: str The original SQL predicate used to select the rows to delete. Warning&para; This is an advanced API for distributed operations. To delete rows from dataset on a single machine, use :meth:lance.LanceDataset.delete. Examples&para; To delete rows from a dataset, call :meth:lance.fragment.LanceFragment.delete on each of the fragments. If that returns a new fragment, add that to the updated_fragments list. If it returns None, that means the whole fragment was deleted, so add the fragment id to the deleted_fragment_ids. Finally, pass the operation to the :meth:LanceDataset.commit method to complete the deletion operation. import lance import pyarrow as pa table = pa.table({"a": [1, 2], "b": ["a", "b"]}) dataset = lance.write_dataset(table, "example") table = pa.table({"a": [3, 4], "b": ["c", "d"]}) dataset = lance.write_dataset(table, "example", mode="append") dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d predicate = "a &gt;= 2" updated_fragments = [] deleted_fragment_ids = [] for fragment in dataset.get_fragments(): ... new_fragment = fragment.delete(predicate) ... if new_fragment is not None: ... updated_fragments.append(new_fragment) ... else: ... deleted_fragment_ids.append(fragment.fragment_id) operation = lance.LanceOperation.Delete(updated_fragments, ... deleted_fragment_ids, ... predicate) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b 0 1 a Merge dataclass &para; Bases: BaseOperation Operation that adds columns. Unlike Overwrite, this should not change the structure of the fragments, allowing existing indices to be kept. Attributes&para; fragments: iterable of FragmentMetadata The fragments that make up the new dataset. schema: LanceSchema or pyarrow.Schema The schema of the new dataset. Passing a LanceSchema is preferred, and passing a pyarrow.Schema is deprecated. Warning&para; This is an advanced API for distributed operations. To overwrite or create new dataset on a single machine, use :func:lance.write_dataset. Examples&para; To add new columns to a dataset, first define a method that will create the new columns based on the existing columns. Then use :meth:lance.fragment.LanceFragment.add_columns import lance import pyarrow as pa import pyarrow.compute as pc table = pa.table({"a": [1, 2, 3, 4], "b": ["a", "b", "c", "d"]}) dataset = lance.write_dataset(table, "example") dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d def double_a(batch: pa.RecordBatch) -&gt; pa.RecordBatch: ... doubled = pc.multiply(batch["a"], 2) ... return pa.record_batch([doubled], ["a_doubled"]) fragments = [] for fragment in dataset.get_fragments(): ... new_fragment, new_schema = fragment.merge_columns(double_a, ... columns=['a']) ... fragments.append(new_fragment) operation = lance.LanceOperation.Merge(fragments, new_schema) dataset = lance.LanceDataset.commit("example", operation, ... read_version=dataset.version) dataset.to_table().to_pandas() a b a_doubled 0 1 a 2 1 2 b 4 2 3 c 6 3 4 d 8 Overwrite dataclass &para; Bases: BaseOperation Overwrite or create a new dataset. Attributes&para; new_schema: pyarrow.Schema The schema of the new dataset. fragments: list[FragmentMetadata] The fragments that make up the new dataset. Warning&para; This is an advanced API for distributed operations. To overwrite or create new dataset on a single machine, use :func:lance.write_dataset. Examples&para; To create or overwrite a dataset, first use :meth:lance.fragment.LanceFragment.create to create fragments. Then collect the fragment metadata into a list and pass it along with the schema to this class. Finally, pass the operation to the :meth:LanceDataset.commit method to create the new dataset. import lance import pyarrow as pa tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]}) tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]}) fragment1 = lance.fragment.LanceFragment.create("example", tab1) fragment2 = lance.fragment.LanceFragment.create("example", tab2) fragments = [fragment1, fragment2] operation = lance.LanceOperation.Overwrite(tab1.schema, fragments) dataset = lance.LanceDataset.commit("example", operation) dataset.to_table().to_pandas() a b 0 1 a 1 2 b 2 3 c 3 4 d Project dataclass &para; Bases: BaseOperation Operation that project columns. Use this operator for drop column or rename/swap column. Attributes&para; schema: LanceSchema The lance schema of the new dataset. Examples&para; Use the projece operator to swap column: import lance import pyarrow as pa import pyarrow.compute as pc from lance.schema import LanceSchema table = pa.table({"a": [1, 2], "b": ["a", "b"], "b1": ["c", "d"]}) dataset = lance.write_dataset(table, "example") dataset.to_table().to_pandas() a b b1 0 1 a c 1 2 b d rename column b into b0 and rename b1 into b&para; table = pa.table({"a": [3, 4], "b0": ["a", "b"], "b": ["c", "d"]}) lance_schema = LanceSchema.from_pyarrow(table.schema) operation = lance.LanceOperation.Project(lance_schema) dataset = lance.LanceDataset.commit("example", operation, read_version=1) dataset.to_table().to_pandas() a b0 b 0 1 a c 1 2 b d Restore dataclass &para; Bases: BaseOperation Operation that restores a previous version of the dataset. Rewrite dataclass &para; Bases: BaseOperation Operation that rewrites one or more files and indices into one or more files and indices. Attributes&para; groups: list[RewriteGroup] Groups of files that have been rewritten. rewritten_indices: list[RewrittenIndex] Indices that have been rewritten. Warning&para; This is an advanced API not intended for general use. RewriteGroup dataclass &para; Collection of rewritten files RewrittenIndex dataclass &para; An index that has been rewritten Update dataclass &para; Bases: BaseOperation Operation that updates rows in the dataset. Attributes&para; removed_fragment_ids: list[int] The ids of the fragments that have been removed entirely. updated_fragments: list[FragmentMetadata] The fragments that have been updated with new deletion vectors. new_fragments: list[FragmentMetadata] The fragments that contain the new rows. LanceScanner &para; Bases: Scanner dataset_schema property &para; The schema with which batches will be read from fragments. analyze_plan() &para; Execute the plan for this scanner and display with runtime metrics. Parameters&para; verbose : bool, default False Use a verbose output format. Returns&para; plan : str count_rows() &para; Count rows matching the scanner filter. Returns&para; count : int explain_plan(verbose=False) &para; Return the execution plan for this scanner. Parameters&para; verbose : bool, default False Use a verbose output format. Returns&para; plan : str from_batches(*args, **kwargs) staticmethod &para; Not implemented from_dataset(*args, **kwargs) staticmethod &para; Not implemented from_fragment(*args, **kwargs) staticmethod &para; Not implemented head(num_rows) &para; Load the first N rows of the dataset. Parameters&para; num_rows : int The number of rows to load. Returns&para; Table scan_batches() &para; Consume a Scanner in record batches with corresponding fragments. Returns&para; record_batches : iterator of TaggedRecordBatch take(indices) &para; Not implemented to_table() &para; Read the data into memory and return a pyarrow Table. LanceStats &para; Statistics about a LanceDataset. data_stats() &para; Statistics about the data in the dataset. dataset_stats(max_rows_per_group=1024) &para; Statistics about the dataset. index_stats(index_name) &para; Statistics about an index. Parameters&para; index_name: str The name of the index to get statistics for. MergeInsertBuilder &para; Bases: _MergeInsertBuilder conflict_retries(max_retries) &para; Set number of times to retry the operation if there is contention. If this is set &gt; 0, then the operation will keep a copy of the input data either in memory or on disk (depending on the size of the data) and will retry the operation if there is contention. Default is 10. execute(data_obj, *, schema=None) &para; Executes the merge insert operation This function updates the original dataset and returns a dictionary with information about merge statistics - i.e. the number of inserted, updated, and deleted rows. Parameters&para; ReaderLike The new data to use as the source table for the operation. This parameter can be any source of data (e.g. table / dataset) that :func:~lance.write_dataset accepts. schema: Optional[pa.Schema] The schema of the data. This only needs to be supplied whenever the data source is some kind of generator. execute_uncommitted(data_obj, *, schema=None) &para; Executes the merge insert operation without committing This function updates the original dataset and returns a dictionary with information about merge statistics - i.e. the number of inserted, updated, and deleted rows. Parameters&para; ReaderLike The new data to use as the source table for the operation. This parameter can be any source of data (e.g. table / dataset) that :func:~lance.write_dataset accepts. schema: Optional[pa.Schema] The schema of the data. This only needs to be supplied whenever the data source is some kind of generator. retry_timeout(timeout) &para; Set the timeout used to limit retries. This is the maximum time to spend on the operation before giving up. At least one attempt will be made, regardless of how long it takes to complete. Subsequent attempts will be cancelled once this timeout is reached. If the timeout has been reached during the first attempt, the operation will be cancelled immediately before making a second attempt. The default is 30 seconds. when_matched_update_all(condition=None) &para; Configure the operation to update matched rows After this method is called, when the merge insert operation executes, any rows that match both the source table and the target table will be updated. The rows from the target table will be removed and the rows from the source table will be added. An optional condition may be specified. This should be an SQL filter and, if present, then only matched rows that also satisfy this filter will be updated. The SQL filter should use the prefix target. to refer to columns in the target table and the prefix source. to refer to columns in the source table. For example, source.last_update &lt; target.last_update. If a condition is specified and rows do not satisfy the condition then these rows will not be updated. Failure to satisfy the filter does not cause a "matched" row to become a "not matched" row. when_not_matched_by_source_delete(expr=None) &para; Configure the operation to delete source rows that do not match After this method is called, when the merge insert operation executes, any rows that exist only in the target table will be deleted. An optional filter can be specified to limit the scope of the delete operation. If given (as an SQL filter) then only rows which match the filter will be deleted. when_not_matched_insert_all() &para; Configure the operation to insert not matched rows After this method is called, when the merge insert operation executes, any rows that exist only in the source table will be inserted into the target table. ScannerBuilder &para; batch_readahead(nbatches=None) &para; This parameter is ignored when reading v2 files batch_size(batch_size) &para; Set batch size for Scanner fast_search(flag) &para; Enable fast search, which only perform search on the indexed data. Users can use Table::optimize() or create_index() to include the new data into index, thus make new data searchable. full_text_search(query, columns=None) &para; Filter rows by full text searching. Experimental API, may remove it after we support to do this within filter SQL-like expression Must create inverted index on the given column before searching, Parameters&para; query : str | Query If str, the query string to search for, a match query would be performed. If Query, the query object to search for, and the columns parameter will be ignored. columns : list of str, optional The columns to search in. If None, search in all indexed columns. include_deleted_rows(flag) &para; Include deleted rows Rows which have been deleted, but are still present in the fragment, will be returned. These rows will have all columns (except _rowaddr) set to null io_buffer_size(io_buffer_size) &para; Set the I/O buffer size for the Scanner This is the amount of RAM that will be reserved for holding I/O received from storage before it is processed. This is used to control the amount of memory used by the scanner. If the buffer is full then the scanner will block until the buffer is processed. Generally this should scale with the number of concurrent I/O threads. The default is 2GiB which comfortably provides enough space for somewhere between 32 and 256 concurrent I/O threads. This value is not a hard cap on the amount of RAM the scanner will use. Some space is used for the compute (which can be controlled by the batch size) and Lance does not keep track of memory after it is returned to the user. Currently, if there is a single batch of data which is larger than the io buffer size then the scanner will deadlock. This is a known issue and will be fixed in a future release. This parameter is only used when reading v2 files scan_in_order(scan_in_order=True) &para; Whether to scan the dataset in order of fragments and batches. If set to False, the scanner may read fragments concurrently and yield batches out of order. This may improve performance since it allows more concurrency in the scan, but can also use more memory. This parameter is ignored when using v2 files. In the v2 file format there is no penalty to scanning in order and so all scans will scan in order. scan_stats_callback(callback) &para; Set a callback function that will be called with the scan statistics after the scan is complete. Errors raised by the callback will be logged but not re-raised. strict_batch_size(strict_batch_size=False) &para; If True, then all batches except the last batch will have exactly batch_size rows. By default, it is false. If this is true then small batches will need to be merged together which will require a data copy and incur a (typically very small) performance penalty. use_scalar_index(use_scalar_index=True) &para; Set whether scalar indices should be used in a query Scans will use scalar indices, when available, to optimize queries with filters. However, in some corner cases, scalar indices may make performance worse. This parameter allows users to disable scalar indices in these cases. use_stats(use_stats=True) &para; Enable use of statistics for query planning. Disabling statistics is used for debugging and benchmarking purposes. This should be left on for normal use. with_row_address(with_row_address=True) &para; Enables returns with row addresses. Row addresses are a unique but unstable identifier for each row in the dataset that consists of the fragment id (upper 32 bits) and the row offset in the fragment (lower 32 bits). Row IDs are generally preferred since they do not change when a row is modified or compacted. However, row addresses may be useful in some advanced use cases. with_row_id(with_row_id=True) &para; Enable returns with row IDs. Tags &para; Dataset tag manager. create(tag, version) &para; Create a tag for a given dataset version. Parameters&para; tag: str, The name of the tag to create. This name must be unique among all tag names for the dataset. version: int, The dataset version to tag. delete(tag) &para; Delete tag from the dataset. Parameters&para; tag: str, The name of the tag to delete. list() &para; List all dataset tags. Returns&para; dict[str, Tag] A dictionary mapping tag names to version numbers. update(tag, version) &para; Update tag to a new version. Parameters&para; tag: str, The name of the tag to update. version: int, The new dataset version to tag. VectorIndexReader &para; This class allows you to initialize a reader for a specific vector index, retrieve the number of partitions, access the centroids of the index, and read specific partitions of the index. Parameters&para; dataset: LanceDataset The dataset containing the index. index_name: str The name of the vector index to read. Examples&para; .. code-block:: python import lance from lance.dataset import VectorIndexReader import numpy as np import pyarrow as pa vectors = np.random.rand(256, 2) data = pa.table({&quot;vector&quot;: pa.array(vectors.tolist(), type=pa.list_(pa.float32(), 2))}) dataset = lance.write_dataset(data, &quot;/tmp/index_reader_demo&quot;) dataset.create_index(&quot;vector&quot;, index_type=&quot;IVF_PQ&quot;, num_partitions=4, num_sub_vectors=2) reader = VectorIndexReader(dataset, &quot;vector_idx&quot;) assert reader.num_partitions() == 4 partition = reader.read_partition(0) assert &quot;_rowid&quot; in partition.column_names Exceptions&para; ValueError If the specified index is not a vector index. centroids() &para; Returns the centroids of the index Returns&para; np.ndarray The centroids of IVF with shape (num_partitions, dim) num_partitions() &para; Returns the number of partitions in the dataset. Returns&para; int The number of partitions. read_partition(partition_id, *, with_vector=False) &para; Returns a pyarrow table for the given IVF partition Parameters&para; partition_id: int The id of the partition to read with_vector: bool, default False Whether to include the vector column in the reader, for IVF_PQ, the vector column is PQ codes Returns&para; pa.Table A pyarrow table for the given partition, containing the row IDs, and quantized vectors (if with_vector is True). write_dataset(data_obj, uri, schema=None, mode=&#39;create&#39;, *, max_rows_per_file=1024 * 1024, max_rows_per_group=1024, max_bytes_per_file=90 * 1024 * 1024 * 1024, commit_lock=None, progress=None, storage_options=None, data_storage_version=None, use_legacy_format=None, enable_v2_manifest_paths=False, enable_move_stable_row_ids=False) &para; Write a given data_obj to the given uri Parameters&para; data_obj: Reader-like The data to be written. Acceptable types are: - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader - Huggingface dataset uri: str, Path, or LanceDataset Where to write the dataset to (directory). If a LanceDataset is passed, the session will be reused. schema: Schema, optional If specified and the input is a pandas DataFrame, use this schema instead of the default pandas to arrow table conversion. mode: str create - create a new dataset (raises if uri already exists). overwrite - create a new snapshot version append - create a new version that is the concat of the input the latest version (raises if uri does not exist) max_rows_per_file: int, default 1024 * 1024 The max number of rows to write before starting a new file max_rows_per_group: int, default 1024 The max number of rows before starting a new group (in the same file) max_bytes_per_file: int, default 90 * 1024 * 1024 * 1024 The max number of bytes to write before starting a new file. This is a soft limit. This limit is checked after each group is written, which means larger groups may cause this to be overshot meaningfully. This defaults to 90 GB, since we have a hard limit of 100 GB per file on object stores. commit_lock : CommitLock, optional A custom commit lock. Only needed if your object store does not support atomic commits. See the user guide for more details. progress: FragmentWriteProgress, optional Experimental API. Progress tracking for writing the fragment. Pass a custom class that defines hooks to be called when each fragment is starting to write and finishing writing. storage_options : optional, dict Extra options that make sense for a particular storage connection. This is used to store connection parameters like credentials, endpoint, etc. data_storage_version: optional, str, default None The version of the data storage format to use. Newer versions are more efficient but require newer versions of lance to read. The default (None) will use the latest stable version. See the user guide for more details. use_legacy_format : optional, bool, default None Deprecated method for setting the data storage version. Use the data_storage_version parameter instead. enable_v2_manifest_paths : bool, optional If True, and this is a new dataset, uses the new V2 manifest paths. These paths provide more efficient opening of datasets with many versions on object stores. This parameter has no effect if the dataset already exists. To migrate an existing dataset, instead use the :meth:LanceDataset.migrate_manifest_paths_v2 method. Default is False. enable_move_stable_row_ids : bool, optional Experimental parameter: if set to true, the writer will use move-stable row ids. These row ids are stable after compaction operations, but not after updates. This makes compaction more efficient, since with stable row ids no secondary indices need to be updated to point to new row ids. and rename b1 into
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Restore" class="md-nav__link">
    <span class="md-ellipsis">
      Restore
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Rewrite" class="md-nav__link">
    <span class="md-ellipsis">
      Rewrite
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Rewrite">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Rewrite--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Rewrite--warning" class="md-nav__link">
    <span class="md-ellipsis">
      Warning
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.RewriteGroup" class="md-nav__link">
    <span class="md-ellipsis">
      RewriteGroup
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.RewrittenIndex" class="md-nav__link">
    <span class="md-ellipsis">
      RewrittenIndex
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Update" class="md-nav__link">
    <span class="md-ellipsis">
      Update
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceOperation.Update--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      Attributes
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner" class="md-nav__link">
    <span class="md-ellipsis">
      LanceScanner
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LanceScanner">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.dataset_schema" class="md-nav__link">
    <span class="md-ellipsis">
      dataset_schema
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.analyze_plan" class="md-nav__link">
    <span class="md-ellipsis">
      analyze_plan
    </span>
  </a>
  
    <nav class="md-nav" aria-label="analyze_plan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.analyze_plan--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.analyze_plan--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.count_rows" class="md-nav__link">
    <span class="md-ellipsis">
      count_rows
    </span>
  </a>
  
    <nav class="md-nav" aria-label="count_rows">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.count_rows--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.explain_plan" class="md-nav__link">
    <span class="md-ellipsis">
      explain_plan
    </span>
  </a>
  
    <nav class="md-nav" aria-label="explain_plan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.explain_plan--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.explain_plan--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.from_batches" class="md-nav__link">
    <span class="md-ellipsis">
      from_batches
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.from_dataset" class="md-nav__link">
    <span class="md-ellipsis">
      from_dataset
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.from_fragment" class="md-nav__link">
    <span class="md-ellipsis">
      from_fragment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.head" class="md-nav__link">
    <span class="md-ellipsis">
      head
    </span>
  </a>
  
    <nav class="md-nav" aria-label="head">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.head--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.head--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.scan_batches" class="md-nav__link">
    <span class="md-ellipsis">
      scan_batches
    </span>
  </a>
  
    <nav class="md-nav" aria-label="scan_batches">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.scan_batches--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.take" class="md-nav__link">
    <span class="md-ellipsis">
      take
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceScanner.to_table" class="md-nav__link">
    <span class="md-ellipsis">
      to_table
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceStats" class="md-nav__link">
    <span class="md-ellipsis">
      LanceStats
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LanceStats">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceStats.data_stats" class="md-nav__link">
    <span class="md-ellipsis">
      data_stats
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceStats.dataset_stats" class="md-nav__link">
    <span class="md-ellipsis">
      dataset_stats
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceStats.index_stats" class="md-nav__link">
    <span class="md-ellipsis">
      index_stats
    </span>
  </a>
  
    <nav class="md-nav" aria-label="index_stats">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.LanceStats.index_stats--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.MergeInsertBuilder" class="md-nav__link">
    <span class="md-ellipsis">
      MergeInsertBuilder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MergeInsertBuilder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.MergeInsertBuilder.conflict_retries" class="md-nav__link">
    <span class="md-ellipsis">
      conflict_retries
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.MergeInsertBuilder.execute" class="md-nav__link">
    <span class="md-ellipsis">
      execute
    </span>
  </a>
  
    <nav class="md-nav" aria-label="execute">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.MergeInsertBuilder.execute--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.MergeInsertBuilder.execute_uncommitted" class="md-nav__link">
    <span class="md-ellipsis">
      execute_uncommitted
    </span>
  </a>
  
    <nav class="md-nav" aria-label="execute_uncommitted">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.MergeInsertBuilder.execute_uncommitted--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.MergeInsertBuilder.retry_timeout" class="md-nav__link">
    <span class="md-ellipsis">
      retry_timeout
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.MergeInsertBuilder.when_matched_update_all" class="md-nav__link">
    <span class="md-ellipsis">
      when_matched_update_all
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.MergeInsertBuilder.when_not_matched_by_source_delete" class="md-nav__link">
    <span class="md-ellipsis">
      when_not_matched_by_source_delete
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.MergeInsertBuilder.when_not_matched_insert_all" class="md-nav__link">
    <span class="md-ellipsis">
      when_not_matched_insert_all
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder" class="md-nav__link">
    <span class="md-ellipsis">
      ScannerBuilder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ScannerBuilder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.batch_readahead" class="md-nav__link">
    <span class="md-ellipsis">
      batch_readahead
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.batch_size" class="md-nav__link">
    <span class="md-ellipsis">
      batch_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.fast_search" class="md-nav__link">
    <span class="md-ellipsis">
      fast_search
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.full_text_search" class="md-nav__link">
    <span class="md-ellipsis">
      full_text_search
    </span>
  </a>
  
    <nav class="md-nav" aria-label="full_text_search">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.full_text_search--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.include_deleted_rows" class="md-nav__link">
    <span class="md-ellipsis">
      include_deleted_rows
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.io_buffer_size" class="md-nav__link">
    <span class="md-ellipsis">
      io_buffer_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.scan_in_order" class="md-nav__link">
    <span class="md-ellipsis">
      scan_in_order
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.scan_stats_callback" class="md-nav__link">
    <span class="md-ellipsis">
      scan_stats_callback
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.strict_batch_size" class="md-nav__link">
    <span class="md-ellipsis">
      strict_batch_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.use_scalar_index" class="md-nav__link">
    <span class="md-ellipsis">
      use_scalar_index
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.use_stats" class="md-nav__link">
    <span class="md-ellipsis">
      use_stats
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.with_row_address" class="md-nav__link">
    <span class="md-ellipsis">
      with_row_address
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.ScannerBuilder.with_row_id" class="md-nav__link">
    <span class="md-ellipsis">
      with_row_id
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.Tags" class="md-nav__link">
    <span class="md-ellipsis">
      Tags
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Tags">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.Tags.create" class="md-nav__link">
    <span class="md-ellipsis">
      create
    </span>
  </a>
  
    <nav class="md-nav" aria-label="create">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.Tags.create--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.Tags.delete" class="md-nav__link">
    <span class="md-ellipsis">
      delete
    </span>
  </a>
  
    <nav class="md-nav" aria-label="delete">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.Tags.delete--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.Tags.list" class="md-nav__link">
    <span class="md-ellipsis">
      list
    </span>
  </a>
  
    <nav class="md-nav" aria-label="list">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.Tags.list--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.Tags.update" class="md-nav__link">
    <span class="md-ellipsis">
      update
    </span>
  </a>
  
    <nav class="md-nav" aria-label="update">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.Tags.update--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.VectorIndexReader" class="md-nav__link">
    <span class="md-ellipsis">
      VectorIndexReader
    </span>
  </a>
  
    <nav class="md-nav" aria-label="VectorIndexReader">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.VectorIndexReader--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.VectorIndexReader--examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.VectorIndexReader--exceptions" class="md-nav__link">
    <span class="md-ellipsis">
      Exceptions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.VectorIndexReader.centroids" class="md-nav__link">
    <span class="md-ellipsis">
      centroids
    </span>
  </a>
  
    <nav class="md-nav" aria-label="centroids">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.VectorIndexReader.centroids--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.VectorIndexReader.num_partitions" class="md-nav__link">
    <span class="md-ellipsis">
      num_partitions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="num_partitions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.VectorIndexReader.num_partitions--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.VectorIndexReader.read_partition" class="md-nav__link">
    <span class="md-ellipsis">
      read_partition
    </span>
  </a>
  
    <nav class="md-nav" aria-label="read_partition">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.VectorIndexReader.read_partition--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.VectorIndexReader.read_partition--returns" class="md-nav__link">
    <span class="md-ellipsis">
      Returns
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lance.dataset.write_dataset" class="md-nav__link">
    <span class="md-ellipsis">
      write_dataset
    </span>
  </a>
  
    <nav class="md-nav" aria-label="write_dataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lance.dataset.write_dataset--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="python-api-reference">Python API Reference<a class="headerlink" href="#python-api-reference" title="Permanent link">&para;</a></h1>
<h2 id="lance_1">lance<a class="headerlink" href="#lance_1" title="Permanent link">&para;</a></h2>


<div class="doc doc-object doc-module">



<h2 id="lance" class="doc doc-heading">
            <code>lance</code>


<a href="#lance" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="lance.__version__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">__version__</span> <span class="o">=</span> <span class="s1">&#39;0.27.3&#39;</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-module"><code>module</code></small>
  </span>

<a href="#lance.__version__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>str(object='') -&gt; str
str(bytes_or_buffer[, encoding[, errors]]) -&gt; str</p>
<p>Create a new string object from the given object. If encoding or
errors is specified, then the object must expose a data buffer
that will be decoded using the given encoding and error handler.
Otherwise, returns the result of object.<strong>str</strong>() (if defined)
or repr(object).
encoding defaults to sys.getdefaultencoding().
errors defaults to 'strict'.</p>

    </div>

</div>


<div class="doc doc-object doc-class">



<h3 id="lance.BlobColumn" class="doc doc-heading">
            <code>BlobColumn</code>


<a href="#lance.BlobColumn" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">


        <p>A utility to wrap a Pyarrow binary column and iterate over the rows as
file-like objects.</p>
<p>This can be useful for working with medium-to-small binary objects that need
to interface with APIs that expect file-like objects.  For very large binary
objects (4-8MB or more per value) you might be better off creating a blob column
and using <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />meth:<code>lance.Dataset.take_blobs</code> to access the blob data.</p>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="lance.BlobFile" class="doc doc-heading">
            <code>BlobFile</code>


<a href="#lance.BlobFile" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="io.RawIOBase">RawIOBase</span></code></p>


        <p>Represents a blob in a Lance dataset as a file-like object.</p>










  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="lance.BlobFile.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">inner</span><span class="p">)</span></code>

<a href="#lance.BlobFile.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Internal only:  To obtain a BlobFile use
<img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />meth:<code>lance.dataset.Dataset.take_blobs</code>.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.BlobFile.size" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">size</span><span class="p">()</span></code>

<a href="#lance.BlobFile.size" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Returns the size of the blob in bytes.</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="lance.DataStatistics" class="doc doc-heading">
            <code>DataStatistics</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.DataStatistics" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">


        <p>Statistics about the data in the dataset</p>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="lance.FieldStatistics" class="doc doc-heading">
            <code>FieldStatistics</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.FieldStatistics" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">


        <p>Statistics about a field in the dataset</p>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="lance.FragmentMetadata" class="doc doc-heading">
            <code>FragmentMetadata</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.FragmentMetadata" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">


        <p>Metadata for a fragment.</p>
<h5 id="lance.FragmentMetadata--attributes">Attributes<a class="headerlink" href="#lance.FragmentMetadata--attributes" title="Permanent link">&para;</a></h5>
<p>id : int
    The ID of the fragment.
files : List[DataFile]
    The data files of the fragment. Each data file must have the same number
    of rows. Each file stores a different subset of the columns.
physical_rows : int
    The number of rows originally in this fragment. This is the number of rows
    in the data files before deletions.
deletion_file : Optional[DeletionFile]
    The deletion file, if any.
row_id_meta : Optional[RowIdMeta]
    The row id metadata, if any.</p>










  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="lance.FragmentMetadata.num_deletions" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">num_deletions</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.FragmentMetadata.num_deletions" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>The number of rows that have been deleted from this fragment.</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="lance.FragmentMetadata.num_rows" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">num_rows</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.FragmentMetadata.num_rows" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>The number of rows in this fragment after deletions.</p>

    </div>

</div>



<div class="doc doc-object doc-function">


<h4 id="lance.FragmentMetadata.to_json" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">to_json</span><span class="p">()</span></code>

<a href="#lance.FragmentMetadata.to_json" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Get this as a simple JSON-serializable dictionary.</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="lance.LanceDataset" class="doc doc-heading">
            <code>LanceDataset</code>


<a href="#lance.LanceDataset" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pyarrow.dataset.Dataset">Dataset</span></code></p>


        <p>A Lance Dataset in Lance format where the data is stored at the given uri.</p>










  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="lance.LanceDataset.data_storage_version" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">data_storage_version</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.LanceDataset.data_storage_version" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>The version of the data storage format this dataset is using</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="lance.LanceDataset.lance_schema" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">lance_schema</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.LanceDataset.lance_schema" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>The LanceSchema for this dataset</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="lance.LanceDataset.latest_version" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">latest_version</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.LanceDataset.latest_version" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Returns the latest version of the dataset.</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="lance.LanceDataset.max_field_id" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">max_field_id</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.LanceDataset.max_field_id" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>The max_field_id in manifest</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="lance.LanceDataset.partition_expression" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">partition_expression</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.LanceDataset.partition_expression" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Not implemented (just override pyarrow dataset to prevent segfault)</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="lance.LanceDataset.schema" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">schema</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.LanceDataset.schema" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>The pyarrow Schema for this dataset</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="lance.LanceDataset.stats" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">stats</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.LanceDataset.stats" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p><strong>Experimental API</strong></p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="lance.LanceDataset.tags" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">tags</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.LanceDataset.tags" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Tag management for the dataset.</p>
<p>Similar to Git, tags are a way to add metadata to a specific version of the
dataset.</p>
<p>.. warning::</p>
<div class="codehilite"><pre><span></span><code>Tagged versions are exempted from the :py:meth:`cleanup_old_versions()`
process.

To remove a version that has been tagged, you must first
:py:meth:`~Tags.delete` the associated tag.
</code></pre></div>

<h6 id="lance.LanceDataset.tags--examples">Examples<a class="headerlink" href="#lance.LanceDataset.tags--examples" title="Permanent link">&para;</a></h6>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>ds = lance.open(&quot;dataset.lance&quot;)
ds.tags.create(&quot;v2-prod-20250203&quot;, 10)

tags = ds.tags.list()
</code></pre></div>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="lance.LanceDataset.uri" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">uri</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.LanceDataset.uri" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>The location of the data</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="lance.LanceDataset.version" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">version</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.LanceDataset.version" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Returns the currently checked out version of the dataset</p>

    </div>

</div>



<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.add_columns" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">add_columns</span><span class="p">(</span><span class="n">transforms</span><span class="p">,</span> <span class="n">read_columns</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reader_schema</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.add_columns" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Add new columns with defined values.</p>
<p>There are several ways to specify the new columns. First, you can provide
SQL expressions for each new column. Second you can provide a UDF that
takes a batch of existing data and returns a new batch with the new
columns. These new columns will be appended to the dataset.</p>
<p>You can also provide a RecordBatchReader which will read the new column
values from some external source.  This is often useful when the new column
values have already been staged to files (often by some distributed process)</p>
<p>See the :func:<code>lance.add_columns_udf</code> decorator for more information on
writing UDFs.</p>
<h6 id="lance.LanceDataset.add_columns--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.add_columns--parameters" title="Permanent link">&para;</a></h6>
<p>transforms : dict or AddColumnsUDF or ReaderLike
    If this is a dictionary, then the keys are the names of the new
    columns and the values are SQL expression strings. These strings can
    reference existing columns in the dataset.
    If this is a AddColumnsUDF, then it is a UDF that takes a batch of
    existing data and returns a new batch with the new columns.
    If this is :class:<code>pyarrow.Field</code> or :class:<code>pyarrow.Schema</code>, it adds
    all NULL columns with the given schema, in a metadata-only operation.
read_columns : list of str, optional
    The names of the columns that the UDF will read. If None, then the
    UDF will read all columns. This is only used when transforms is a
    UDF. Otherwise, the read columns are inferred from the SQL expressions.
reader_schema: pa.Schema, optional
    Only valid if transforms is a <code>ReaderLike</code> object.  This will be used to
    determine the schema of the reader.
batch_size: int, optional
    The number of rows to read at a time from the source dataset when applying
    the transform.  This is ignored if the dataset is a v1 dataset.</p>
<h6 id="lance.LanceDataset.add_columns--examples">Examples<a class="headerlink" href="#lance.LanceDataset.add_columns--examples" title="Permanent link">&para;</a></h6>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
table = pa.table({"a": [1, 2, 3]})
dataset = lance.write_dataset(table, "my_dataset")
<a class="magiclink magiclink-github magiclink-mention" href="https://github.com/lance" title="GitHub User: lance">@lance</a>.batch_udf()
... def double_a(batch):
...     df = batch.to_pandas()
...     return pd.DataFrame({'double_a': 2 * df['a']})
dataset.add_columns(double_a)
dataset.to_table().to_pandas()
   a  double_a
0  1         2
1  2         4
2  3         6
dataset.add_columns({"triple_a": "a * 3"})
dataset.to_table().to_pandas()
   a  double_a  triple_a
0  1         2         3
1  2         4         6
2  3         6         9</p>
</blockquote>
</blockquote>
</blockquote>
<h6 id="lance.LanceDataset.add_columns--see-also">See Also<a class="headerlink" href="#lance.LanceDataset.add_columns--see-also" title="Permanent link">&para;</a></h6>
<p>LanceDataset.merge :
    Merge a pre-computed set of columns into the dataset.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.alter_columns" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">alter_columns</span><span class="p">(</span><span class="o">*</span><span class="n">alterations</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.alter_columns" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Alter column name, data type, and nullability.</p>
<p>Columns that are renamed can keep any indices that are on them. If a
column has an IVF_PQ index, it can be kept if the column is casted to
another type. However, other index types don't support casting at this
time.</p>
<p>Column types can be upcasted (such as int32 to int64) or downcasted
(such as int64 to int32). However, downcasting will fail if there are
any values that cannot be represented in the new type. In general,
columns can be casted to same general type: integers to integers,
floats to floats, and strings to strings. However, strings, binary, and
list columns can be casted between their size variants. For example,
string to large string, binary to large binary, and list to large list.</p>
<p>Columns that are renamed can keep any indices that are on them. However, if
the column is casted to a different type, its indices will be dropped.</p>
<h6 id="lance.LanceDataset.alter_columns--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.alter_columns--parameters" title="Permanent link">&para;</a></h6>
<p>alterations : Iterable[Dict[str, Any]]
    A sequence of dictionaries, each with the following keys:</p>
<div class="codehilite"><pre><span></span><code>- &quot;path&quot;: str
    The column path to alter. For a top-level column, this is the name.
    For a nested column, this is the dot-separated path, e.g. &quot;a.b.c&quot;.
- &quot;name&quot;: str, optional
    The new name of the column. If not specified, the column name is
    not changed.
- &quot;nullable&quot;: bool, optional
    Whether the column should be nullable. If not specified, the column
    nullability is not changed. Only non-nullable columns can be changed
    to nullable. Currently, you cannot change a nullable column to
    non-nullable.
- &quot;data_type&quot;: pyarrow.DataType, optional
    The new data type to cast the column to. If not specified, the column
    data type is not changed.
</code></pre></div>

<h6 id="lance.LanceDataset.alter_columns--examples">Examples<a class="headerlink" href="#lance.LanceDataset.alter_columns--examples" title="Permanent link">&para;</a></h6>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
schema = pa.schema([pa.field('a', pa.int64()),
...                     pa.field('b', pa.string(), nullable=False)])
table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]})
dataset = lance.write_dataset(table, "example")
dataset.alter_columns({"path": "a", "name": "x"},
...                       {"path": "b", "nullable": True})
dataset.to_table().to_pandas()
   x  b
0  1  a
1  2  b
2  3  c
dataset.alter_columns({"path": "x", "data_type": pa.int32()})
dataset.schema
x: int32
b: string</p>
</blockquote>
</blockquote>
</blockquote>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.checkout_version" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">checkout_version</span><span class="p">(</span><span class="n">version</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.checkout_version" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Load the given version of the dataset.</p>
<p>Unlike the :func:<code>dataset</code> constructor, this will re-use the
current cache.
This is a no-op if the dataset is already at the given version.</p>
<h6 id="lance.LanceDataset.checkout_version--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.checkout_version--parameters" title="Permanent link">&para;</a></h6>
<p>version: int | str,
    The version to check out. A version number (<code>int</code>) or a tag
    (<code>str</code>) can be provided.</p>
<h6 id="lance.LanceDataset.checkout_version--returns">Returns<a class="headerlink" href="#lance.LanceDataset.checkout_version--returns" title="Permanent link">&para;</a></h6>
<p>LanceDataset</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.cleanup_old_versions" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">cleanup_old_versions</span><span class="p">(</span><span class="n">older_than</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">delete_unverified</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">error_if_tagged_old_versions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.cleanup_old_versions" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Cleans up old versions of the dataset.</p>
<p>Some dataset changes, such as overwriting, leave behind data that is not
referenced by the latest dataset version.  The old data is left in place
to allow the dataset to be restored back to an older version.</p>
<p>This method will remove older versions and any data files they reference.
Once this cleanup task has run you will not be able to checkout or restore
these older versions.</p>
<h6 id="lance.LanceDataset.cleanup_old_versions--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.cleanup_old_versions--parameters" title="Permanent link">&para;</a></h6>


<details class="older_than" open>
  <summary>timedelta, optional</summary>
  <p>Only versions older than this will be removed.  If not specified, this
will default to two weeks.</p>
</details>

<details class="delete_unverified" open>
  <summary>bool, default False</summary>
  <p>Files leftover from a failed transaction may appear to be part of an
in-progress operation (e.g. appending new data) and these files will
not be deleted unless they are at least 7 days old.  If delete_unverified
is True then these files will be deleted regardless of their age.</p>
<p>This should only be set to True if you can guarantee that no other process
is currently working on this dataset.  Otherwise the dataset could be put
into a corrupted state.</p>
</details>

<details class="error_if_tagged_old_versions" open>
  <summary>bool, default True</summary>
  <p>Some versions may have tags associated with them. Tagged versions will
not be cleaned up, regardless of how old they are. If this argument
is set to <code>True</code> (the default), an exception will be raised if any
tagged versions match the parameters. Otherwise, tagged versions will
be ignored without any error and only untagged versions will be
cleaned up.</p>
</details>

    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.commit" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">commit</span><span class="p">(</span><span class="n">base_uri</span><span class="p">,</span> <span class="n">operation</span><span class="p">,</span> <span class="n">blobs_op</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">read_version</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">commit_lock</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">storage_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">enable_v2_manifest_paths</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">detached</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_retries</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#lance.LanceDataset.commit" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Create a new version of dataset</p>
<p>This method is an advanced method which allows users to describe a change
that has been made to the data files.  This method is not needed when using
Lance to apply changes (e.g. when using <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />class:<code>LanceDataset</code> or
<img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />func:<code>write_dataset</code>.)</p>
<p>It's current purpose is to allow for changes being made in a distributed
environment where no single process is doing all of the work.  For example,
a distributed bulk update or a distributed bulk modify operation.</p>
<p>Once all of the changes have been made, this method can be called to make
the changes visible by updating the dataset manifest.</p>
<h6 id="lance.LanceDataset.commit--warnings">Warnings<a class="headerlink" href="#lance.LanceDataset.commit--warnings" title="Permanent link">&para;</a></h6>
<p>This is an advanced API and doesn't provide the same level of validation
as the other APIs. For example, it's the responsibility of the caller to
ensure that the fragments are valid for the schema.</p>
<h6 id="lance.LanceDataset.commit--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.commit--parameters" title="Permanent link">&para;</a></h6>
<p>base_uri: str, Path, or LanceDataset
    The base uri of the dataset, or the dataset object itself. Using
    the dataset object can be more efficient because it can re-use the
    file metadata cache.
operation: BaseOperation
    The operation to apply to the dataset.  This describes what changes
    have been made. See available operations under :class:<code>LanceOperation</code>.
read_version: int, optional
    The version of the dataset that was used as the base for the changes.
    This is not needed for overwrite or restore operations.
commit_lock : CommitLock, optional
    A custom commit lock.  Only needed if your object store does not support
    atomic commits.  See the user guide for more details.
storage_options : optional, dict
    Extra options that make sense for a particular storage connection. This is
    used to store connection parameters like credentials, endpoint, etc.
enable_v2_manifest_paths : bool, optional
    If True, and this is a new dataset, uses the new V2 manifest paths.
    These paths provide more efficient opening of datasets with many
    versions on object stores. This parameter has no effect if the dataset
    already exists. To migrate an existing dataset, instead use the
    :meth:<code>migrate_manifest_paths_v2</code> method. Default is False. WARNING:
    turning this on will make the dataset unreadable for older versions
    of Lance (prior to 0.17.0).
detached : bool, optional
    If True, then the commit will not be part of the dataset lineage.  It will
    never show up as the latest dataset and the only way to check it out in the
    future will be to specifically check it out by version.  The version will be
    a random version that is only unique amongst detached commits.  The caller
    should store this somewhere as there will be no other way to obtain it in
    the future.
max_retries : int
    The maximum number of retries to perform when committing the dataset.</p>
<h6 id="lance.LanceDataset.commit--returns">Returns<a class="headerlink" href="#lance.LanceDataset.commit--returns" title="Permanent link">&para;</a></h6>
<p>LanceDataset
    A new version of Lance Dataset.</p>
<h6 id="lance.LanceDataset.commit--examples">Examples<a class="headerlink" href="#lance.LanceDataset.commit--examples" title="Permanent link">&para;</a></h6>
<p>Creating a new dataset with the :class:<code>LanceOperation.Overwrite</code> operation:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]})
tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]})
fragment1 = lance.fragment.LanceFragment.create("example", tab1)
fragment2 = lance.fragment.LanceFragment.create("example", tab2)
fragments = [fragment1, fragment2]
operation = lance.LanceOperation.Overwrite(tab1.schema, fragments)
dataset = lance.LanceDataset.commit("example", operation)
dataset.to_table().to_pandas()
   a  b
0  1  a
1  2  b
2  3  c
3  4  d</p>
</blockquote>
</blockquote>
</blockquote>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.commit_batch" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">commit_batch</span><span class="p">(</span><span class="n">dest</span><span class="p">,</span> <span class="n">transactions</span><span class="p">,</span> <span class="n">commit_lock</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">storage_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">enable_v2_manifest_paths</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">detached</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_retries</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#lance.LanceDataset.commit_batch" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Create a new version of dataset with multiple transactions.</p>
<p>This method is an advanced method which allows users to describe a change
that has been made to the data files.  This method is not needed when using
Lance to apply changes (e.g. when using <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />class:<code>LanceDataset</code> or
<img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />func:<code>write_dataset</code>.)</p>
<h6 id="lance.LanceDataset.commit_batch--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.commit_batch--parameters" title="Permanent link">&para;</a></h6>
<p>dest: str, Path, or LanceDataset
    The base uri of the dataset, or the dataset object itself. Using
    the dataset object can be more efficient because it can re-use the
    file metadata cache.
transactions: Iterable[Transaction]
    The transactions to apply to the dataset. These will be merged into
    a single transaction and applied to the dataset. Note: Only append
    transactions are currently supported. Other transaction types will be
    supported in the future.
commit_lock : CommitLock, optional
    A custom commit lock.  Only needed if your object store does not support
    atomic commits.  See the user guide for more details.
storage_options : optional, dict
    Extra options that make sense for a particular storage connection. This is
    used to store connection parameters like credentials, endpoint, etc.
enable_v2_manifest_paths : bool, optional
    If True, and this is a new dataset, uses the new V2 manifest paths.
    These paths provide more efficient opening of datasets with many
    versions on object stores. This parameter has no effect if the dataset
    already exists. To migrate an existing dataset, instead use the
    :meth:<code>migrate_manifest_paths_v2</code> method. Default is False. WARNING:
    turning this on will make the dataset unreadable for older versions
    of Lance (prior to 0.17.0).
detached : bool, optional
    If True, then the commit will not be part of the dataset lineage.  It will
    never show up as the latest dataset and the only way to check it out in the
    future will be to specifically check it out by version.  The version will be
    a random version that is only unique amongst detached commits.  The caller
    should store this somewhere as there will be no other way to obtain it in
    the future.
max_retries : int
    The maximum number of retries to perform when committing the dataset.</p>
<h6 id="lance.LanceDataset.commit_batch--returns">Returns<a class="headerlink" href="#lance.LanceDataset.commit_batch--returns" title="Permanent link">&para;</a></h6>
<p>dict with keys:
    dataset: LanceDataset
        A new version of Lance Dataset.
    merged: Transaction
        The merged transaction that was applied to the dataset.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.count_rows" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">count_rows</span><span class="p">(</span><span class="nb">filter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.count_rows" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Count rows matching the scanner filter.</p>
<h6 id="lance.LanceDataset.count_rows--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.count_rows--parameters" title="Permanent link">&para;</a></h6>
<p>**kwargs : dict, optional
    See py:method:<code>scanner</code> method for full parameter description.</p>
<h6 id="lance.LanceDataset.count_rows--returns">Returns<a class="headerlink" href="#lance.LanceDataset.count_rows--returns" title="Permanent link">&para;</a></h6>
<p>count : int
    The total number of rows in the dataset.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.create_index" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">create_index</span><span class="p">(</span><span class="n">column</span><span class="p">,</span> <span class="n">index_type</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;L2&#39;</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_partitions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ivf_centroids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pq_codebook</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_sub_vectors</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">index_cache_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shuffle_partition_batches</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shuffle_partition_concurrency</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ivf_centroids_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">precomputed_partition_dataset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">storage_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">filter_nan</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">one_pass_ivfpq</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.create_index" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Create index on column.</p>
<p><strong>Experimental API</strong></p>
<h6 id="lance.LanceDataset.create_index--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.create_index--parameters" title="Permanent link">&para;</a></h6>
<p>column : str
    The column to be indexed.
index_type : str
    The type of the index.
    <code>"IVF_PQ, IVF_HNSW_PQ and IVF_HNSW_SQ"</code> are supported now.
name : str, optional
    The index name. If not provided, it will be generated from the
    column name.
metric : str
    The distance metric type, i.e., "L2" (alias to "euclidean"), "cosine"
    or "dot" (dot product). Default is "L2".
replace : bool
    Replace the existing index if it exists.
num_partitions : int, optional
    The number of partitions of IVF (Inverted File Index).
ivf_centroids : optional
    It can be either <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />class:<code>np.ndarray</code>,
    <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />class:<code>pyarrow.FixedSizeListArray</code> or
    <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />class:<code>pyarrow.FixedShapeTensorArray</code>.
    A <code>num_partitions x dimension</code> array of existing K-mean centroids
    for IVF clustering. If not provided, a new KMeans model will be trained.
pq_codebook : optional,
    It can be <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />class:<code>np.ndarray</code>, <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />class:<code>pyarrow.FixedSizeListArray</code>,
    or <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />class:<code>pyarrow.FixedShapeTensorArray</code>.
    A <code>num_sub_vectors x (2 ^ nbits * dimensions // num_sub_vectors)</code>
    array of K-mean centroids for PQ codebook.</p>
<div class="codehilite"><pre><span></span><code>Note: ``nbits`` is always 8 for now.
If not provided, a new PQ model will be trained.
</code></pre></div>

<p>num_sub_vectors : int, optional
    The number of sub-vectors for PQ (Product Quantization).
accelerator : str or <code>torch.Device</code>, optional
    If set, use an accelerator to speed up the training process.
    Accepted accelerator: "cuda" (Nvidia GPU) and "mps" (Apple Silicon GPU).
    If not set, use the CPU.
index_cache_size : int, optional
    The size of the index cache in number of entries. Default value is 256.
shuffle_partition_batches : int, optional
    The number of batches, using the row group size of the dataset, to include
    in each shuffle partition. Default value is 10240.</p>
<div class="codehilite"><pre><span></span><code>Assuming the row group size is 1024, each shuffle partition will hold
10240 * 1024 = 10,485,760 rows. By making this value smaller, this shuffle
will consume less memory but will take longer to complete, and vice versa.
</code></pre></div>

<p>shuffle_partition_concurrency : int, optional
    The number of shuffle partitions to process concurrently. Default value is 2</p>
<div class="codehilite"><pre><span></span><code>By making this value smaller, this shuffle will consume less memory but will
take longer to complete, and vice versa.
</code></pre></div>

<p>storage_options : optional, dict
    Extra options that make sense for a particular storage connection. This is
    used to store connection parameters like credentials, endpoint, etc.
filter_nan: bool
    Defaults to True. False is UNSAFE, and will cause a crash if any null/nan
    values are present (and otherwise will not). Disables the null filter used
    for nullable columns. Obtains a small speed boost.
one_pass_ivfpq: bool
    Defaults to False. If enabled, index type must be "IVF_PQ". Reduces disk IO.
kwargs :
    Parameters passed to the index building process.</p>
<p>The SQ (Scalar Quantization) is available for only <code>IVF_HNSW_SQ</code> index type,
this quantization method is used to reduce the memory usage of the index,
it maps the float vectors to integer vectors, each integer is of <code>num_bits</code>,
now only 8 bits are supported.</p>
<p>If <code>index_type</code> is "IVF_*", then the following parameters are required:
    num_partitions</p>
<p>If <code>index_type</code> is with "PQ", then the following parameters are required:
    num_sub_vectors</p>
<p>Optional parameters for <code>IVF_PQ</code>:</p>
<div class="codehilite"><pre><span></span><code>- ivf_centroids
    Existing K-mean centroids for IVF clustering.
- num_bits
    The number of bits for PQ (Product Quantization). Default is 8.
    Only 4, 8 are supported.
</code></pre></div>

<p>Optional parameters for <code>IVF_HNSW_*</code>:
    max_level
        Int, the maximum number of levels in the graph.
    m
        Int, the number of edges per node in the graph.
    ef_construction
        Int, the number of nodes to examine during the construction.</p>
<h6 id="lance.LanceDataset.create_index--examples">Examples<a class="headerlink" href="#lance.LanceDataset.create_index--examples" title="Permanent link">&para;</a></h6>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>import lance

dataset = lance.dataset(&quot;/tmp/sift.lance&quot;)
dataset.create_index(
    &quot;vector&quot;,
    &quot;IVF_PQ&quot;,
    num_partitions=256,
    num_sub_vectors=16
)
</code></pre></div>

<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>import lance

dataset = lance.dataset(&quot;/tmp/sift.lance&quot;)
dataset.create_index(
    &quot;vector&quot;,
    &quot;IVF_HNSW_SQ&quot;,
    num_partitions=256,
)
</code></pre></div>

<p>Experimental Accelerator (GPU) support:</p>
<ul>
<li><em>accelerate</em>: use GPU to train IVF partitions.
    Only supports CUDA (Nvidia) or MPS (Apple) currently.
    Requires PyTorch being installed.</li>
</ul>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>import lance

dataset = lance.dataset(&quot;/tmp/sift.lance&quot;)
dataset.create_index(
    &quot;vector&quot;,
    &quot;IVF_PQ&quot;,
    num_partitions=256,
    num_sub_vectors=16,
    accelerator=&quot;cuda&quot;
)
</code></pre></div>

<h6 id="lance.LanceDataset.create_index--references">References<a class="headerlink" href="#lance.LanceDataset.create_index--references" title="Permanent link">&para;</a></h6>
<ul>
<li><code>Faiss Index &lt;https://github.com/facebookresearch/faiss/wiki/Faiss-indexes&gt;</code>_</li>
<li>IVF introduced in <code>Video Google: a text retrieval approach to object matching
  in videos &lt;https://ieeexplore.ieee.org/abstract/document/1238663&gt;</code>_</li>
<li><code>Product quantization for nearest neighbor search
  &lt;https://hal.inria.fr/inria-00514462v2/document&gt;</code>_</li>
</ul>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.create_scalar_index" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">create_scalar_index</span><span class="p">(</span><span class="n">column</span><span class="p">,</span> <span class="n">index_type</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.create_scalar_index" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Create a scalar index on a column.</p>
<p>Scalar indices, like vector indices, can be used to speed up scans.  A scalar
index can speed up scans that contain filter expressions on the indexed column.
For example, the following scan will be faster if the column <code>my_col</code> has
a scalar index:</p>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>import lance

dataset = lance.dataset(&quot;/tmp/images.lance&quot;)
my_table = dataset.scanner(filter=&quot;my_col != 7&quot;).to_table()
</code></pre></div>

<p>Vector search with pre-filers can also benefit from scalar indices. For example,</p>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>import lance

dataset = lance.dataset(&quot;/tmp/images.lance&quot;)
my_table = dataset.scanner(
    nearest=dict(
       column=&quot;vector&quot;,
       q=[1, 2, 3, 4],
       k=10,
    )
    filter=&quot;my_col != 7&quot;,
    prefilter=True
)
</code></pre></div>

<p>There are 5 types of scalar indices available today.</p>
<ul>
<li><code>BTREE</code>. The most common type is <code>BTREE</code>. This index is inspired
  by the btree data structure although only the first few layers of the btree
  are cached in memory.  It will
  perform well on columns with a large number of unique values and few rows per
  value.</li>
<li><code>BITMAP</code>. This index stores a bitmap for each unique value in the column.
  This index is useful for columns with a small number of unique values and
  many rows per value.</li>
<li><code>LABEL_LIST</code>. A special index that is used to index list
  columns whose values have small cardinality.  For example, a column that
  contains lists of tags (e.g. <code>["tag1", "tag2", "tag3"]</code>) can be indexed
  with a <code>LABEL_LIST</code> index.  This index can only speedup queries with
  <code>array_has_any</code> or <code>array_has_all</code> filters.</li>
<li><code>NGRAM</code>. A special index that is used to index string columns.  This index
  creates a bitmap for each ngram in the string.  By default we use trigrams.
  This index can currently speed up queries using the <code>contains</code> function
  in filters.</li>
<li><code>FTS/INVERTED</code>. It is used to index document columns. This index
  can conduct full-text searches. For example, a column that contains any word
  of query string "hello world". The results will be ranked by BM25.</li>
</ul>
<p>Note that the <code>LANCE_BYPASS_SPILLING</code> environment variable can be used to
bypass spilling to disk. Setting this to true can avoid memory exhaustion
issues (see <a href="https://github.com/apache/datafusion/issues/10073">https://github.com/apache/datafusion/issues/10073</a> for more info).</p>
<p><strong>Experimental API</strong></p>
<h6 id="lance.LanceDataset.create_scalar_index--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.create_scalar_index--parameters" title="Permanent link">&para;</a></h6>
<p>column : str
    The column to be indexed.  Must be a boolean, integer, float,
    or string column.
index_type : str
    The type of the index.  One of <code>"BTREE"</code>, <code>"BITMAP"</code>,
    <code>"LABEL_LIST"</code>, <code>"NGRAM"</code>, <code>"FTS"</code> or <code>"INVERTED"</code>.
name : str, optional
    The index name. If not provided, it will be generated from the
    column name.
replace : bool, default True
    Replace the existing index if it exists.</p>


<details class="with_position" open>
  <summary>bool, default True</summary>
  <p>This is for the <code>INVERTED</code> index. If True, the index will store the
positions of the words in the document, so that you can conduct phrase
query. This will significantly increase the index size.
It won't impact the performance of non-phrase queries even if it is set to
True.</p>
</details>        <p>base_tokenizer: str, default "simple"
    This is for the <code>INVERTED</code> index. The base tokenizer to use. The value
    can be:
    * "simple": splits tokens on whitespace and punctuation.
    * "whitespace": splits tokens on whitespace.
    * "raw": no tokenization.
language: str, default "English"
    This is for the <code>INVERTED</code> index. The language for stemming
    and stop words. This is only used when <code>stem</code> or <code>remove_stop_words</code> is true
max_token_length: Optional[int], default 40
    This is for the <code>INVERTED</code> index. The maximum token length.
    Any token longer than this will be removed.
lower_case: bool, default True
    This is for the <code>INVERTED</code> index. If True, the index will convert all
    text to lowercase.
stem: bool, default False
    This is for the <code>INVERTED</code> index. If True, the index will stem the
    tokens.
remove_stop_words: bool, default False
    This is for the <code>INVERTED</code> index. If True, the index will remove
    stop words.
ascii_folding: bool, default False
    This is for the <code>INVERTED</code> index. If True, the index will convert
    non-ascii characters to ascii characters if possible.
    This would remove accents like "é" -&gt; "e".</p>
<h6 id="lance.LanceDataset.create_scalar_index--examples">Examples<a class="headerlink" href="#lance.LanceDataset.create_scalar_index--examples" title="Permanent link">&para;</a></h6>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>import lance

dataset = lance.dataset(&quot;/tmp/images.lance&quot;)
dataset.create_index(
    &quot;category&quot;,
    &quot;BTREE&quot;,
)
</code></pre></div>

<p>Scalar indices can only speed up scans for basic filters using
equality, comparison, range (e.g. <code>my_col BETWEEN 0 AND 100</code>), and set
membership (e.g. <code>my_col IN (0, 1, 2)</code>)</p>
<p>Scalar indices can be used if the filter contains multiple indexed columns and
the filter criteria are AND'd or OR'd together
(e.g. <code>my_col &lt; 0 AND other_col&gt; 100</code>)</p>
<p>Scalar indices may be used if the filter contains non-indexed columns but,
depending on the structure of the filter, they may not be usable.  For example,
if the column <code>not_indexed</code> does not have a scalar index then the filter
<code>my_col = 0 OR not_indexed = 1</code> will not be able to use any scalar index on
<code>my_col</code>.</p>
<p>To determine if a scan is making use of a scalar index you can use
<code>explain_plan</code> to look at the query plan that lance has created.  Queries
that use scalar indices will either have a <code>ScalarIndexQuery</code> relation or a
<code>MaterializeIndex</code> operator.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.delete" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">delete</span><span class="p">(</span><span class="n">predicate</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.delete" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Delete rows from the dataset.</p>
<p>This marks rows as deleted, but does not physically remove them from the
files. This keeps the existing indexes still valid.</p>
<h6 id="lance.LanceDataset.delete--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.delete--parameters" title="Permanent link">&para;</a></h6>
<p>predicate : str or pa.compute.Expression
    The predicate to use to select rows to delete. May either be a SQL
    string or a pyarrow Expression.</p>
<h6 id="lance.LanceDataset.delete--examples">Examples<a class="headerlink" href="#lance.LanceDataset.delete--examples" title="Permanent link">&para;</a></h6>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]})
dataset = lance.write_dataset(table, "example")
dataset.delete("a = 1 or b in ('a', 'b')")
dataset.to_table()
pyarrow.Table
a: int64
b: string</p>
</blockquote>
</blockquote>
</blockquote>
<hr />
<p>a: [[3]]
b: [["c"]]</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.drop_columns" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">drop_columns</span><span class="p">(</span><span class="n">columns</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.drop_columns" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Drop one or more columns from the dataset</p>
<p>This is a metadata-only operation and does not remove the data from the
underlying storage. In order to remove the data, you must subsequently
call <code>compact_files</code> to rewrite the data without the removed columns and
then call <code>cleanup_old_versions</code> to remove the old files.</p>
<h6 id="lance.LanceDataset.drop_columns--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.drop_columns--parameters" title="Permanent link">&para;</a></h6>
<p>columns : list of str
    The names of the columns to drop. These can be nested column references
    (e.g. "a.b.c") or top-level column names (e.g. "a").</p>
<h6 id="lance.LanceDataset.drop_columns--examples">Examples<a class="headerlink" href="#lance.LanceDataset.drop_columns--examples" title="Permanent link">&para;</a></h6>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]})
dataset = lance.write_dataset(table, "example")
dataset.drop_columns(["a"])
dataset.to_table().to_pandas()
   b
0  a
1  b
2  c</p>
</blockquote>
</blockquote>
</blockquote>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.drop_index" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">drop_index</span><span class="p">(</span><span class="n">name</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.drop_index" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Drops an index from the dataset</p>
<p>Note: Indices are dropped by "index name".  This is not the same as the field
name. If you did not specify a name when you created the index then a name was
generated for you.  You can use the <code>list_indices</code> method to get the names of
the indices.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.get_fragment" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_fragment</span><span class="p">(</span><span class="n">fragment_id</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.get_fragment" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Get the fragment with fragment id.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.get_fragments" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_fragments</span><span class="p">(</span><span class="nb">filter</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.get_fragments" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Get all fragments from the dataset.</p>
<p>Note: filter is not supported yet.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.head" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">head</span><span class="p">(</span><span class="n">num_rows</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.head" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Load the first N rows of the dataset.</p>
<h6 id="lance.LanceDataset.head--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.head--parameters" title="Permanent link">&para;</a></h6>
<p>num_rows : int
    The number of rows to load.
**kwargs : dict, optional
    See scanner() method for full parameter description.</p>
<h6 id="lance.LanceDataset.head--returns">Returns<a class="headerlink" href="#lance.LanceDataset.head--returns" title="Permanent link">&para;</a></h6>
<p>table : Table</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.insert" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">insert</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;append&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.insert" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Insert data into the dataset.</p>
<h6 id="lance.LanceDataset.insert--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.insert--parameters" title="Permanent link">&para;</a></h6>
<p>data_obj: Reader-like
    The data to be written. Acceptable types are:
    - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader
    - Huggingface dataset
mode: str, default 'append'
    The mode to use when writing the data. Options are:
        <strong>create</strong> - create a new dataset (raises if uri already exists).
        <strong>overwrite</strong> - create a new snapshot version
        <strong>append</strong> - create a new version that is the concat of the input the
        latest version (raises if uri does not exist)
**kwargs : dict, optional
    Additional keyword arguments to pass to :func:<code>write_dataset</code>.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.join" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">join</span><span class="p">(</span><span class="n">right_dataset</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">right_keys</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">join_type</span><span class="o">=</span><span class="s1">&#39;left outer&#39;</span><span class="p">,</span> <span class="n">left_suffix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">right_suffix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">coalesce_keys</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_threads</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.join" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Not implemented (just override pyarrow dataset to prevent segfault)</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.merge" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">merge</span><span class="p">(</span><span class="n">data_obj</span><span class="p">,</span> <span class="n">left_on</span><span class="p">,</span> <span class="n">right_on</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.merge" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Merge another dataset into this one.</p>
<p>Performs a left join, where the dataset is the left side and data_obj
is the right side. Rows existing in the dataset but not on the left will
be filled with null values, unless Lance doesn't support null values for
some types, in which case an error will be raised.</p>
<h6 id="lance.LanceDataset.merge--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.merge--parameters" title="Permanent link">&para;</a></h6>
<p>data_obj: Reader-like
    The data to be merged. Acceptable types are:
    - Pandas DataFrame, Pyarrow Table, Dataset, Scanner,
    Iterator[RecordBatch], or RecordBatchReader
left_on: str
    The name of the column in the dataset to join on.
right_on: str or None
    The name of the column in data_obj to join on. If None, defaults to
    left_on.</p>
<h6 id="lance.LanceDataset.merge--examples">Examples<a class="headerlink" href="#lance.LanceDataset.merge--examples" title="Permanent link">&para;</a></h6>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
df = pa.table({'x': [1, 2, 3], 'y': ['a', 'b', 'c']})
dataset = lance.write_dataset(df, "dataset")
dataset.to_table().to_pandas()
   x  y
0  1  a
1  2  b
2  3  c
new_df = pa.table({'x': [1, 2, 3], 'z': ['d', 'e', 'f']})
dataset.merge(new_df, 'x')
dataset.to_table().to_pandas()
   x  y  z
0  1  a  d
1  2  b  e
2  3  c  f</p>
</blockquote>
</blockquote>
</blockquote>
<h6 id="lance.LanceDataset.merge--see-also">See Also<a class="headerlink" href="#lance.LanceDataset.merge--see-also" title="Permanent link">&para;</a></h6>
<p>LanceDataset.add_columns :
    Add new columns by computing batch-by-batch.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.merge_insert" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">merge_insert</span><span class="p">(</span><span class="n">on</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.merge_insert" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Returns a builder that can be used to create a "merge insert" operation</p>
<p>This operation can add rows, update rows, and remove rows in a single
transaction. It is a very generic tool that can be used to create
behaviors like "insert if not exists", "update or insert (i.e. upsert)",
or even replace a portion of existing data with new data (e.g. replace
all data where month="january")</p>
<p>The merge insert operation works by combining new data from a
<strong>source table</strong> with existing data in a <strong>target table</strong> by using a
join.  There are three categories of records.</p>
<p>"Matched" records are records that exist in both the source table and
the target table. "Not matched" records exist only in the source table
(e.g. these are new data). "Not matched by source" records exist only
in the target table (this is old data).</p>
<p>The builder returned by this method can be used to customize what
should happen for each category of data.</p>
<p>Please note that the data will be reordered as part of this
operation.  This is because updated rows will be deleted from the
dataset and then reinserted at the end with the new values.  The
order of the newly inserted rows may fluctuate randomly because a
hash-join operation is used internally.</p>
<h6 id="lance.LanceDataset.merge_insert--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.merge_insert--parameters" title="Permanent link">&para;</a></h6>


<details class="on" open>
  <summary>Union[str, Iterable[str]]</summary>
  <p>A column (or columns) to join on.  This is how records from the
source table and target table are matched.  Typically this is some
kind of key or id column.</p>
</details>        <h6 id="lance.LanceDataset.merge_insert--examples">Examples<a class="headerlink" href="#lance.LanceDataset.merge_insert--examples" title="Permanent link">&para;</a></h6>
<p>Use <code>when_matched_update_all()</code> and <code>when_not_matched_insert_all()</code> to
perform an "upsert" operation.  This will update rows that already exist
in the dataset and insert rows that do not exist.</p>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
table = pa.table({"a": [2, 1, 3], "b": ["a", "b", "c"]})
dataset = lance.write_dataset(table, "example")
new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]})</p>
<h5 id="lance.LanceDataset.merge_insert--perform-a-upsert-operation">Perform a "upsert" operation<a class="headerlink" href="#lance.LanceDataset.merge_insert--perform-a-upsert-operation" title="Permanent link">&para;</a></h5>
<p>dataset.merge_insert("a")     \
...             .when_matched_update_all()     \
...             .when_not_matched_insert_all() \
...             .execute(new_table)
{'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0}
dataset.to_table().sort_by("a").to_pandas()
   a  b
0  1  b
1  2  x
2  3  y
3  4  z</p>
</blockquote>
</blockquote>
</blockquote>
<p>Use <code>when_not_matched_insert_all()</code> to perform an "insert if not exists"
operation.  This will only insert rows that do not already exist in the
dataset.</p>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]})
dataset = lance.write_dataset(table, "example2")
new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]})</p>
<h5 id="lance.LanceDataset.merge_insert--perform-an-insert-if-not-exists-operation">Perform an "insert if not exists" operation<a class="headerlink" href="#lance.LanceDataset.merge_insert--perform-an-insert-if-not-exists-operation" title="Permanent link">&para;</a></h5>
<p>dataset.merge_insert("a")     \
...             .when_not_matched_insert_all() \
...             .execute(new_table)
{'num_inserted_rows': 1, 'num_updated_rows': 0, 'num_deleted_rows': 0}
dataset.to_table().sort_by("a").to_pandas()
   a  b
0  1  a
1  2  b
2  3  c
3  4  z</p>
</blockquote>
</blockquote>
</blockquote>
<p>You are not required to provide all the columns. If you only want to
update a subset of columns, you can omit columns you don't want to
update. Omitted columns will keep their existing values if they are
updated, or will be null if they are inserted.</p>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"], \
...                   "c": ["x", "y", "z"]})
dataset = lance.write_dataset(table, "example3")
new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]})</p>
<h5 id="lance.LanceDataset.merge_insert--perform-an-upsert-operation-only-updating-column-a">Perform an "upsert" operation, only updating column "a"<a class="headerlink" href="#lance.LanceDataset.merge_insert--perform-an-upsert-operation-only-updating-column-a" title="Permanent link">&para;</a></h5>
<p>dataset.merge_insert("a")     \
...             .when_matched_update_all()     \
...             .when_not_matched_insert_all() \
...             .execute(new_table)
{'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0}
dataset.to_table().sort_by("a").to_pandas()
   a  b     c
0  1  a     x
1  2  x     y
2  3  y     z
3  4  z  None</p>
</blockquote>
</blockquote>
</blockquote>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.migrate_manifest_paths_v2" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">migrate_manifest_paths_v2</span><span class="p">()</span></code>

<a href="#lance.LanceDataset.migrate_manifest_paths_v2" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Migrate the manifest paths to the new format.</p>
<p>This will update the manifest to use the new v2 format for paths.</p>
<p>This function is idempotent, and can be run multiple times without
changing the state of the object store.</p>
<p>DANGER: this should not be run while other concurrent operations are happening.
And it should also run until completion before resuming other operations.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.prewarm_index" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">prewarm_index</span><span class="p">(</span><span class="n">name</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.prewarm_index" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Prewarm an index</p>
<p>This will load the entire index into memory.  This can help avoid cold start
issues with index queries.  If the index does not fit in the index cache, then
this will result in wasted I/O.</p>
<h6 id="lance.LanceDataset.prewarm_index--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.prewarm_index--parameters" title="Permanent link">&para;</a></h6>
<p>name: str
    The name of the index to prewarm.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.replace_field_metadata" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">replace_field_metadata</span><span class="p">(</span><span class="n">field_name</span><span class="p">,</span> <span class="n">new_metadata</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.replace_field_metadata" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Replace the metadata of a field in the schema</p>
<h6 id="lance.LanceDataset.replace_field_metadata--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.replace_field_metadata--parameters" title="Permanent link">&para;</a></h6>
<p>field_name: str
    The name of the field to replace the metadata for
new_metadata: dict
    The new metadata to set</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.replace_schema" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">replace_schema</span><span class="p">(</span><span class="n">schema</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.replace_schema" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Not implemented (just override pyarrow dataset to prevent segfault)</p>
<p>See <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />method:<code>replace_schema_metadata</code> or <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />method:<code>replace_field_metadata</code></p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.replace_schema_metadata" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">replace_schema_metadata</span><span class="p">(</span><span class="n">new_metadata</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.replace_schema_metadata" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Replace the schema metadata of the dataset</p>
<h6 id="lance.LanceDataset.replace_schema_metadata--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.replace_schema_metadata--parameters" title="Permanent link">&para;</a></h6>
<p>new_metadata: dict
    The new metadata to set</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.restore" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">restore</span><span class="p">()</span></code>

<a href="#lance.LanceDataset.restore" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Restore the currently checked out version as the latest version of the dataset.</p>
<p>This creates a new commit.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.sample" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sample</span><span class="p">(</span><span class="n">num_rows</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">randomize_order</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.sample" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Select a random sample of data</p>
<h6 id="lance.LanceDataset.sample--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.sample--parameters" title="Permanent link">&para;</a></h6>
<p>num_rows: int
    number of rows to retrieve
columns: list of str, or dict of str to str default None
    List of column names to be fetched.
    Or a dictionary of column names to SQL expressions.
    All columns are fetched if None or unspecified.
**kwargs : dict, optional
    see scanner() method for full parameter description.</p>
<h6 id="lance.LanceDataset.sample--returns">Returns<a class="headerlink" href="#lance.LanceDataset.sample--returns" title="Permanent link">&para;</a></h6>
<p>table : Table</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.scanner" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">scanner</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">filter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nearest</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_readahead</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fragment_readahead</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scan_in_order</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fragments</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">full_text_query</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">prefilter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_row_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_row_address</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_stats</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fast_search</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">io_buffer_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">late_materialization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_scalar_index</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">include_deleted_rows</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scan_stats_callback</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">strict_batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.scanner" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Return a Scanner that can support various pushdowns.</p>
<h6 id="lance.LanceDataset.scanner--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.scanner--parameters" title="Permanent link">&para;</a></h6>
<p>columns: list of str, or dict of str to str default None
    List of column names to be fetched.
    Or a dictionary of column names to SQL expressions.
    All columns are fetched if None or unspecified.
filter: pa.compute.Expression or str
    Expression or str that is a valid SQL where clause. See
    <code>Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;</code>_
    for valid SQL expressions.
limit: int, default None
    Fetch up to this many rows. All rows if None or unspecified.
offset: int, default None
    Fetch starting with this row. 0 if None or unspecified.
nearest: dict, default None
    Get the rows corresponding to the K most similar vectors. Example:</p>
<div class="codehilite"><pre><span></span><code>.. code-block:: python

    {
        &quot;column&quot;: &lt;embedding col name&gt;,
        &quot;q&quot;: &lt;query vector as pa.Float32Array&gt;,
        &quot;k&quot;: 10,
        &quot;nprobes&quot;: 1,
        &quot;refine_factor&quot;: 1
    }
</code></pre></div>


<details class="batch_size" open>
  <summary>int, default None</summary>
  <p>The target size of batches returned.  In some cases batches can be up to
twice this size (but never larger than this).  In some cases batches can
be smaller than this size.</p>
</details>        <p>io_buffer_size: int, default None
    The size of the IO buffer.  See <code>ScannerBuilder.io_buffer_size</code>
    for more information.
batch_readahead: int, optional
    The number of batches to read ahead.
fragment_readahead: int, optional
    The number of fragments to read ahead.
scan_in_order: bool, default True
    Whether to read the fragments and batches in order. If false,
    throughput may be higher, but batches will be returned out of order
    and memory use might increase.
fragments: iterable of LanceFragment, default None
    If specified, only scan these fragments. If scan_in_order is True, then
    the fragments will be scanned in the order given.
prefilter: bool, default False
    If True then the filter will be applied before the vector query is run.
    This will generate more correct results but it may be a more costly
    query.  It's generally good when the filter is highly selective.</p>
<div class="codehilite"><pre><span></span><code>If False then the filter will be applied after the vector query is run.
This will perform well but the results may have fewer than the requested
number of rows (or be empty) if the rows closest to the query do not
match the filter.  It&#39;s generally good when the filter is not very
selective.
</code></pre></div>

<p>use_scalar_index: bool, default True
    Lance will automatically use scalar indices to optimize a query.  In some
    corner cases this can make query performance worse and this parameter can
    be used to disable scalar indices in these cases.
late_materialization: bool or List[str], default None
    Allows custom control over late materialization.  Late materialization
    fetches non-query columns using a take operation after the filter.  This
    is useful when there are few results or columns are very large.</p>
<div class="codehilite"><pre><span></span><code>Early materialization can be better when there are many results or the
columns are very narrow.

If True, then all columns are late materialized.
If False, then all columns are early materialized.
If a list of strings, then only the columns in the list are
late materialized.

The default uses a heuristic that assumes filters will select about 0.1%
of the rows.  If your filter is more selective (e.g. find by id) you may
want to set this to True.  If your filter is not very selective (e.g.
matches 20% of the rows) you may want to set this to False.
</code></pre></div>

<p>full_text_query: str or dict, optional
    query string to search for, the results will be ranked by BM25.
    e.g. "hello world", would match documents containing "hello" or "world".
    or a dictionary with the following keys:</p>
<div class="codehilite"><pre><span></span><code>- columns: list[str]
    The columns to search,
    currently only supports a single column in the columns list.
- query: str
    The query string to search for.
</code></pre></div>

<p>fast_search:  bool, default False
    If True, then the search will only be performed on the indexed data, which
    yields faster search time.
scan_stats_callback: Callable[[ScanStatistics], None], default None
    A callback function that will be called with the scan statistics after the
    scan is complete.  Errors raised by the callback will be logged but not
    re-raised.
include_deleted_rows: bool, default False
    If True, then rows that have been deleted, but are still present in the
    fragment, will be returned.  These rows will have the _rowid column set
    to null.  All other columns will reflect the value stored on disk and may
    not be null.</p>
<div class="codehilite"><pre><span></span><code>Note: if this is a search operation, or a take operation (including scalar
indexed scans) then deleted rows cannot be returned.
</code></pre></div>

<p>.. note::</p>
<div class="codehilite"><pre><span></span><code>For now, if BOTH filter and nearest is specified, then:

1. nearest is executed first.
2. The results are filtered afterwards.
</code></pre></div>

<p>For debugging ANN results, you can choose to not use the index
even if present by specifying <code>use_index=False</code>. For example,
the following will always return exact KNN results:</p>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>dataset.to_table(nearest={
    &quot;column&quot;: &quot;vector&quot;,
    &quot;k&quot;: 10,
    &quot;q&quot;: &lt;query vector&gt;,
    &quot;use_index&quot;: False
}
</code></pre></div>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.session" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">session</span><span class="p">()</span></code>

<a href="#lance.LanceDataset.session" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Return the dataset session, which holds the dataset's state.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.take" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">take</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.take" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Select rows of data by index.</p>
<h6 id="lance.LanceDataset.take--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.take--parameters" title="Permanent link">&para;</a></h6>
<p>indices : Array or array-like
    indices of rows to select in the dataset.
columns: list of str, or dict of str to str default None
    List of column names to be fetched.
    Or a dictionary of column names to SQL expressions.
    All columns are fetched if None or unspecified.</p>
<h6 id="lance.LanceDataset.take--returns">Returns<a class="headerlink" href="#lance.LanceDataset.take--returns" title="Permanent link">&para;</a></h6>
<p>table : pyarrow.Table</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.take_blobs" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">take_blobs</span><span class="p">(</span><span class="n">blob_column</span><span class="p">,</span> <span class="n">ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">addresses</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.take_blobs" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Select blobs by row IDs.</p>
<p>Instead of loading large binary blob data into memory before processing it,
this API allows you to open binary blob data as a regular Python file-like
object. For more details, see <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />class:<code>lance.BlobFile</code>.</p>
<p>Exactly one of ids, addresses, or indices must be specified.
Parameters</p>
<hr />
<p>blob_column : str
    The name of the blob column to select.
ids : Integer Array or array-like
    row IDs to select in the dataset.
addresses: Integer Array or array-like
    The (unstable) row addresses to select in the dataset.
indices : Integer Array or array-like
    The offset / indices of the row in the dataset.</p>
<h6 id="lance.LanceDataset.take_blobs--returns">Returns<a class="headerlink" href="#lance.LanceDataset.take_blobs--returns" title="Permanent link">&para;</a></h6>
<p>blob_files : List[BlobFile]</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.to_batches" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">to_batches</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">filter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nearest</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_readahead</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fragment_readahead</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scan_in_order</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">prefilter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_row_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_row_address</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_stats</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">full_text_query</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">io_buffer_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">late_materialization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_scalar_index</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">strict_batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.to_batches" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Read the dataset as materialized record batches.</p>
<h6 id="lance.LanceDataset.to_batches--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.to_batches--parameters" title="Permanent link">&para;</a></h6>
<p>**kwargs : dict, optional
    Arguments for <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />meth:<code>~LanceDataset.scanner</code>.</p>
<h6 id="lance.LanceDataset.to_batches--returns">Returns<a class="headerlink" href="#lance.LanceDataset.to_batches--returns" title="Permanent link">&para;</a></h6>
<p>record_batches : Iterator of <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />class:<code>~pyarrow.RecordBatch</code></p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.to_table" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">to_table</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">filter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nearest</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_readahead</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fragment_readahead</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scan_in_order</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">prefilter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_row_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_row_address</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_stats</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fast_search</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">full_text_query</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">io_buffer_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">late_materialization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_scalar_index</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">include_deleted_rows</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.to_table" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Read the data into memory as a <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />class:<code>pyarrow.Table</code></p>
<h6 id="lance.LanceDataset.to_table--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.to_table--parameters" title="Permanent link">&para;</a></h6>
<p>columns: list of str, or dict of str to str default None
    List of column names to be fetched.
    Or a dictionary of column names to SQL expressions.
    All columns are fetched if None or unspecified.
filter : pa.compute.Expression or str
    Expression or str that is a valid SQL where clause. See
    <code>Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;</code>_
    for valid SQL expressions.
limit: int, default None
    Fetch up to this many rows. All rows if None or unspecified.
offset: int, default None
    Fetch starting with this row. 0 if None or unspecified.
nearest: dict, default None
    Get the rows corresponding to the K most similar vectors. Example:</p>
<div class="codehilite"><pre><span></span><code>.. code-block:: python

    {
        &quot;column&quot;: &lt;embedding col name&gt;,
        &quot;q&quot;: &lt;query vector as pa.Float32Array&gt;,
        &quot;k&quot;: 10,
        &quot;metric&quot;: &quot;cosine&quot;,
        &quot;nprobes&quot;: 1,
        &quot;refine_factor&quot;: 1
    }
</code></pre></div>


<details class="batch_size" open>
  <summary>int, optional</summary>
  <p>The number of rows to read at a time.</p>
</details>        <p>io_buffer_size: int, default None
    The size of the IO buffer.  See <code>ScannerBuilder.io_buffer_size</code>
    for more information.
batch_readahead: int, optional
    The number of batches to read ahead.
fragment_readahead: int, optional
    The number of fragments to read ahead.
scan_in_order: bool, optional, default True
    Whether to read the fragments and batches in order. If false,
    throughput may be higher, but batches will be returned out of order
    and memory use might increase.
prefilter: bool, optional, default False
    Run filter before the vector search.
late_materialization: bool or List[str], default None
    Allows custom control over late materialization.  See
    <code>ScannerBuilder.late_materialization</code> for more information.
use_scalar_index: bool, default True
    Allows custom control over scalar index usage.  See
    <code>ScannerBuilder.use_scalar_index</code> for more information.
with_row_id: bool, optional, default False
    Return row ID.
with_row_address: bool, optional, default False
    Return row address
use_stats: bool, optional, default True
    Use stats pushdown during filters.
fast_search: bool, optional, default False
full_text_query: str or dict, optional
    query string to search for, the results will be ranked by BM25.
    e.g. "hello world", would match documents contains "hello" or "world".
    or a dictionary with the following keys:</p>
<div class="codehilite"><pre><span></span><code>- columns: list[str]
    The columns to search,
    currently only supports a single column in the columns list.
- query: str
    The query string to search for.
</code></pre></div>

<p>include_deleted_rows: bool, optional, default False
    If True, then rows that have been deleted, but are still present in the
    fragment, will be returned.  These rows will have the _rowid column set
    to null.  All other columns will reflect the value stored on disk and may
    not be null.</p>
<div class="codehilite"><pre><span></span><code>Note: if this is a search operation, or a take operation (including scalar
indexed scans) then deleted rows cannot be returned.
</code></pre></div>

<h6 id="lance.LanceDataset.to_table--notes">Notes<a class="headerlink" href="#lance.LanceDataset.to_table--notes" title="Permanent link">&para;</a></h6>
<p>If BOTH filter and nearest is specified, then:</p>
<ol>
<li>nearest is executed first.</li>
<li>The results are filtered afterward, unless pre-filter sets to True.</li>
</ol>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.update" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">update</span><span class="p">(</span><span class="n">updates</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.LanceDataset.update" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Update column values for rows matching where.</p>
<h6 id="lance.LanceDataset.update--parameters">Parameters<a class="headerlink" href="#lance.LanceDataset.update--parameters" title="Permanent link">&para;</a></h6>
<p>updates : dict of str to str
    A mapping of column names to a SQL expression.
where : str, optional
    A SQL predicate indicating which rows should be updated.</p>
<h6 id="lance.LanceDataset.update--returns">Returns<a class="headerlink" href="#lance.LanceDataset.update--returns" title="Permanent link">&para;</a></h6>
<p>updates : dict
    A dictionary containing the number of rows updated.</p>
<h6 id="lance.LanceDataset.update--examples">Examples<a class="headerlink" href="#lance.LanceDataset.update--examples" title="Permanent link">&para;</a></h6>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]})
dataset = lance.write_dataset(table, "example")
update_stats = dataset.update(dict(a = 'a + 2'), where="b != 'a'")
update_stats["num_updated_rows"] = 2
dataset.to_table().to_pandas()
   a  b
0  1  a
1  4  b
2  5  c</p>
</blockquote>
</blockquote>
</blockquote>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.validate" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">validate</span><span class="p">()</span></code>

<a href="#lance.LanceDataset.validate" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Validate the dataset.</p>
<p>This checks the integrity of the dataset and will raise an exception if
the dataset is corrupted.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceDataset.versions" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">versions</span><span class="p">()</span></code>

<a href="#lance.LanceDataset.versions" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Return all versions in this dataset.</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="lance.LanceFragment" class="doc doc-heading">
            <code>LanceFragment</code>


<a href="#lance.LanceFragment" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pyarrow.dataset.Fragment">Fragment</span></code></p>











  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="lance.LanceFragment.metadata" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">metadata</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.LanceFragment.metadata" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Return the metadata of this fragment.</p>
<h6 id="lance.LanceFragment.metadata--returns">Returns<a class="headerlink" href="#lance.LanceFragment.metadata--returns" title="Permanent link">&para;</a></h6>
<p>FragmentMetadata</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="lance.LanceFragment.num_deletions" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">num_deletions</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.LanceFragment.num_deletions" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Return the number of deleted rows in this fragment.</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="lance.LanceFragment.physical_rows" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">physical_rows</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.LanceFragment.physical_rows" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Return the number of rows originally in this fragment.</p>
<p>To get the number of rows after deletions, use
:meth:<code>count_rows</code> instead.</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="lance.LanceFragment.schema" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">schema</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.LanceFragment.schema" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Return the schema of this fragment.</p>

    </div>

</div>



<div class="doc doc-object doc-function">


<h4 id="lance.LanceFragment.create" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">create</span><span class="p">(</span><span class="n">dataset_uri</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">fragment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_rows_per_group</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">progress</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;append&#39;</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">data_storage_version</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_legacy_format</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">storage_options</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#lance.LanceFragment.create" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Create a :class:<code>FragmentMetadata</code> from the given data.</p>
<p>This can be used if the dataset is not yet created.</p>
<p>.. warning::</p>
<div class="codehilite"><pre><span></span><code>Internal API. This method is not intended to be used by end users.
</code></pre></div>

<h6 id="lance.LanceFragment.create--parameters">Parameters<a class="headerlink" href="#lance.LanceFragment.create--parameters" title="Permanent link">&para;</a></h6>
<p>dataset_uri: str
    The URI of the dataset.
fragment_id: int
    The ID of the fragment.
data: pa.Table or pa.RecordBatchReader
    The data to be written to the fragment.
schema: pa.Schema, optional
    The schema of the data. If not specified, the schema will be inferred
    from the data.
max_rows_per_group: int, default 1024
    The maximum number of rows per group in the data file.
progress: FragmentWriteProgress, optional
    <em>Experimental API</em>. Progress tracking for writing the fragment. Pass
    a custom class that defines hooks to be called when each fragment is
    starting to write and finishing writing.
mode: str, default "append"
    The write mode. If "append" is specified, the data will be checked
    against the existing dataset's schema. Otherwise, pass "create" or
    "overwrite" to assign new field ids to the schema.
data_storage_version: optional, str, default None
    The version of the data storage format to use. Newer versions are more
    efficient but require newer versions of lance to read.  The default (None)
    will use the latest stable version.  See the user guide for more details.
use_legacy_format: bool, default None
    Deprecated parameter.  Use data_storage_version instead.
storage_options : optional, dict
    Extra options that make sense for a particular storage connection. This is
    used to store connection parameters like credentials, endpoint, etc.</p>
<h6 id="lance.LanceFragment.create--see-also">See Also<a class="headerlink" href="#lance.LanceFragment.create--see-also" title="Permanent link">&para;</a></h6>
<p>lance.dataset.LanceOperation.Overwrite :
    The operation used to create a new dataset or overwrite one using
    fragments created with this API. See the doc page for an example of
    using this API.
lance.dataset.LanceOperation.Append :
    The operation used to append fragments created with this API to an
    existing dataset. See the doc page for an example of using this API.</p>
<h6 id="lance.LanceFragment.create--returns">Returns<a class="headerlink" href="#lance.LanceFragment.create--returns" title="Permanent link">&para;</a></h6>
<p>FragmentMetadata</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceFragment.create_from_file" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">create_from_file</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">fragment_id</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#lance.LanceFragment.create_from_file" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Create a fragment from the given datafile uri.</p>
<p>This can be used if the datafile is loss from dataset.</p>
<p>.. warning::</p>
<div class="codehilite"><pre><span></span><code>Internal API. This method is not intended to be used by end users.
</code></pre></div>

<h6 id="lance.LanceFragment.create_from_file--parameters">Parameters<a class="headerlink" href="#lance.LanceFragment.create_from_file--parameters" title="Permanent link">&para;</a></h6>
<p>filename: str
    The filename of the datafile.
dataset: LanceDataset
    The dataset that the fragment belongs to.
fragment_id: int
    The ID of the fragment.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceFragment.data_files" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">data_files</span><span class="p">()</span></code>

<a href="#lance.LanceFragment.data_files" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Return the data files of this fragment.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceFragment.delete" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">delete</span><span class="p">(</span><span class="n">predicate</span><span class="p">)</span></code>

<a href="#lance.LanceFragment.delete" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Delete rows from this Fragment.</p>
<p>This will add or update the deletion file of this fragment. It does not
modify or delete the data files of this fragment. If no rows are left after
the deletion, this method will return None.</p>
<p>.. warning::</p>
<div class="codehilite"><pre><span></span><code>Internal API. This method is not intended to be used by end users.
</code></pre></div>

<h6 id="lance.LanceFragment.delete--parameters">Parameters<a class="headerlink" href="#lance.LanceFragment.delete--parameters" title="Permanent link">&para;</a></h6>
<p>predicate: str
    A SQL predicate that specifies the rows to delete.</p>
<h6 id="lance.LanceFragment.delete--returns">Returns<a class="headerlink" href="#lance.LanceFragment.delete--returns" title="Permanent link">&para;</a></h6>
<p>FragmentMetadata or None
    A new fragment containing the new deletion file, or None if no rows left.</p>
<h6 id="lance.LanceFragment.delete--examples">Examples<a class="headerlink" href="#lance.LanceFragment.delete--examples" title="Permanent link">&para;</a></h6>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
tab = pa.table({"a": [1, 2, 3], "b": [4, 5, 6]})
dataset = lance.write_dataset(tab, "dataset")
frag = dataset.get_fragment(0)
frag.delete("a &gt; 1")
FragmentMetadata(id=0, files=[DataFile(path='...', fields=[0, 1], ...), ...)
frag.delete("a &gt; 0") is None
True</p>
</blockquote>
</blockquote>
</blockquote>
<h6 id="lance.LanceFragment.delete--see-also">See Also<a class="headerlink" href="#lance.LanceFragment.delete--see-also" title="Permanent link">&para;</a></h6>
<p>lance.dataset.LanceOperation.Delete :
    The operation used to commit these changes to a dataset. See the
    doc page for an example of using this API.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceFragment.deletion_file" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">deletion_file</span><span class="p">()</span></code>

<a href="#lance.LanceFragment.deletion_file" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Return the deletion file, if any</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceFragment.merge" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">merge</span><span class="p">(</span><span class="n">data_obj</span><span class="p">,</span> <span class="n">left_on</span><span class="p">,</span> <span class="n">right_on</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.LanceFragment.merge" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Merge another dataset into this fragment.</p>
<p>Performs a left join, where the fragment is the left side and data_obj
is the right side. Rows existing in the dataset but not on the left will
be filled with null values, unless Lance doesn't support null values for
some types, in which case an error will be raised.</p>
<h6 id="lance.LanceFragment.merge--parameters">Parameters<a class="headerlink" href="#lance.LanceFragment.merge--parameters" title="Permanent link">&para;</a></h6>
<p>data_obj: Reader-like
    The data to be merged. Acceptable types are:
    - Pandas DataFrame, Pyarrow Table, Dataset, Scanner,
    Iterator[RecordBatch], or RecordBatchReader
left_on: str
    The name of the column in the dataset to join on.
right_on: str or None
    The name of the column in data_obj to join on. If None, defaults to
    left_on.</p>
<h6 id="lance.LanceFragment.merge--examples">Examples<a class="headerlink" href="#lance.LanceFragment.merge--examples" title="Permanent link">&para;</a></h6>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
df = pa.table({'x': [1, 2, 3], 'y': ['a', 'b', 'c']})
dataset = lance.write_dataset(df, "dataset")
dataset.to_table().to_pandas()
   x  y
0  1  a
1  2  b
2  3  c
fragments = dataset.get_fragments()
new_df = pa.table({'x': [1, 2, 3], 'z': ['d', 'e', 'f']})
merged = []
schema = None
for f in fragments:
...     f, schema = f.merge(new_df, 'x')
...     merged.append(f)
merge = lance.LanceOperation.Merge(merged, schema)
dataset = lance.LanceDataset.commit("dataset", merge, read_version=1)
dataset.to_table().to_pandas()
   x  y  z
0  1  a  d
1  2  b  e
2  3  c  f</p>
</blockquote>
</blockquote>
</blockquote>
<h6 id="lance.LanceFragment.merge--see-also">See Also<a class="headerlink" href="#lance.LanceFragment.merge--see-also" title="Permanent link">&para;</a></h6>
<p>LanceDataset.merge_columns :
    Add columns to this Fragment.</p>
<h6 id="lance.LanceFragment.merge--returns">Returns<a class="headerlink" href="#lance.LanceFragment.merge--returns" title="Permanent link">&para;</a></h6>
<p>Tuple[FragmentMetadata, LanceSchema]
    A new fragment with the merged column(s) and the final schema.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceFragment.merge_columns" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">merge_columns</span><span class="p">(</span><span class="n">value_func</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reader_schema</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.LanceFragment.merge_columns" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Add columns to this Fragment.</p>
<p>.. warning::</p>
<div class="codehilite"><pre><span></span><code>Internal API. This method is not intended to be used by end users.
</code></pre></div>

<p>The parameters and their interpretation are the same as in the
:meth:<code>lance.dataset.LanceDataset.add_columns</code> operation.</p>
<p>The only difference is that, instead of modifying the dataset, a new
fragment is created.  The new schema of the fragment is returned as well.
These can be used in a later operation to commit the changes to the dataset.</p>
<h6 id="lance.LanceFragment.merge_columns--see-also">See Also<a class="headerlink" href="#lance.LanceFragment.merge_columns--see-also" title="Permanent link">&para;</a></h6>
<p>lance.dataset.LanceOperation.Merge :
    The operation used to commit these changes to the dataset. See the
    doc page for an example of using this API.</p>
<h6 id="lance.LanceFragment.merge_columns--returns">Returns<a class="headerlink" href="#lance.LanceFragment.merge_columns--returns" title="Permanent link">&para;</a></h6>
<p>Tuple[FragmentMetadata, LanceSchema]
    A new fragment with the added column(s) and the final schema.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceFragment.scanner" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">scanner</span><span class="p">(</span><span class="o">*</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">filter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_row_id</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">with_row_address</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">batch_readahead</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span></code>

<a href="#lance.LanceFragment.scanner" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>See Dataset::scanner for details</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="lance.LanceOperation" class="doc doc-heading">
            <code>LanceOperation</code>


<a href="#lance.LanceOperation" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">











  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="lance.LanceOperation.Append" class="doc doc-heading">
            <code>Append</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.LanceOperation.Append" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="BaseOperation (lance.dataset.LanceOperation.BaseOperation)" href="#lance.dataset.LanceOperation.BaseOperation">BaseOperation</a></code></p>


        <p>Append new rows to the dataset.</p>
<h6 id="lance.LanceOperation.Append--attributes">Attributes<a class="headerlink" href="#lance.LanceOperation.Append--attributes" title="Permanent link">&para;</a></h6>
<p>fragments: list[FragmentMetadata]
    The fragments that contain the new rows.</p>
<h6 id="lance.LanceOperation.Append--warning">Warning<a class="headerlink" href="#lance.LanceOperation.Append--warning" title="Permanent link">&para;</a></h6>
<p>This is an advanced API for distributed operations. To append to a
dataset on a single machine, use :func:<code>lance.write_dataset</code>.</p>
<h6 id="lance.LanceOperation.Append--examples">Examples<a class="headerlink" href="#lance.LanceOperation.Append--examples" title="Permanent link">&para;</a></h6>
<p>To append new rows to a dataset, first use
:meth:<code>lance.fragment.LanceFragment.create</code> to create fragments. Then
collect the fragment metadata into a list and pass it to this class.
Finally, pass the operation to the :meth:<code>LanceDataset.commit</code>
method to create the new dataset.</p>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]})
dataset = lance.write_dataset(tab1, "example")
tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]})
fragment = lance.fragment.LanceFragment.create("example", tab2)
operation = lance.LanceOperation.Append([fragment])
dataset = lance.LanceDataset.commit("example", operation,
...                                     read_version=dataset.version)
dataset.to_table().to_pandas()
   a  b
0  1  a
1  2  b
2  3  c
3  4  d</p>
</blockquote>
</blockquote>
</blockquote>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.LanceOperation.BaseOperation" class="doc doc-heading">
            <code>BaseOperation</code>


<a href="#lance.LanceOperation.BaseOperation" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="abc.ABC">ABC</span></code></p>


        <p>Base class for operations that can be applied to a dataset.</p>
<p>See available operations under :class:<code>LanceOperation</code>.</p>








    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.LanceOperation.CreateIndex" class="doc doc-heading">
            <code>CreateIndex</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.LanceOperation.CreateIndex" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="BaseOperation (lance.dataset.LanceOperation.BaseOperation)" href="#lance.dataset.LanceOperation.BaseOperation">BaseOperation</a></code></p>


        <p>Operation that creates an index on the dataset.</p>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.LanceOperation.DataReplacement" class="doc doc-heading">
            <code>DataReplacement</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.LanceOperation.DataReplacement" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="BaseOperation (lance.dataset.LanceOperation.BaseOperation)" href="#lance.dataset.LanceOperation.BaseOperation">BaseOperation</a></code></p>


        <p>Operation that replaces existing datafiles in the dataset.</p>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.LanceOperation.DataReplacementGroup" class="doc doc-heading">
            <code>DataReplacementGroup</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.LanceOperation.DataReplacementGroup" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">


        <p>Group of data replacements</p>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.LanceOperation.Delete" class="doc doc-heading">
            <code>Delete</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.LanceOperation.Delete" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="BaseOperation (lance.dataset.LanceOperation.BaseOperation)" href="#lance.dataset.LanceOperation.BaseOperation">BaseOperation</a></code></p>


        <p>Remove fragments or rows from the dataset.</p>
<h6 id="lance.LanceOperation.Delete--attributes">Attributes<a class="headerlink" href="#lance.LanceOperation.Delete--attributes" title="Permanent link">&para;</a></h6>
<p>updated_fragments: list[FragmentMetadata]
    The fragments that have been updated with new deletion vectors.
deleted_fragment_ids: list[int]
    The ids of the fragments that have been deleted entirely. These are
    the fragments where :meth:<code>LanceFragment.delete()</code> returned None.
predicate: str
    The original SQL predicate used to select the rows to delete.</p>
<h6 id="lance.LanceOperation.Delete--warning">Warning<a class="headerlink" href="#lance.LanceOperation.Delete--warning" title="Permanent link">&para;</a></h6>
<p>This is an advanced API for distributed operations. To delete rows from
dataset on a single machine, use :meth:<code>lance.LanceDataset.delete</code>.</p>
<h6 id="lance.LanceOperation.Delete--examples">Examples<a class="headerlink" href="#lance.LanceOperation.Delete--examples" title="Permanent link">&para;</a></h6>
<p>To delete rows from a dataset, call :meth:<code>lance.fragment.LanceFragment.delete</code>
on each of the fragments. If that returns a new fragment, add that to
the <code>updated_fragments</code> list. If it returns None, that means the whole
fragment was deleted, so add the fragment id to the <code>deleted_fragment_ids</code>.
Finally, pass the operation to the :meth:<code>LanceDataset.commit</code> method to
complete the deletion operation.</p>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
table = pa.table({"a": [1, 2], "b": ["a", "b"]})
dataset = lance.write_dataset(table, "example")
table = pa.table({"a": [3, 4], "b": ["c", "d"]})
dataset = lance.write_dataset(table, "example", mode="append")
dataset.to_table().to_pandas()
   a  b
0  1  a
1  2  b
2  3  c
3  4  d
predicate = "a &gt;= 2"
updated_fragments = []
deleted_fragment_ids = []
for fragment in dataset.get_fragments():
...     new_fragment = fragment.delete(predicate)
...     if new_fragment is not None:
...         updated_fragments.append(new_fragment)
...     else:
...         deleted_fragment_ids.append(fragment.fragment_id)
operation = lance.LanceOperation.Delete(updated_fragments,
...                                         deleted_fragment_ids,
...                                         predicate)
dataset = lance.LanceDataset.commit("example", operation,
...                                     read_version=dataset.version)
dataset.to_table().to_pandas()
   a  b
0  1  a</p>
</blockquote>
</blockquote>
</blockquote>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.LanceOperation.Merge" class="doc doc-heading">
            <code>Merge</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.LanceOperation.Merge" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="BaseOperation (lance.dataset.LanceOperation.BaseOperation)" href="#lance.dataset.LanceOperation.BaseOperation">BaseOperation</a></code></p>


        <p>Operation that adds columns. Unlike Overwrite, this should not change
the structure of the fragments, allowing existing indices to be kept.</p>
<h6 id="lance.LanceOperation.Merge--attributes">Attributes<a class="headerlink" href="#lance.LanceOperation.Merge--attributes" title="Permanent link">&para;</a></h6>
<p>fragments: iterable of FragmentMetadata
    The fragments that make up the new dataset.
schema: LanceSchema or pyarrow.Schema
    The schema of the new dataset. Passing a LanceSchema is preferred,
    and passing a pyarrow.Schema is deprecated.</p>
<h6 id="lance.LanceOperation.Merge--warning">Warning<a class="headerlink" href="#lance.LanceOperation.Merge--warning" title="Permanent link">&para;</a></h6>
<p>This is an advanced API for distributed operations. To overwrite or
create new dataset on a single machine, use :func:<code>lance.write_dataset</code>.</p>
<h6 id="lance.LanceOperation.Merge--examples">Examples<a class="headerlink" href="#lance.LanceOperation.Merge--examples" title="Permanent link">&para;</a></h6>
<p>To add new columns to a dataset, first define a method that will create
the new columns based on the existing columns. Then use
:meth:<code>lance.fragment.LanceFragment.add_columns</code></p>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
import pyarrow.compute as pc
table = pa.table({"a": [1, 2, 3, 4], "b": ["a", "b", "c", "d"]})
dataset = lance.write_dataset(table, "example")
dataset.to_table().to_pandas()
   a  b
0  1  a
1  2  b
2  3  c
3  4  d
def double_a(batch: pa.RecordBatch) -&gt; pa.RecordBatch:
...     doubled = pc.multiply(batch["a"], 2)
...     return pa.record_batch([doubled], ["a_doubled"])
fragments = []
for fragment in dataset.get_fragments():
...     new_fragment, new_schema = fragment.merge_columns(double_a,
...                                                       columns=['a'])
...     fragments.append(new_fragment)
operation = lance.LanceOperation.Merge(fragments, new_schema)
dataset = lance.LanceDataset.commit("example", operation,
...                                     read_version=dataset.version)
dataset.to_table().to_pandas()
   a  b  a_doubled
0  1  a          2
1  2  b          4
2  3  c          6
3  4  d          8</p>
</blockquote>
</blockquote>
</blockquote>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.LanceOperation.Overwrite" class="doc doc-heading">
            <code>Overwrite</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.LanceOperation.Overwrite" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="BaseOperation (lance.dataset.LanceOperation.BaseOperation)" href="#lance.dataset.LanceOperation.BaseOperation">BaseOperation</a></code></p>


        <p>Overwrite or create a new dataset.</p>
<h6 id="lance.LanceOperation.Overwrite--attributes">Attributes<a class="headerlink" href="#lance.LanceOperation.Overwrite--attributes" title="Permanent link">&para;</a></h6>
<p>new_schema: pyarrow.Schema
    The schema of the new dataset.
fragments: list[FragmentMetadata]
    The fragments that make up the new dataset.</p>
<h6 id="lance.LanceOperation.Overwrite--warning">Warning<a class="headerlink" href="#lance.LanceOperation.Overwrite--warning" title="Permanent link">&para;</a></h6>
<p>This is an advanced API for distributed operations. To overwrite or
create new dataset on a single machine, use :func:<code>lance.write_dataset</code>.</p>
<h6 id="lance.LanceOperation.Overwrite--examples">Examples<a class="headerlink" href="#lance.LanceOperation.Overwrite--examples" title="Permanent link">&para;</a></h6>
<p>To create or overwrite a dataset, first use
:meth:<code>lance.fragment.LanceFragment.create</code> to create fragments. Then
collect the fragment metadata into a list and pass it along with the
schema to this class. Finally, pass the operation to the
:meth:<code>LanceDataset.commit</code> method to create the new dataset.</p>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]})
tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]})
fragment1 = lance.fragment.LanceFragment.create("example", tab1)
fragment2 = lance.fragment.LanceFragment.create("example", tab2)
fragments = [fragment1, fragment2]
operation = lance.LanceOperation.Overwrite(tab1.schema, fragments)
dataset = lance.LanceDataset.commit("example", operation)
dataset.to_table().to_pandas()
   a  b
0  1  a
1  2  b
2  3  c
3  4  d</p>
</blockquote>
</blockquote>
</blockquote>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.LanceOperation.Project" class="doc doc-heading">
            <code>Project</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.LanceOperation.Project" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="BaseOperation (lance.dataset.LanceOperation.BaseOperation)" href="#lance.dataset.LanceOperation.BaseOperation">BaseOperation</a></code></p>


        <p>Operation that project columns.
Use this operator for drop column or rename/swap column.</p>
<h6 id="lance.LanceOperation.Project--attributes">Attributes<a class="headerlink" href="#lance.LanceOperation.Project--attributes" title="Permanent link">&para;</a></h6>
<p>schema: LanceSchema
    The lance schema of the new dataset.</p>
<h6 id="lance.LanceOperation.Project--examples">Examples<a class="headerlink" href="#lance.LanceOperation.Project--examples" title="Permanent link">&para;</a></h6>
<p>Use the projece operator to swap column:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
import pyarrow.compute as pc
from lance.schema import LanceSchema
table = pa.table({"a": [1, 2], "b": ["a", "b"], "b1": ["c", "d"]})
dataset = lance.write_dataset(table, "example")
dataset.to_table().to_pandas()
   a  b b1
0  1  a  c
1  2  b  d</p>
<h6 id="lance.LanceOperation.Project--rename-column-b-into-b0-and-rename-b1-into-b">rename column <code>b</code> into <code>b0</code> and rename b1 into <code>b</code><a class="headerlink" href="#lance.LanceOperation.Project--rename-column-b-into-b0-and-rename-b1-into-b" title="Permanent link">&para;</a></h6>
<p>table = pa.table({"a": [3, 4], "b0": ["a", "b"], "b": ["c", "d"]})
lance_schema = LanceSchema.from_pyarrow(table.schema)
operation = lance.LanceOperation.Project(lance_schema)
dataset = lance.LanceDataset.commit("example", operation, read_version=1)
dataset.to_table().to_pandas()
   a b0  b
0  1  a  c
1  2  b  d</p>
</blockquote>
</blockquote>
</blockquote>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.LanceOperation.Restore" class="doc doc-heading">
            <code>Restore</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.LanceOperation.Restore" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="BaseOperation (lance.dataset.LanceOperation.BaseOperation)" href="#lance.dataset.LanceOperation.BaseOperation">BaseOperation</a></code></p>


        <p>Operation that restores a previous version of the dataset.</p>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.LanceOperation.Rewrite" class="doc doc-heading">
            <code>Rewrite</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.LanceOperation.Rewrite" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="BaseOperation (lance.dataset.LanceOperation.BaseOperation)" href="#lance.dataset.LanceOperation.BaseOperation">BaseOperation</a></code></p>


        <p>Operation that rewrites one or more files and indices into one
or more files and indices.</p>
<h6 id="lance.LanceOperation.Rewrite--attributes">Attributes<a class="headerlink" href="#lance.LanceOperation.Rewrite--attributes" title="Permanent link">&para;</a></h6>
<p>groups: list[RewriteGroup]
    Groups of files that have been rewritten.
rewritten_indices: list[RewrittenIndex]
    Indices that have been rewritten.</p>
<h6 id="lance.LanceOperation.Rewrite--warning">Warning<a class="headerlink" href="#lance.LanceOperation.Rewrite--warning" title="Permanent link">&para;</a></h6>
<p>This is an advanced API not intended for general use.</p>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.LanceOperation.RewriteGroup" class="doc doc-heading">
            <code>RewriteGroup</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.LanceOperation.RewriteGroup" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">


        <p>Collection of rewritten files</p>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.LanceOperation.RewrittenIndex" class="doc doc-heading">
            <code>RewrittenIndex</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.LanceOperation.RewrittenIndex" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">


        <p>An index that has been rewritten</p>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.LanceOperation.Update" class="doc doc-heading">
            <code>Update</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.LanceOperation.Update" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="BaseOperation (lance.dataset.LanceOperation.BaseOperation)" href="#lance.dataset.LanceOperation.BaseOperation">BaseOperation</a></code></p>


        <p>Operation that updates rows in the dataset.</p>
<h6 id="lance.LanceOperation.Update--attributes">Attributes<a class="headerlink" href="#lance.LanceOperation.Update--attributes" title="Permanent link">&para;</a></h6>
<p>removed_fragment_ids: list[int]
    The ids of the fragments that have been removed entirely.
updated_fragments: list[FragmentMetadata]
    The fragments that have been updated with new deletion vectors.
new_fragments: list[FragmentMetadata]
    The fragments that contain the new rows.</p>










  <div class="doc doc-children">











  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="lance.LanceScanner" class="doc doc-heading">
            <code>LanceScanner</code>


<a href="#lance.LanceScanner" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pyarrow.dataset.Scanner">Scanner</span></code></p>











  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="lance.LanceScanner.dataset_schema" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">dataset_schema</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.LanceScanner.dataset_schema" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>The schema with which batches will be read from fragments.</p>

    </div>

</div>



<div class="doc doc-object doc-function">


<h4 id="lance.LanceScanner.analyze_plan" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">analyze_plan</span><span class="p">()</span></code>

<a href="#lance.LanceScanner.analyze_plan" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Execute the plan for this scanner and display with runtime metrics.</p>
<h6 id="lance.LanceScanner.analyze_plan--parameters">Parameters<a class="headerlink" href="#lance.LanceScanner.analyze_plan--parameters" title="Permanent link">&para;</a></h6>
<p>verbose : bool, default False
    Use a verbose output format.</p>
<h6 id="lance.LanceScanner.analyze_plan--returns">Returns<a class="headerlink" href="#lance.LanceScanner.analyze_plan--returns" title="Permanent link">&para;</a></h6>
<p>plan : str</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceScanner.count_rows" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">count_rows</span><span class="p">()</span></code>

<a href="#lance.LanceScanner.count_rows" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Count rows matching the scanner filter.</p>
<h6 id="lance.LanceScanner.count_rows--returns">Returns<a class="headerlink" href="#lance.LanceScanner.count_rows--returns" title="Permanent link">&para;</a></h6>
<p>count : int</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceScanner.explain_plan" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">explain_plan</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#lance.LanceScanner.explain_plan" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Return the execution plan for this scanner.</p>
<h6 id="lance.LanceScanner.explain_plan--parameters">Parameters<a class="headerlink" href="#lance.LanceScanner.explain_plan--parameters" title="Permanent link">&para;</a></h6>
<p>verbose : bool, default False
    Use a verbose output format.</p>
<h6 id="lance.LanceScanner.explain_plan--returns">Returns<a class="headerlink" href="#lance.LanceScanner.explain_plan--returns" title="Permanent link">&para;</a></h6>
<p>plan : str</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceScanner.from_batches" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">from_batches</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#lance.LanceScanner.from_batches" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Not implemented</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceScanner.from_dataset" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">from_dataset</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#lance.LanceScanner.from_dataset" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Not implemented</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceScanner.from_fragment" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">from_fragment</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#lance.LanceScanner.from_fragment" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Not implemented</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceScanner.head" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">head</span><span class="p">(</span><span class="n">num_rows</span><span class="p">)</span></code>

<a href="#lance.LanceScanner.head" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Load the first N rows of the dataset.</p>
<h6 id="lance.LanceScanner.head--parameters">Parameters<a class="headerlink" href="#lance.LanceScanner.head--parameters" title="Permanent link">&para;</a></h6>
<p>num_rows : int
    The number of rows to load.</p>
<h6 id="lance.LanceScanner.head--returns">Returns<a class="headerlink" href="#lance.LanceScanner.head--returns" title="Permanent link">&para;</a></h6>
<p>Table</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceScanner.scan_batches" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">scan_batches</span><span class="p">()</span></code>

<a href="#lance.LanceScanner.scan_batches" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Consume a Scanner in record batches with corresponding fragments.</p>
<h6 id="lance.LanceScanner.scan_batches--returns">Returns<a class="headerlink" href="#lance.LanceScanner.scan_batches--returns" title="Permanent link">&para;</a></h6>
<p>record_batches : iterator of TaggedRecordBatch</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceScanner.take" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">take</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span></code>

<a href="#lance.LanceScanner.take" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Not implemented</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.LanceScanner.to_table" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">to_table</span><span class="p">()</span></code>

<a href="#lance.LanceScanner.to_table" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Read the data into memory and return a pyarrow Table.</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="lance.MergeInsertBuilder" class="doc doc-heading">
            <code>MergeInsertBuilder</code>


<a href="#lance.MergeInsertBuilder" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="lance.lance._MergeInsertBuilder">_MergeInsertBuilder</span></code></p>











  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="lance.MergeInsertBuilder.conflict_retries" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">conflict_retries</span><span class="p">(</span><span class="n">max_retries</span><span class="p">)</span></code>

<a href="#lance.MergeInsertBuilder.conflict_retries" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Set number of times to retry the operation if there is contention.</p>
<p>If this is set &gt; 0, then the operation will keep a copy of the input data
either in memory or on disk (depending on the size of the data) and will
retry the operation if there is contention.</p>
<p>Default is 10.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.MergeInsertBuilder.execute" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">execute</span><span class="p">(</span><span class="n">data_obj</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.MergeInsertBuilder.execute" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Executes the merge insert operation</p>
<p>This function updates the original dataset and returns a dictionary with
information about merge statistics - i.e. the number of inserted, updated,
and deleted rows.</p>
<h6 id="lance.MergeInsertBuilder.execute--parameters">Parameters<a class="headerlink" href="#lance.MergeInsertBuilder.execute--parameters" title="Permanent link">&para;</a></h6>


<details class="data_obj" open>
  <summary>ReaderLike</summary>
  <p>The new data to use as the source table for the operation.  This parameter
can be any source of data (e.g. table / dataset) that
:func:<code>~lance.write_dataset</code> accepts.</p>
</details>        <p>schema: Optional[pa.Schema]
    The schema of the data.  This only needs to be supplied whenever the data
    source is some kind of generator.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.MergeInsertBuilder.execute_uncommitted" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">execute_uncommitted</span><span class="p">(</span><span class="n">data_obj</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.MergeInsertBuilder.execute_uncommitted" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Executes the merge insert operation without committing</p>
<p>This function updates the original dataset and returns a dictionary with
information about merge statistics - i.e. the number of inserted, updated,
and deleted rows.</p>
<h6 id="lance.MergeInsertBuilder.execute_uncommitted--parameters">Parameters<a class="headerlink" href="#lance.MergeInsertBuilder.execute_uncommitted--parameters" title="Permanent link">&para;</a></h6>


<details class="data_obj" open>
  <summary>ReaderLike</summary>
  <p>The new data to use as the source table for the operation.  This parameter
can be any source of data (e.g. table / dataset) that
:func:<code>~lance.write_dataset</code> accepts.</p>
</details>        <p>schema: Optional[pa.Schema]
    The schema of the data.  This only needs to be supplied whenever the data
    source is some kind of generator.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.MergeInsertBuilder.retry_timeout" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">retry_timeout</span><span class="p">(</span><span class="n">timeout</span><span class="p">)</span></code>

<a href="#lance.MergeInsertBuilder.retry_timeout" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Set the timeout used to limit retries.</p>
<p>This is the maximum time to spend on the operation before giving up. At
least one attempt will be made, regardless of how long it takes to complete.
Subsequent attempts will be cancelled once this timeout is reached. If
the timeout has been reached during the first attempt, the operation
will be cancelled immediately before making a second attempt.</p>
<p>The default is 30 seconds.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.MergeInsertBuilder.when_matched_update_all" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">when_matched_update_all</span><span class="p">(</span><span class="n">condition</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.MergeInsertBuilder.when_matched_update_all" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Configure the operation to update matched rows</p>
<p>After this method is called, when the merge insert operation executes,
any rows that match both the source table and the target table will be
updated.  The rows from the target table will be removed and the rows
from the source table will be added.</p>
<p>An optional condition may be specified.  This should be an SQL filter
and, if present, then only matched rows that also satisfy this filter will
be updated.  The SQL filter should use the prefix <code>target.</code> to refer to
columns in the target table and the prefix <code>source.</code> to refer to columns
in the source table.  For example, <code>source.last_update &lt; target.last_update</code>.</p>
<p>If a condition is specified and rows do not satisfy the condition then these
rows will not be updated.  Failure to satisfy the filter does not cause
a "matched" row to become a "not matched" row.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.MergeInsertBuilder.when_not_matched_by_source_delete" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">when_not_matched_by_source_delete</span><span class="p">(</span><span class="n">expr</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.MergeInsertBuilder.when_not_matched_by_source_delete" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Configure the operation to delete source rows that do not match</p>
<p>After this method is called, when the merge insert operation executes,
any rows that exist only in the target table will be deleted.  An
optional filter can be specified to limit the scope of the delete
operation.  If given (as an SQL filter) then only rows which match
the filter will be deleted.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.MergeInsertBuilder.when_not_matched_insert_all" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">when_not_matched_insert_all</span><span class="p">()</span></code>

<a href="#lance.MergeInsertBuilder.when_not_matched_insert_all" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Configure the operation to insert not matched rows</p>
<p>After this method is called, when the merge insert operation executes,
any rows that exist only in the source table will be inserted into
the target table.</p>


    </div>

</div>



  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h3 id="lance.batch_udf" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">batch_udf</span><span class="p">(</span><span class="n">output_schema</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">checkpoint_file</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.batch_udf" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Create a user defined function (UDF) that adds columns to a dataset.</p>
<p>This function is used to add columns to a dataset. It takes a function that
takes a single argument, a RecordBatch, and returns a RecordBatch. The
function is called once for each batch in the dataset. The function should
not modify the input batch, but instead create a new batch with the new
columns added.</p>
<h5 id="lance.batch_udf--parameters">Parameters<a class="headerlink" href="#lance.batch_udf--parameters" title="Permanent link">&para;</a></h5>
<p>output_schema : Schema, optional
    The schema of the output RecordBatch. This is used to validate the
    output of the function. If not provided, the schema of the first output
    RecordBatch will be used.
checkpoint_file : str or Path, optional
    If specified, this file will be used as a cache for unsaved results of
    this UDF. If the process fails, and you call add_columns again with this
    same file, it will resume from the last saved state. This is useful for
    long running processes that may fail and need to be resumed. This file
    may get very large. It will hold up to an entire data files' worth of
    results on disk, which can be multiple gigabytes of data.</p>
<h5 id="lance.batch_udf--returns">Returns<a class="headerlink" href="#lance.batch_udf--returns" title="Permanent link">&para;</a></h5>
<p>AddColumnsUDF</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="lance.json_to_schema" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">json_to_schema</span><span class="p">(</span><span class="n">schema_json</span><span class="p">)</span></code>

<a href="#lance.json_to_schema" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Converts a JSON string to a PyArrow schema.</p>
<h5 id="lance.json_to_schema--parameters">Parameters<a class="headerlink" href="#lance.json_to_schema--parameters" title="Permanent link">&para;</a></h5>
<p>schema_json: Dict[str, Any]
    The JSON payload to convert to a PyArrow Schema.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="lance.schema_to_json" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">schema_to_json</span><span class="p">(</span><span class="n">schema</span><span class="p">)</span></code>

<a href="#lance.schema_to_json" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Converts a pyarrow schema to a JSON string.</p>
<h5 id="lance.schema_to_json--parameters">Parameters<a class="headerlink" href="#lance.schema_to_json--parameters" title="Permanent link">&para;</a></h5>


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="lance.write_dataset" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">write_dataset</span><span class="p">(</span><span class="n">data_obj</span><span class="p">,</span> <span class="n">uri</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;create&#39;</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">max_rows_per_file</span><span class="o">=</span><span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">max_rows_per_group</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">max_bytes_per_file</span><span class="o">=</span><span class="mi">90</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">commit_lock</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">progress</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">storage_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">data_storage_version</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_legacy_format</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">enable_v2_manifest_paths</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">enable_move_stable_row_ids</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#lance.write_dataset" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Write a given data_obj to the given uri</p>
<h5 id="lance.write_dataset--parameters">Parameters<a class="headerlink" href="#lance.write_dataset--parameters" title="Permanent link">&para;</a></h5>
<p>data_obj: Reader-like
    The data to be written. Acceptable types are:
    - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader
    - Huggingface dataset
uri: str, Path, or LanceDataset
    Where to write the dataset to (directory). If a LanceDataset is passed,
    the session will be reused.
schema: Schema, optional
    If specified and the input is a pandas DataFrame, use this schema
    instead of the default pandas to arrow table conversion.
mode: str
    <strong>create</strong> - create a new dataset (raises if uri already exists).
    <strong>overwrite</strong> - create a new snapshot version
    <strong>append</strong> - create a new version that is the concat of the input the
    latest version (raises if uri does not exist)
max_rows_per_file: int, default 1024 * 1024
    The max number of rows to write before starting a new file
max_rows_per_group: int, default 1024
    The max number of rows before starting a new group (in the same file)
max_bytes_per_file: int, default 90 * 1024 * 1024 * 1024
    The max number of bytes to write before starting a new file. This is a
    soft limit. This limit is checked after each group is written, which
    means larger groups may cause this to be overshot meaningfully. This
    defaults to 90 GB, since we have a hard limit of 100 GB per file on
    object stores.
commit_lock : CommitLock, optional
    A custom commit lock.  Only needed if your object store does not support
    atomic commits.  See the user guide for more details.
progress: FragmentWriteProgress, optional
    <em>Experimental API</em>. Progress tracking for writing the fragment. Pass
    a custom class that defines hooks to be called when each fragment is
    starting to write and finishing writing.
storage_options : optional, dict
    Extra options that make sense for a particular storage connection. This is
    used to store connection parameters like credentials, endpoint, etc.
data_storage_version: optional, str, default None
    The version of the data storage format to use. Newer versions are more
    efficient but require newer versions of lance to read.  The default (None)
    will use the latest stable version.  See the user guide for more details.
use_legacy_format : optional, bool, default None
    Deprecated method for setting the data storage version. Use the
    <code>data_storage_version</code> parameter instead.
enable_v2_manifest_paths : bool, optional
    If True, and this is a new dataset, uses the new V2 manifest paths.
    These paths provide more efficient opening of datasets with many
    versions on object stores. This parameter has no effect if the dataset
    already exists. To migrate an existing dataset, instead use the
    :meth:<code>LanceDataset.migrate_manifest_paths_v2</code> method. Default is False.
enable_move_stable_row_ids : bool, optional
    Experimental parameter: if set to true, the writer will use move-stable row ids.
    These row ids are stable after compaction operations, but not after updates.
    This makes compaction more efficient, since with stable row ids no
    secondary indices need to be updated to point to new row ids.</p>


    </div>

</div>



  </div>

    </div>

</div><h2 id="lancedataset">lance.dataset<a class="headerlink" href="#lancedataset" title="Permanent link">&para;</a></h2>


<div class="doc doc-object doc-module">



<h2 id="lance.dataset" class="doc doc-heading">
            <code>lance.dataset</code>


<a href="#lance.dataset" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">









  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="lance.dataset.DataStatistics" class="doc doc-heading">
            <code>DataStatistics</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.dataset.DataStatistics" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">


        <p>Statistics about the data in the dataset</p>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="lance.dataset.DatasetOptimizer" class="doc doc-heading">
            <code>DatasetOptimizer</code>


<a href="#lance.dataset.DatasetOptimizer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">











  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="lance.dataset.DatasetOptimizer.compact_files" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compact_files</span><span class="p">(</span><span class="o">*</span><span class="p">,</span> <span class="n">target_rows_per_fragment</span><span class="o">=</span><span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">max_rows_per_group</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">max_bytes_per_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">materialize_deletions</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">materialize_deletions_threshold</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">num_threads</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.dataset.DatasetOptimizer.compact_files" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Compacts small files in the dataset, reducing total number of files.</p>


<details class="this-does-a-few-things" open>
  <summary>This does a few things</summary>
  <ul>
<li>Removes deleted rows from fragments</li>
<li>Removes dropped columns from fragments</li>
<li>Merges small fragments into larger ones</li>
</ul>
</details>        <p>This method preserves the insertion order of the dataset. This may mean
it leaves small fragments in the dataset if they are not adjacent to
other fragments that need compaction. For example, if you have fragments
with row counts 5 million, 100, and 5 million, the middle fragment will
not be compacted because the fragments it is adjacent to do not need
compaction.</p>
<h6 id="lance.dataset.DatasetOptimizer.compact_files--parameters">Parameters<a class="headerlink" href="#lance.dataset.DatasetOptimizer.compact_files--parameters" title="Permanent link">&para;</a></h6>
<p>target_rows_per_fragment: int, default 1024*1024
    The target number of rows per fragment. This is the number of rows
    that will be in each fragment after compaction.
max_rows_per_group: int, default 1024
    Max number of rows per group. This does not affect which fragments
    need compaction, but does affect how they are re-written if selected.</p>
<div class="codehilite"><pre><span></span><code>This setting only affects datasets using the legacy storage format.
The newer format does not require row groups.
</code></pre></div>

<p>max_bytes_per_file: Optional[int], default None
    Max number of bytes in a single file.  This does not affect which
    fragments need compaction, but does affect how they are re-written if
    selected.  If this value is too small you may end up with fragments
    that are smaller than <code>target_rows_per_fragment</code>.</p>
<div class="codehilite"><pre><span></span><code>The default will use the default from ``write_dataset``.
</code></pre></div>

<p>materialize_deletions: bool, default True
    Whether to compact fragments with soft deleted rows so they are no
    longer present in the file.
materialize_deletions_threshold: float, default 0.1
    The fraction of original rows that are soft deleted in a fragment
    before the fragment is a candidate for compaction.
num_threads: int, optional
    The number of threads to use when performing compaction. If not
    specified, defaults to the number of cores on the machine.
batch_size: int, optional
    The batch size to use when scanning input fragments.  You may want
    to reduce this if you are running out of memory during compaction.</p>
<div class="codehilite"><pre><span></span><code>The default will use the same default from ``scanner``.
</code></pre></div>

<h6 id="lance.dataset.DatasetOptimizer.compact_files--returns">Returns<a class="headerlink" href="#lance.dataset.DatasetOptimizer.compact_files--returns" title="Permanent link">&para;</a></h6>
<p>CompactionMetrics
    Metrics about the compaction process</p>
<h6 id="lance.dataset.DatasetOptimizer.compact_files--see-also">See Also<a class="headerlink" href="#lance.dataset.DatasetOptimizer.compact_files--see-also" title="Permanent link">&para;</a></h6>
<p>lance.optimize.Compaction</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.DatasetOptimizer.optimize_indices" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">optimize_indices</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#lance.dataset.DatasetOptimizer.optimize_indices" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Optimizes index performance.</p>
<p>As new data arrives it is not added to existing indexes automatically.
When searching we need to perform an indexed search of the old data plus
an expensive unindexed search on the new data.  As the amount of new
unindexed data grows this can have an impact on search latency.
This function will add the new data to existing indexes, restoring the
performance.  This function does not retrain the index, it only assigns
the new data to existing partitions.  This means an update is much quicker
than retraining the entire index but may have less accuracy (especially
if the new data exhibits new patterns, concepts, or trends)</p>
<h6 id="lance.dataset.DatasetOptimizer.optimize_indices--parameters">Parameters<a class="headerlink" href="#lance.dataset.DatasetOptimizer.optimize_indices--parameters" title="Permanent link">&para;</a></h6>
<p>num_indices_to_merge: int, default 1
    The number of indices to merge.
    If set to 0, new delta index will be created.
index_names: List[str], default None
    The names of the indices to optimize.
    If None, all indices will be optimized.
retrain: bool, default False
    Whether to retrain the whole index.
    If true, the index will be retrained based on the current data,
    <code>num_indices_to_merge</code> will be ignored,
    and all indices will be merged into one.</p>
<div class="codehilite"><pre><span></span><code>This is useful when the data distribution has changed significantly,
and we want to retrain the index to improve the search quality.
This would be faster than re-create the index from scratch.
</code></pre></div>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="lance.dataset.FieldStatistics" class="doc doc-heading">
            <code>FieldStatistics</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.dataset.FieldStatistics" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">


        <p>Statistics about a field in the dataset</p>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="lance.dataset.LanceDataset" class="doc doc-heading">
            <code>LanceDataset</code>


<a href="#lance.dataset.LanceDataset" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pyarrow.dataset.Dataset">Dataset</span></code></p>


        <p>A Lance Dataset in Lance format where the data is stored at the given uri.</p>










  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="lance.dataset.LanceDataset.data_storage_version" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">data_storage_version</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.dataset.LanceDataset.data_storage_version" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>The version of the data storage format this dataset is using</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="lance.dataset.LanceDataset.lance_schema" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">lance_schema</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.dataset.LanceDataset.lance_schema" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>The LanceSchema for this dataset</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="lance.dataset.LanceDataset.latest_version" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">latest_version</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.dataset.LanceDataset.latest_version" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Returns the latest version of the dataset.</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="lance.dataset.LanceDataset.max_field_id" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">max_field_id</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.dataset.LanceDataset.max_field_id" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>The max_field_id in manifest</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="lance.dataset.LanceDataset.partition_expression" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">partition_expression</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.dataset.LanceDataset.partition_expression" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Not implemented (just override pyarrow dataset to prevent segfault)</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="lance.dataset.LanceDataset.schema" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">schema</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.dataset.LanceDataset.schema" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>The pyarrow Schema for this dataset</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="lance.dataset.LanceDataset.stats" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">stats</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.dataset.LanceDataset.stats" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p><strong>Experimental API</strong></p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="lance.dataset.LanceDataset.tags" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">tags</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.dataset.LanceDataset.tags" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Tag management for the dataset.</p>
<p>Similar to Git, tags are a way to add metadata to a specific version of the
dataset.</p>
<p>.. warning::</p>
<div class="codehilite"><pre><span></span><code>Tagged versions are exempted from the :py:meth:`cleanup_old_versions()`
process.

To remove a version that has been tagged, you must first
:py:meth:`~Tags.delete` the associated tag.
</code></pre></div>

<h6 id="lance.dataset.LanceDataset.tags--examples">Examples<a class="headerlink" href="#lance.dataset.LanceDataset.tags--examples" title="Permanent link">&para;</a></h6>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>ds = lance.open(&quot;dataset.lance&quot;)
ds.tags.create(&quot;v2-prod-20250203&quot;, 10)

tags = ds.tags.list()
</code></pre></div>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="lance.dataset.LanceDataset.uri" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">uri</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.dataset.LanceDataset.uri" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>The location of the data</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="lance.dataset.LanceDataset.version" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">version</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.dataset.LanceDataset.version" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Returns the currently checked out version of the dataset</p>

    </div>

</div>



<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.add_columns" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">add_columns</span><span class="p">(</span><span class="n">transforms</span><span class="p">,</span> <span class="n">read_columns</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reader_schema</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.add_columns" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Add new columns with defined values.</p>
<p>There are several ways to specify the new columns. First, you can provide
SQL expressions for each new column. Second you can provide a UDF that
takes a batch of existing data and returns a new batch with the new
columns. These new columns will be appended to the dataset.</p>
<p>You can also provide a RecordBatchReader which will read the new column
values from some external source.  This is often useful when the new column
values have already been staged to files (often by some distributed process)</p>
<p>See the :func:<code>lance.add_columns_udf</code> decorator for more information on
writing UDFs.</p>
<h6 id="lance.dataset.LanceDataset.add_columns--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.add_columns--parameters" title="Permanent link">&para;</a></h6>
<p>transforms : dict or AddColumnsUDF or ReaderLike
    If this is a dictionary, then the keys are the names of the new
    columns and the values are SQL expression strings. These strings can
    reference existing columns in the dataset.
    If this is a AddColumnsUDF, then it is a UDF that takes a batch of
    existing data and returns a new batch with the new columns.
    If this is :class:<code>pyarrow.Field</code> or :class:<code>pyarrow.Schema</code>, it adds
    all NULL columns with the given schema, in a metadata-only operation.
read_columns : list of str, optional
    The names of the columns that the UDF will read. If None, then the
    UDF will read all columns. This is only used when transforms is a
    UDF. Otherwise, the read columns are inferred from the SQL expressions.
reader_schema: pa.Schema, optional
    Only valid if transforms is a <code>ReaderLike</code> object.  This will be used to
    determine the schema of the reader.
batch_size: int, optional
    The number of rows to read at a time from the source dataset when applying
    the transform.  This is ignored if the dataset is a v1 dataset.</p>
<h6 id="lance.dataset.LanceDataset.add_columns--examples">Examples<a class="headerlink" href="#lance.dataset.LanceDataset.add_columns--examples" title="Permanent link">&para;</a></h6>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
table = pa.table({"a": [1, 2, 3]})
dataset = lance.write_dataset(table, "my_dataset")
<a class="magiclink magiclink-github magiclink-mention" href="https://github.com/lance" title="GitHub User: lance">@lance</a>.batch_udf()
... def double_a(batch):
...     df = batch.to_pandas()
...     return pd.DataFrame({'double_a': 2 * df['a']})
dataset.add_columns(double_a)
dataset.to_table().to_pandas()
   a  double_a
0  1         2
1  2         4
2  3         6
dataset.add_columns({"triple_a": "a * 3"})
dataset.to_table().to_pandas()
   a  double_a  triple_a
0  1         2         3
1  2         4         6
2  3         6         9</p>
</blockquote>
</blockquote>
</blockquote>
<h6 id="lance.dataset.LanceDataset.add_columns--see-also">See Also<a class="headerlink" href="#lance.dataset.LanceDataset.add_columns--see-also" title="Permanent link">&para;</a></h6>
<p>LanceDataset.merge :
    Merge a pre-computed set of columns into the dataset.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.alter_columns" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">alter_columns</span><span class="p">(</span><span class="o">*</span><span class="n">alterations</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.alter_columns" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Alter column name, data type, and nullability.</p>
<p>Columns that are renamed can keep any indices that are on them. If a
column has an IVF_PQ index, it can be kept if the column is casted to
another type. However, other index types don't support casting at this
time.</p>
<p>Column types can be upcasted (such as int32 to int64) or downcasted
(such as int64 to int32). However, downcasting will fail if there are
any values that cannot be represented in the new type. In general,
columns can be casted to same general type: integers to integers,
floats to floats, and strings to strings. However, strings, binary, and
list columns can be casted between their size variants. For example,
string to large string, binary to large binary, and list to large list.</p>
<p>Columns that are renamed can keep any indices that are on them. However, if
the column is casted to a different type, its indices will be dropped.</p>
<h6 id="lance.dataset.LanceDataset.alter_columns--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.alter_columns--parameters" title="Permanent link">&para;</a></h6>
<p>alterations : Iterable[Dict[str, Any]]
    A sequence of dictionaries, each with the following keys:</p>
<div class="codehilite"><pre><span></span><code>- &quot;path&quot;: str
    The column path to alter. For a top-level column, this is the name.
    For a nested column, this is the dot-separated path, e.g. &quot;a.b.c&quot;.
- &quot;name&quot;: str, optional
    The new name of the column. If not specified, the column name is
    not changed.
- &quot;nullable&quot;: bool, optional
    Whether the column should be nullable. If not specified, the column
    nullability is not changed. Only non-nullable columns can be changed
    to nullable. Currently, you cannot change a nullable column to
    non-nullable.
- &quot;data_type&quot;: pyarrow.DataType, optional
    The new data type to cast the column to. If not specified, the column
    data type is not changed.
</code></pre></div>

<h6 id="lance.dataset.LanceDataset.alter_columns--examples">Examples<a class="headerlink" href="#lance.dataset.LanceDataset.alter_columns--examples" title="Permanent link">&para;</a></h6>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
schema = pa.schema([pa.field('a', pa.int64()),
...                     pa.field('b', pa.string(), nullable=False)])
table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]})
dataset = lance.write_dataset(table, "example")
dataset.alter_columns({"path": "a", "name": "x"},
...                       {"path": "b", "nullable": True})
dataset.to_table().to_pandas()
   x  b
0  1  a
1  2  b
2  3  c
dataset.alter_columns({"path": "x", "data_type": pa.int32()})
dataset.schema
x: int32
b: string</p>
</blockquote>
</blockquote>
</blockquote>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.checkout_version" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">checkout_version</span><span class="p">(</span><span class="n">version</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.checkout_version" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Load the given version of the dataset.</p>
<p>Unlike the :func:<code>dataset</code> constructor, this will re-use the
current cache.
This is a no-op if the dataset is already at the given version.</p>
<h6 id="lance.dataset.LanceDataset.checkout_version--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.checkout_version--parameters" title="Permanent link">&para;</a></h6>
<p>version: int | str,
    The version to check out. A version number (<code>int</code>) or a tag
    (<code>str</code>) can be provided.</p>
<h6 id="lance.dataset.LanceDataset.checkout_version--returns">Returns<a class="headerlink" href="#lance.dataset.LanceDataset.checkout_version--returns" title="Permanent link">&para;</a></h6>
<p>LanceDataset</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.cleanup_old_versions" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">cleanup_old_versions</span><span class="p">(</span><span class="n">older_than</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">delete_unverified</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">error_if_tagged_old_versions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.cleanup_old_versions" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Cleans up old versions of the dataset.</p>
<p>Some dataset changes, such as overwriting, leave behind data that is not
referenced by the latest dataset version.  The old data is left in place
to allow the dataset to be restored back to an older version.</p>
<p>This method will remove older versions and any data files they reference.
Once this cleanup task has run you will not be able to checkout or restore
these older versions.</p>
<h6 id="lance.dataset.LanceDataset.cleanup_old_versions--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.cleanup_old_versions--parameters" title="Permanent link">&para;</a></h6>


<details class="older_than" open>
  <summary>timedelta, optional</summary>
  <p>Only versions older than this will be removed.  If not specified, this
will default to two weeks.</p>
</details>

<details class="delete_unverified" open>
  <summary>bool, default False</summary>
  <p>Files leftover from a failed transaction may appear to be part of an
in-progress operation (e.g. appending new data) and these files will
not be deleted unless they are at least 7 days old.  If delete_unverified
is True then these files will be deleted regardless of their age.</p>
<p>This should only be set to True if you can guarantee that no other process
is currently working on this dataset.  Otherwise the dataset could be put
into a corrupted state.</p>
</details>

<details class="error_if_tagged_old_versions" open>
  <summary>bool, default True</summary>
  <p>Some versions may have tags associated with them. Tagged versions will
not be cleaned up, regardless of how old they are. If this argument
is set to <code>True</code> (the default), an exception will be raised if any
tagged versions match the parameters. Otherwise, tagged versions will
be ignored without any error and only untagged versions will be
cleaned up.</p>
</details>

    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.commit" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">commit</span><span class="p">(</span><span class="n">base_uri</span><span class="p">,</span> <span class="n">operation</span><span class="p">,</span> <span class="n">blobs_op</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">read_version</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">commit_lock</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">storage_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">enable_v2_manifest_paths</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">detached</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_retries</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#lance.dataset.LanceDataset.commit" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Create a new version of dataset</p>
<p>This method is an advanced method which allows users to describe a change
that has been made to the data files.  This method is not needed when using
Lance to apply changes (e.g. when using <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />class:<code>LanceDataset</code> or
<img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />func:<code>write_dataset</code>.)</p>
<p>It's current purpose is to allow for changes being made in a distributed
environment where no single process is doing all of the work.  For example,
a distributed bulk update or a distributed bulk modify operation.</p>
<p>Once all of the changes have been made, this method can be called to make
the changes visible by updating the dataset manifest.</p>
<h6 id="lance.dataset.LanceDataset.commit--warnings">Warnings<a class="headerlink" href="#lance.dataset.LanceDataset.commit--warnings" title="Permanent link">&para;</a></h6>
<p>This is an advanced API and doesn't provide the same level of validation
as the other APIs. For example, it's the responsibility of the caller to
ensure that the fragments are valid for the schema.</p>
<h6 id="lance.dataset.LanceDataset.commit--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.commit--parameters" title="Permanent link">&para;</a></h6>
<p>base_uri: str, Path, or LanceDataset
    The base uri of the dataset, or the dataset object itself. Using
    the dataset object can be more efficient because it can re-use the
    file metadata cache.
operation: BaseOperation
    The operation to apply to the dataset.  This describes what changes
    have been made. See available operations under :class:<code>LanceOperation</code>.
read_version: int, optional
    The version of the dataset that was used as the base for the changes.
    This is not needed for overwrite or restore operations.
commit_lock : CommitLock, optional
    A custom commit lock.  Only needed if your object store does not support
    atomic commits.  See the user guide for more details.
storage_options : optional, dict
    Extra options that make sense for a particular storage connection. This is
    used to store connection parameters like credentials, endpoint, etc.
enable_v2_manifest_paths : bool, optional
    If True, and this is a new dataset, uses the new V2 manifest paths.
    These paths provide more efficient opening of datasets with many
    versions on object stores. This parameter has no effect if the dataset
    already exists. To migrate an existing dataset, instead use the
    :meth:<code>migrate_manifest_paths_v2</code> method. Default is False. WARNING:
    turning this on will make the dataset unreadable for older versions
    of Lance (prior to 0.17.0).
detached : bool, optional
    If True, then the commit will not be part of the dataset lineage.  It will
    never show up as the latest dataset and the only way to check it out in the
    future will be to specifically check it out by version.  The version will be
    a random version that is only unique amongst detached commits.  The caller
    should store this somewhere as there will be no other way to obtain it in
    the future.
max_retries : int
    The maximum number of retries to perform when committing the dataset.</p>
<h6 id="lance.dataset.LanceDataset.commit--returns">Returns<a class="headerlink" href="#lance.dataset.LanceDataset.commit--returns" title="Permanent link">&para;</a></h6>
<p>LanceDataset
    A new version of Lance Dataset.</p>
<h6 id="lance.dataset.LanceDataset.commit--examples">Examples<a class="headerlink" href="#lance.dataset.LanceDataset.commit--examples" title="Permanent link">&para;</a></h6>
<p>Creating a new dataset with the :class:<code>LanceOperation.Overwrite</code> operation:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]})
tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]})
fragment1 = lance.fragment.LanceFragment.create("example", tab1)
fragment2 = lance.fragment.LanceFragment.create("example", tab2)
fragments = [fragment1, fragment2]
operation = lance.LanceOperation.Overwrite(tab1.schema, fragments)
dataset = lance.LanceDataset.commit("example", operation)
dataset.to_table().to_pandas()
   a  b
0  1  a
1  2  b
2  3  c
3  4  d</p>
</blockquote>
</blockquote>
</blockquote>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.commit_batch" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">commit_batch</span><span class="p">(</span><span class="n">dest</span><span class="p">,</span> <span class="n">transactions</span><span class="p">,</span> <span class="n">commit_lock</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">storage_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">enable_v2_manifest_paths</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">detached</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_retries</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#lance.dataset.LanceDataset.commit_batch" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Create a new version of dataset with multiple transactions.</p>
<p>This method is an advanced method which allows users to describe a change
that has been made to the data files.  This method is not needed when using
Lance to apply changes (e.g. when using <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />class:<code>LanceDataset</code> or
<img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />func:<code>write_dataset</code>.)</p>
<h6 id="lance.dataset.LanceDataset.commit_batch--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.commit_batch--parameters" title="Permanent link">&para;</a></h6>
<p>dest: str, Path, or LanceDataset
    The base uri of the dataset, or the dataset object itself. Using
    the dataset object can be more efficient because it can re-use the
    file metadata cache.
transactions: Iterable[Transaction]
    The transactions to apply to the dataset. These will be merged into
    a single transaction and applied to the dataset. Note: Only append
    transactions are currently supported. Other transaction types will be
    supported in the future.
commit_lock : CommitLock, optional
    A custom commit lock.  Only needed if your object store does not support
    atomic commits.  See the user guide for more details.
storage_options : optional, dict
    Extra options that make sense for a particular storage connection. This is
    used to store connection parameters like credentials, endpoint, etc.
enable_v2_manifest_paths : bool, optional
    If True, and this is a new dataset, uses the new V2 manifest paths.
    These paths provide more efficient opening of datasets with many
    versions on object stores. This parameter has no effect if the dataset
    already exists. To migrate an existing dataset, instead use the
    :meth:<code>migrate_manifest_paths_v2</code> method. Default is False. WARNING:
    turning this on will make the dataset unreadable for older versions
    of Lance (prior to 0.17.0).
detached : bool, optional
    If True, then the commit will not be part of the dataset lineage.  It will
    never show up as the latest dataset and the only way to check it out in the
    future will be to specifically check it out by version.  The version will be
    a random version that is only unique amongst detached commits.  The caller
    should store this somewhere as there will be no other way to obtain it in
    the future.
max_retries : int
    The maximum number of retries to perform when committing the dataset.</p>
<h6 id="lance.dataset.LanceDataset.commit_batch--returns">Returns<a class="headerlink" href="#lance.dataset.LanceDataset.commit_batch--returns" title="Permanent link">&para;</a></h6>
<p>dict with keys:
    dataset: LanceDataset
        A new version of Lance Dataset.
    merged: Transaction
        The merged transaction that was applied to the dataset.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.count_rows" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">count_rows</span><span class="p">(</span><span class="nb">filter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.count_rows" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Count rows matching the scanner filter.</p>
<h6 id="lance.dataset.LanceDataset.count_rows--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.count_rows--parameters" title="Permanent link">&para;</a></h6>
<p>**kwargs : dict, optional
    See py:method:<code>scanner</code> method for full parameter description.</p>
<h6 id="lance.dataset.LanceDataset.count_rows--returns">Returns<a class="headerlink" href="#lance.dataset.LanceDataset.count_rows--returns" title="Permanent link">&para;</a></h6>
<p>count : int
    The total number of rows in the dataset.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.create_index" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">create_index</span><span class="p">(</span><span class="n">column</span><span class="p">,</span> <span class="n">index_type</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;L2&#39;</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_partitions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ivf_centroids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pq_codebook</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_sub_vectors</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">index_cache_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shuffle_partition_batches</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shuffle_partition_concurrency</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ivf_centroids_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">precomputed_partition_dataset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">storage_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">filter_nan</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">one_pass_ivfpq</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.create_index" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Create index on column.</p>
<p><strong>Experimental API</strong></p>
<h6 id="lance.dataset.LanceDataset.create_index--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.create_index--parameters" title="Permanent link">&para;</a></h6>
<p>column : str
    The column to be indexed.
index_type : str
    The type of the index.
    <code>"IVF_PQ, IVF_HNSW_PQ and IVF_HNSW_SQ"</code> are supported now.
name : str, optional
    The index name. If not provided, it will be generated from the
    column name.
metric : str
    The distance metric type, i.e., "L2" (alias to "euclidean"), "cosine"
    or "dot" (dot product). Default is "L2".
replace : bool
    Replace the existing index if it exists.
num_partitions : int, optional
    The number of partitions of IVF (Inverted File Index).
ivf_centroids : optional
    It can be either <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />class:<code>np.ndarray</code>,
    <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />class:<code>pyarrow.FixedSizeListArray</code> or
    <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />class:<code>pyarrow.FixedShapeTensorArray</code>.
    A <code>num_partitions x dimension</code> array of existing K-mean centroids
    for IVF clustering. If not provided, a new KMeans model will be trained.
pq_codebook : optional,
    It can be <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />class:<code>np.ndarray</code>, <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />class:<code>pyarrow.FixedSizeListArray</code>,
    or <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />class:<code>pyarrow.FixedShapeTensorArray</code>.
    A <code>num_sub_vectors x (2 ^ nbits * dimensions // num_sub_vectors)</code>
    array of K-mean centroids for PQ codebook.</p>
<div class="codehilite"><pre><span></span><code>Note: ``nbits`` is always 8 for now.
If not provided, a new PQ model will be trained.
</code></pre></div>

<p>num_sub_vectors : int, optional
    The number of sub-vectors for PQ (Product Quantization).
accelerator : str or <code>torch.Device</code>, optional
    If set, use an accelerator to speed up the training process.
    Accepted accelerator: "cuda" (Nvidia GPU) and "mps" (Apple Silicon GPU).
    If not set, use the CPU.
index_cache_size : int, optional
    The size of the index cache in number of entries. Default value is 256.
shuffle_partition_batches : int, optional
    The number of batches, using the row group size of the dataset, to include
    in each shuffle partition. Default value is 10240.</p>
<div class="codehilite"><pre><span></span><code>Assuming the row group size is 1024, each shuffle partition will hold
10240 * 1024 = 10,485,760 rows. By making this value smaller, this shuffle
will consume less memory but will take longer to complete, and vice versa.
</code></pre></div>

<p>shuffle_partition_concurrency : int, optional
    The number of shuffle partitions to process concurrently. Default value is 2</p>
<div class="codehilite"><pre><span></span><code>By making this value smaller, this shuffle will consume less memory but will
take longer to complete, and vice versa.
</code></pre></div>

<p>storage_options : optional, dict
    Extra options that make sense for a particular storage connection. This is
    used to store connection parameters like credentials, endpoint, etc.
filter_nan: bool
    Defaults to True. False is UNSAFE, and will cause a crash if any null/nan
    values are present (and otherwise will not). Disables the null filter used
    for nullable columns. Obtains a small speed boost.
one_pass_ivfpq: bool
    Defaults to False. If enabled, index type must be "IVF_PQ". Reduces disk IO.
kwargs :
    Parameters passed to the index building process.</p>
<p>The SQ (Scalar Quantization) is available for only <code>IVF_HNSW_SQ</code> index type,
this quantization method is used to reduce the memory usage of the index,
it maps the float vectors to integer vectors, each integer is of <code>num_bits</code>,
now only 8 bits are supported.</p>
<p>If <code>index_type</code> is "IVF_*", then the following parameters are required:
    num_partitions</p>
<p>If <code>index_type</code> is with "PQ", then the following parameters are required:
    num_sub_vectors</p>
<p>Optional parameters for <code>IVF_PQ</code>:</p>
<div class="codehilite"><pre><span></span><code>- ivf_centroids
    Existing K-mean centroids for IVF clustering.
- num_bits
    The number of bits for PQ (Product Quantization). Default is 8.
    Only 4, 8 are supported.
</code></pre></div>

<p>Optional parameters for <code>IVF_HNSW_*</code>:
    max_level
        Int, the maximum number of levels in the graph.
    m
        Int, the number of edges per node in the graph.
    ef_construction
        Int, the number of nodes to examine during the construction.</p>
<h6 id="lance.dataset.LanceDataset.create_index--examples">Examples<a class="headerlink" href="#lance.dataset.LanceDataset.create_index--examples" title="Permanent link">&para;</a></h6>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>import lance

dataset = lance.dataset(&quot;/tmp/sift.lance&quot;)
dataset.create_index(
    &quot;vector&quot;,
    &quot;IVF_PQ&quot;,
    num_partitions=256,
    num_sub_vectors=16
)
</code></pre></div>

<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>import lance

dataset = lance.dataset(&quot;/tmp/sift.lance&quot;)
dataset.create_index(
    &quot;vector&quot;,
    &quot;IVF_HNSW_SQ&quot;,
    num_partitions=256,
)
</code></pre></div>

<p>Experimental Accelerator (GPU) support:</p>
<ul>
<li><em>accelerate</em>: use GPU to train IVF partitions.
    Only supports CUDA (Nvidia) or MPS (Apple) currently.
    Requires PyTorch being installed.</li>
</ul>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>import lance

dataset = lance.dataset(&quot;/tmp/sift.lance&quot;)
dataset.create_index(
    &quot;vector&quot;,
    &quot;IVF_PQ&quot;,
    num_partitions=256,
    num_sub_vectors=16,
    accelerator=&quot;cuda&quot;
)
</code></pre></div>

<h6 id="lance.dataset.LanceDataset.create_index--references">References<a class="headerlink" href="#lance.dataset.LanceDataset.create_index--references" title="Permanent link">&para;</a></h6>
<ul>
<li><code>Faiss Index &lt;https://github.com/facebookresearch/faiss/wiki/Faiss-indexes&gt;</code>_</li>
<li>IVF introduced in <code>Video Google: a text retrieval approach to object matching
  in videos &lt;https://ieeexplore.ieee.org/abstract/document/1238663&gt;</code>_</li>
<li><code>Product quantization for nearest neighbor search
  &lt;https://hal.inria.fr/inria-00514462v2/document&gt;</code>_</li>
</ul>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.create_scalar_index" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">create_scalar_index</span><span class="p">(</span><span class="n">column</span><span class="p">,</span> <span class="n">index_type</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.create_scalar_index" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Create a scalar index on a column.</p>
<p>Scalar indices, like vector indices, can be used to speed up scans.  A scalar
index can speed up scans that contain filter expressions on the indexed column.
For example, the following scan will be faster if the column <code>my_col</code> has
a scalar index:</p>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>import lance

dataset = lance.dataset(&quot;/tmp/images.lance&quot;)
my_table = dataset.scanner(filter=&quot;my_col != 7&quot;).to_table()
</code></pre></div>

<p>Vector search with pre-filers can also benefit from scalar indices. For example,</p>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>import lance

dataset = lance.dataset(&quot;/tmp/images.lance&quot;)
my_table = dataset.scanner(
    nearest=dict(
       column=&quot;vector&quot;,
       q=[1, 2, 3, 4],
       k=10,
    )
    filter=&quot;my_col != 7&quot;,
    prefilter=True
)
</code></pre></div>

<p>There are 5 types of scalar indices available today.</p>
<ul>
<li><code>BTREE</code>. The most common type is <code>BTREE</code>. This index is inspired
  by the btree data structure although only the first few layers of the btree
  are cached in memory.  It will
  perform well on columns with a large number of unique values and few rows per
  value.</li>
<li><code>BITMAP</code>. This index stores a bitmap for each unique value in the column.
  This index is useful for columns with a small number of unique values and
  many rows per value.</li>
<li><code>LABEL_LIST</code>. A special index that is used to index list
  columns whose values have small cardinality.  For example, a column that
  contains lists of tags (e.g. <code>["tag1", "tag2", "tag3"]</code>) can be indexed
  with a <code>LABEL_LIST</code> index.  This index can only speedup queries with
  <code>array_has_any</code> or <code>array_has_all</code> filters.</li>
<li><code>NGRAM</code>. A special index that is used to index string columns.  This index
  creates a bitmap for each ngram in the string.  By default we use trigrams.
  This index can currently speed up queries using the <code>contains</code> function
  in filters.</li>
<li><code>FTS/INVERTED</code>. It is used to index document columns. This index
  can conduct full-text searches. For example, a column that contains any word
  of query string "hello world". The results will be ranked by BM25.</li>
</ul>
<p>Note that the <code>LANCE_BYPASS_SPILLING</code> environment variable can be used to
bypass spilling to disk. Setting this to true can avoid memory exhaustion
issues (see <a href="https://github.com/apache/datafusion/issues/10073">https://github.com/apache/datafusion/issues/10073</a> for more info).</p>
<p><strong>Experimental API</strong></p>
<h6 id="lance.dataset.LanceDataset.create_scalar_index--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.create_scalar_index--parameters" title="Permanent link">&para;</a></h6>
<p>column : str
    The column to be indexed.  Must be a boolean, integer, float,
    or string column.
index_type : str
    The type of the index.  One of <code>"BTREE"</code>, <code>"BITMAP"</code>,
    <code>"LABEL_LIST"</code>, <code>"NGRAM"</code>, <code>"FTS"</code> or <code>"INVERTED"</code>.
name : str, optional
    The index name. If not provided, it will be generated from the
    column name.
replace : bool, default True
    Replace the existing index if it exists.</p>


<details class="with_position" open>
  <summary>bool, default True</summary>
  <p>This is for the <code>INVERTED</code> index. If True, the index will store the
positions of the words in the document, so that you can conduct phrase
query. This will significantly increase the index size.
It won't impact the performance of non-phrase queries even if it is set to
True.</p>
</details>        <p>base_tokenizer: str, default "simple"
    This is for the <code>INVERTED</code> index. The base tokenizer to use. The value
    can be:
    * "simple": splits tokens on whitespace and punctuation.
    * "whitespace": splits tokens on whitespace.
    * "raw": no tokenization.
language: str, default "English"
    This is for the <code>INVERTED</code> index. The language for stemming
    and stop words. This is only used when <code>stem</code> or <code>remove_stop_words</code> is true
max_token_length: Optional[int], default 40
    This is for the <code>INVERTED</code> index. The maximum token length.
    Any token longer than this will be removed.
lower_case: bool, default True
    This is for the <code>INVERTED</code> index. If True, the index will convert all
    text to lowercase.
stem: bool, default False
    This is for the <code>INVERTED</code> index. If True, the index will stem the
    tokens.
remove_stop_words: bool, default False
    This is for the <code>INVERTED</code> index. If True, the index will remove
    stop words.
ascii_folding: bool, default False
    This is for the <code>INVERTED</code> index. If True, the index will convert
    non-ascii characters to ascii characters if possible.
    This would remove accents like "é" -&gt; "e".</p>
<h6 id="lance.dataset.LanceDataset.create_scalar_index--examples">Examples<a class="headerlink" href="#lance.dataset.LanceDataset.create_scalar_index--examples" title="Permanent link">&para;</a></h6>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>import lance

dataset = lance.dataset(&quot;/tmp/images.lance&quot;)
dataset.create_index(
    &quot;category&quot;,
    &quot;BTREE&quot;,
)
</code></pre></div>

<p>Scalar indices can only speed up scans for basic filters using
equality, comparison, range (e.g. <code>my_col BETWEEN 0 AND 100</code>), and set
membership (e.g. <code>my_col IN (0, 1, 2)</code>)</p>
<p>Scalar indices can be used if the filter contains multiple indexed columns and
the filter criteria are AND'd or OR'd together
(e.g. <code>my_col &lt; 0 AND other_col&gt; 100</code>)</p>
<p>Scalar indices may be used if the filter contains non-indexed columns but,
depending on the structure of the filter, they may not be usable.  For example,
if the column <code>not_indexed</code> does not have a scalar index then the filter
<code>my_col = 0 OR not_indexed = 1</code> will not be able to use any scalar index on
<code>my_col</code>.</p>
<p>To determine if a scan is making use of a scalar index you can use
<code>explain_plan</code> to look at the query plan that lance has created.  Queries
that use scalar indices will either have a <code>ScalarIndexQuery</code> relation or a
<code>MaterializeIndex</code> operator.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.delete" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">delete</span><span class="p">(</span><span class="n">predicate</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.delete" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Delete rows from the dataset.</p>
<p>This marks rows as deleted, but does not physically remove them from the
files. This keeps the existing indexes still valid.</p>
<h6 id="lance.dataset.LanceDataset.delete--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.delete--parameters" title="Permanent link">&para;</a></h6>
<p>predicate : str or pa.compute.Expression
    The predicate to use to select rows to delete. May either be a SQL
    string or a pyarrow Expression.</p>
<h6 id="lance.dataset.LanceDataset.delete--examples">Examples<a class="headerlink" href="#lance.dataset.LanceDataset.delete--examples" title="Permanent link">&para;</a></h6>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]})
dataset = lance.write_dataset(table, "example")
dataset.delete("a = 1 or b in ('a', 'b')")
dataset.to_table()
pyarrow.Table
a: int64
b: string</p>
</blockquote>
</blockquote>
</blockquote>
<hr />
<p>a: [[3]]
b: [["c"]]</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.drop_columns" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">drop_columns</span><span class="p">(</span><span class="n">columns</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.drop_columns" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Drop one or more columns from the dataset</p>
<p>This is a metadata-only operation and does not remove the data from the
underlying storage. In order to remove the data, you must subsequently
call <code>compact_files</code> to rewrite the data without the removed columns and
then call <code>cleanup_old_versions</code> to remove the old files.</p>
<h6 id="lance.dataset.LanceDataset.drop_columns--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.drop_columns--parameters" title="Permanent link">&para;</a></h6>
<p>columns : list of str
    The names of the columns to drop. These can be nested column references
    (e.g. "a.b.c") or top-level column names (e.g. "a").</p>
<h6 id="lance.dataset.LanceDataset.drop_columns--examples">Examples<a class="headerlink" href="#lance.dataset.LanceDataset.drop_columns--examples" title="Permanent link">&para;</a></h6>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]})
dataset = lance.write_dataset(table, "example")
dataset.drop_columns(["a"])
dataset.to_table().to_pandas()
   b
0  a
1  b
2  c</p>
</blockquote>
</blockquote>
</blockquote>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.drop_index" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">drop_index</span><span class="p">(</span><span class="n">name</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.drop_index" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Drops an index from the dataset</p>
<p>Note: Indices are dropped by "index name".  This is not the same as the field
name. If you did not specify a name when you created the index then a name was
generated for you.  You can use the <code>list_indices</code> method to get the names of
the indices.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.get_fragment" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_fragment</span><span class="p">(</span><span class="n">fragment_id</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.get_fragment" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Get the fragment with fragment id.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.get_fragments" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_fragments</span><span class="p">(</span><span class="nb">filter</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.get_fragments" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Get all fragments from the dataset.</p>
<p>Note: filter is not supported yet.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.head" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">head</span><span class="p">(</span><span class="n">num_rows</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.head" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Load the first N rows of the dataset.</p>
<h6 id="lance.dataset.LanceDataset.head--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.head--parameters" title="Permanent link">&para;</a></h6>
<p>num_rows : int
    The number of rows to load.
**kwargs : dict, optional
    See scanner() method for full parameter description.</p>
<h6 id="lance.dataset.LanceDataset.head--returns">Returns<a class="headerlink" href="#lance.dataset.LanceDataset.head--returns" title="Permanent link">&para;</a></h6>
<p>table : Table</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.insert" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">insert</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;append&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.insert" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Insert data into the dataset.</p>
<h6 id="lance.dataset.LanceDataset.insert--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.insert--parameters" title="Permanent link">&para;</a></h6>
<p>data_obj: Reader-like
    The data to be written. Acceptable types are:
    - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader
    - Huggingface dataset
mode: str, default 'append'
    The mode to use when writing the data. Options are:
        <strong>create</strong> - create a new dataset (raises if uri already exists).
        <strong>overwrite</strong> - create a new snapshot version
        <strong>append</strong> - create a new version that is the concat of the input the
        latest version (raises if uri does not exist)
**kwargs : dict, optional
    Additional keyword arguments to pass to :func:<code>write_dataset</code>.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.join" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">join</span><span class="p">(</span><span class="n">right_dataset</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">right_keys</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">join_type</span><span class="o">=</span><span class="s1">&#39;left outer&#39;</span><span class="p">,</span> <span class="n">left_suffix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">right_suffix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">coalesce_keys</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_threads</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.join" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Not implemented (just override pyarrow dataset to prevent segfault)</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.merge" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">merge</span><span class="p">(</span><span class="n">data_obj</span><span class="p">,</span> <span class="n">left_on</span><span class="p">,</span> <span class="n">right_on</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.merge" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Merge another dataset into this one.</p>
<p>Performs a left join, where the dataset is the left side and data_obj
is the right side. Rows existing in the dataset but not on the left will
be filled with null values, unless Lance doesn't support null values for
some types, in which case an error will be raised.</p>
<h6 id="lance.dataset.LanceDataset.merge--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.merge--parameters" title="Permanent link">&para;</a></h6>
<p>data_obj: Reader-like
    The data to be merged. Acceptable types are:
    - Pandas DataFrame, Pyarrow Table, Dataset, Scanner,
    Iterator[RecordBatch], or RecordBatchReader
left_on: str
    The name of the column in the dataset to join on.
right_on: str or None
    The name of the column in data_obj to join on. If None, defaults to
    left_on.</p>
<h6 id="lance.dataset.LanceDataset.merge--examples">Examples<a class="headerlink" href="#lance.dataset.LanceDataset.merge--examples" title="Permanent link">&para;</a></h6>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
df = pa.table({'x': [1, 2, 3], 'y': ['a', 'b', 'c']})
dataset = lance.write_dataset(df, "dataset")
dataset.to_table().to_pandas()
   x  y
0  1  a
1  2  b
2  3  c
new_df = pa.table({'x': [1, 2, 3], 'z': ['d', 'e', 'f']})
dataset.merge(new_df, 'x')
dataset.to_table().to_pandas()
   x  y  z
0  1  a  d
1  2  b  e
2  3  c  f</p>
</blockquote>
</blockquote>
</blockquote>
<h6 id="lance.dataset.LanceDataset.merge--see-also">See Also<a class="headerlink" href="#lance.dataset.LanceDataset.merge--see-also" title="Permanent link">&para;</a></h6>
<p>LanceDataset.add_columns :
    Add new columns by computing batch-by-batch.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.merge_insert" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">merge_insert</span><span class="p">(</span><span class="n">on</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.merge_insert" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Returns a builder that can be used to create a "merge insert" operation</p>
<p>This operation can add rows, update rows, and remove rows in a single
transaction. It is a very generic tool that can be used to create
behaviors like "insert if not exists", "update or insert (i.e. upsert)",
or even replace a portion of existing data with new data (e.g. replace
all data where month="january")</p>
<p>The merge insert operation works by combining new data from a
<strong>source table</strong> with existing data in a <strong>target table</strong> by using a
join.  There are three categories of records.</p>
<p>"Matched" records are records that exist in both the source table and
the target table. "Not matched" records exist only in the source table
(e.g. these are new data). "Not matched by source" records exist only
in the target table (this is old data).</p>
<p>The builder returned by this method can be used to customize what
should happen for each category of data.</p>
<p>Please note that the data will be reordered as part of this
operation.  This is because updated rows will be deleted from the
dataset and then reinserted at the end with the new values.  The
order of the newly inserted rows may fluctuate randomly because a
hash-join operation is used internally.</p>
<h6 id="lance.dataset.LanceDataset.merge_insert--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.merge_insert--parameters" title="Permanent link">&para;</a></h6>


<details class="on" open>
  <summary>Union[str, Iterable[str]]</summary>
  <p>A column (or columns) to join on.  This is how records from the
source table and target table are matched.  Typically this is some
kind of key or id column.</p>
</details>        <h6 id="lance.dataset.LanceDataset.merge_insert--examples">Examples<a class="headerlink" href="#lance.dataset.LanceDataset.merge_insert--examples" title="Permanent link">&para;</a></h6>
<p>Use <code>when_matched_update_all()</code> and <code>when_not_matched_insert_all()</code> to
perform an "upsert" operation.  This will update rows that already exist
in the dataset and insert rows that do not exist.</p>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
table = pa.table({"a": [2, 1, 3], "b": ["a", "b", "c"]})
dataset = lance.write_dataset(table, "example")
new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]})</p>
<h5 id="lance.dataset.LanceDataset.merge_insert--perform-a-upsert-operation">Perform a "upsert" operation<a class="headerlink" href="#lance.dataset.LanceDataset.merge_insert--perform-a-upsert-operation" title="Permanent link">&para;</a></h5>
<p>dataset.merge_insert("a")     \
...             .when_matched_update_all()     \
...             .when_not_matched_insert_all() \
...             .execute(new_table)
{'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0}
dataset.to_table().sort_by("a").to_pandas()
   a  b
0  1  b
1  2  x
2  3  y
3  4  z</p>
</blockquote>
</blockquote>
</blockquote>
<p>Use <code>when_not_matched_insert_all()</code> to perform an "insert if not exists"
operation.  This will only insert rows that do not already exist in the
dataset.</p>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]})
dataset = lance.write_dataset(table, "example2")
new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]})</p>
<h5 id="lance.dataset.LanceDataset.merge_insert--perform-an-insert-if-not-exists-operation">Perform an "insert if not exists" operation<a class="headerlink" href="#lance.dataset.LanceDataset.merge_insert--perform-an-insert-if-not-exists-operation" title="Permanent link">&para;</a></h5>
<p>dataset.merge_insert("a")     \
...             .when_not_matched_insert_all() \
...             .execute(new_table)
{'num_inserted_rows': 1, 'num_updated_rows': 0, 'num_deleted_rows': 0}
dataset.to_table().sort_by("a").to_pandas()
   a  b
0  1  a
1  2  b
2  3  c
3  4  z</p>
</blockquote>
</blockquote>
</blockquote>
<p>You are not required to provide all the columns. If you only want to
update a subset of columns, you can omit columns you don't want to
update. Omitted columns will keep their existing values if they are
updated, or will be null if they are inserted.</p>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"], \
...                   "c": ["x", "y", "z"]})
dataset = lance.write_dataset(table, "example3")
new_table = pa.table({"a": [2, 3, 4], "b": ["x", "y", "z"]})</p>
<h5 id="lance.dataset.LanceDataset.merge_insert--perform-an-upsert-operation-only-updating-column-a">Perform an "upsert" operation, only updating column "a"<a class="headerlink" href="#lance.dataset.LanceDataset.merge_insert--perform-an-upsert-operation-only-updating-column-a" title="Permanent link">&para;</a></h5>
<p>dataset.merge_insert("a")     \
...             .when_matched_update_all()     \
...             .when_not_matched_insert_all() \
...             .execute(new_table)
{'num_inserted_rows': 1, 'num_updated_rows': 2, 'num_deleted_rows': 0}
dataset.to_table().sort_by("a").to_pandas()
   a  b     c
0  1  a     x
1  2  x     y
2  3  y     z
3  4  z  None</p>
</blockquote>
</blockquote>
</blockquote>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.migrate_manifest_paths_v2" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">migrate_manifest_paths_v2</span><span class="p">()</span></code>

<a href="#lance.dataset.LanceDataset.migrate_manifest_paths_v2" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Migrate the manifest paths to the new format.</p>
<p>This will update the manifest to use the new v2 format for paths.</p>
<p>This function is idempotent, and can be run multiple times without
changing the state of the object store.</p>
<p>DANGER: this should not be run while other concurrent operations are happening.
And it should also run until completion before resuming other operations.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.prewarm_index" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">prewarm_index</span><span class="p">(</span><span class="n">name</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.prewarm_index" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Prewarm an index</p>
<p>This will load the entire index into memory.  This can help avoid cold start
issues with index queries.  If the index does not fit in the index cache, then
this will result in wasted I/O.</p>
<h6 id="lance.dataset.LanceDataset.prewarm_index--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.prewarm_index--parameters" title="Permanent link">&para;</a></h6>
<p>name: str
    The name of the index to prewarm.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.replace_field_metadata" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">replace_field_metadata</span><span class="p">(</span><span class="n">field_name</span><span class="p">,</span> <span class="n">new_metadata</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.replace_field_metadata" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Replace the metadata of a field in the schema</p>
<h6 id="lance.dataset.LanceDataset.replace_field_metadata--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.replace_field_metadata--parameters" title="Permanent link">&para;</a></h6>
<p>field_name: str
    The name of the field to replace the metadata for
new_metadata: dict
    The new metadata to set</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.replace_schema" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">replace_schema</span><span class="p">(</span><span class="n">schema</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.replace_schema" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Not implemented (just override pyarrow dataset to prevent segfault)</p>
<p>See <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />method:<code>replace_schema_metadata</code> or <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />method:<code>replace_field_metadata</code></p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.replace_schema_metadata" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">replace_schema_metadata</span><span class="p">(</span><span class="n">new_metadata</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.replace_schema_metadata" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Replace the schema metadata of the dataset</p>
<h6 id="lance.dataset.LanceDataset.replace_schema_metadata--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.replace_schema_metadata--parameters" title="Permanent link">&para;</a></h6>
<p>new_metadata: dict
    The new metadata to set</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.restore" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">restore</span><span class="p">()</span></code>

<a href="#lance.dataset.LanceDataset.restore" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Restore the currently checked out version as the latest version of the dataset.</p>
<p>This creates a new commit.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.sample" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sample</span><span class="p">(</span><span class="n">num_rows</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">randomize_order</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.sample" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Select a random sample of data</p>
<h6 id="lance.dataset.LanceDataset.sample--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.sample--parameters" title="Permanent link">&para;</a></h6>
<p>num_rows: int
    number of rows to retrieve
columns: list of str, or dict of str to str default None
    List of column names to be fetched.
    Or a dictionary of column names to SQL expressions.
    All columns are fetched if None or unspecified.
**kwargs : dict, optional
    see scanner() method for full parameter description.</p>
<h6 id="lance.dataset.LanceDataset.sample--returns">Returns<a class="headerlink" href="#lance.dataset.LanceDataset.sample--returns" title="Permanent link">&para;</a></h6>
<p>table : Table</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.scanner" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">scanner</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">filter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nearest</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_readahead</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fragment_readahead</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scan_in_order</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fragments</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">full_text_query</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">prefilter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_row_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_row_address</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_stats</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fast_search</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">io_buffer_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">late_materialization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_scalar_index</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">include_deleted_rows</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scan_stats_callback</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">strict_batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.scanner" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Return a Scanner that can support various pushdowns.</p>
<h6 id="lance.dataset.LanceDataset.scanner--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.scanner--parameters" title="Permanent link">&para;</a></h6>
<p>columns: list of str, or dict of str to str default None
    List of column names to be fetched.
    Or a dictionary of column names to SQL expressions.
    All columns are fetched if None or unspecified.
filter: pa.compute.Expression or str
    Expression or str that is a valid SQL where clause. See
    <code>Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;</code>_
    for valid SQL expressions.
limit: int, default None
    Fetch up to this many rows. All rows if None or unspecified.
offset: int, default None
    Fetch starting with this row. 0 if None or unspecified.
nearest: dict, default None
    Get the rows corresponding to the K most similar vectors. Example:</p>
<div class="codehilite"><pre><span></span><code>.. code-block:: python

    {
        &quot;column&quot;: &lt;embedding col name&gt;,
        &quot;q&quot;: &lt;query vector as pa.Float32Array&gt;,
        &quot;k&quot;: 10,
        &quot;nprobes&quot;: 1,
        &quot;refine_factor&quot;: 1
    }
</code></pre></div>


<details class="batch_size" open>
  <summary>int, default None</summary>
  <p>The target size of batches returned.  In some cases batches can be up to
twice this size (but never larger than this).  In some cases batches can
be smaller than this size.</p>
</details>        <p>io_buffer_size: int, default None
    The size of the IO buffer.  See <code>ScannerBuilder.io_buffer_size</code>
    for more information.
batch_readahead: int, optional
    The number of batches to read ahead.
fragment_readahead: int, optional
    The number of fragments to read ahead.
scan_in_order: bool, default True
    Whether to read the fragments and batches in order. If false,
    throughput may be higher, but batches will be returned out of order
    and memory use might increase.
fragments: iterable of LanceFragment, default None
    If specified, only scan these fragments. If scan_in_order is True, then
    the fragments will be scanned in the order given.
prefilter: bool, default False
    If True then the filter will be applied before the vector query is run.
    This will generate more correct results but it may be a more costly
    query.  It's generally good when the filter is highly selective.</p>
<div class="codehilite"><pre><span></span><code>If False then the filter will be applied after the vector query is run.
This will perform well but the results may have fewer than the requested
number of rows (or be empty) if the rows closest to the query do not
match the filter.  It&#39;s generally good when the filter is not very
selective.
</code></pre></div>

<p>use_scalar_index: bool, default True
    Lance will automatically use scalar indices to optimize a query.  In some
    corner cases this can make query performance worse and this parameter can
    be used to disable scalar indices in these cases.
late_materialization: bool or List[str], default None
    Allows custom control over late materialization.  Late materialization
    fetches non-query columns using a take operation after the filter.  This
    is useful when there are few results or columns are very large.</p>
<div class="codehilite"><pre><span></span><code>Early materialization can be better when there are many results or the
columns are very narrow.

If True, then all columns are late materialized.
If False, then all columns are early materialized.
If a list of strings, then only the columns in the list are
late materialized.

The default uses a heuristic that assumes filters will select about 0.1%
of the rows.  If your filter is more selective (e.g. find by id) you may
want to set this to True.  If your filter is not very selective (e.g.
matches 20% of the rows) you may want to set this to False.
</code></pre></div>

<p>full_text_query: str or dict, optional
    query string to search for, the results will be ranked by BM25.
    e.g. "hello world", would match documents containing "hello" or "world".
    or a dictionary with the following keys:</p>
<div class="codehilite"><pre><span></span><code>- columns: list[str]
    The columns to search,
    currently only supports a single column in the columns list.
- query: str
    The query string to search for.
</code></pre></div>

<p>fast_search:  bool, default False
    If True, then the search will only be performed on the indexed data, which
    yields faster search time.
scan_stats_callback: Callable[[ScanStatistics], None], default None
    A callback function that will be called with the scan statistics after the
    scan is complete.  Errors raised by the callback will be logged but not
    re-raised.
include_deleted_rows: bool, default False
    If True, then rows that have been deleted, but are still present in the
    fragment, will be returned.  These rows will have the _rowid column set
    to null.  All other columns will reflect the value stored on disk and may
    not be null.</p>
<div class="codehilite"><pre><span></span><code>Note: if this is a search operation, or a take operation (including scalar
indexed scans) then deleted rows cannot be returned.
</code></pre></div>

<p>.. note::</p>
<div class="codehilite"><pre><span></span><code>For now, if BOTH filter and nearest is specified, then:

1. nearest is executed first.
2. The results are filtered afterwards.
</code></pre></div>

<p>For debugging ANN results, you can choose to not use the index
even if present by specifying <code>use_index=False</code>. For example,
the following will always return exact KNN results:</p>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>dataset.to_table(nearest={
    &quot;column&quot;: &quot;vector&quot;,
    &quot;k&quot;: 10,
    &quot;q&quot;: &lt;query vector&gt;,
    &quot;use_index&quot;: False
}
</code></pre></div>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.session" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">session</span><span class="p">()</span></code>

<a href="#lance.dataset.LanceDataset.session" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Return the dataset session, which holds the dataset's state.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.take" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">take</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.take" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Select rows of data by index.</p>
<h6 id="lance.dataset.LanceDataset.take--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.take--parameters" title="Permanent link">&para;</a></h6>
<p>indices : Array or array-like
    indices of rows to select in the dataset.
columns: list of str, or dict of str to str default None
    List of column names to be fetched.
    Or a dictionary of column names to SQL expressions.
    All columns are fetched if None or unspecified.</p>
<h6 id="lance.dataset.LanceDataset.take--returns">Returns<a class="headerlink" href="#lance.dataset.LanceDataset.take--returns" title="Permanent link">&para;</a></h6>
<p>table : pyarrow.Table</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.take_blobs" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">take_blobs</span><span class="p">(</span><span class="n">blob_column</span><span class="p">,</span> <span class="n">ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">addresses</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.take_blobs" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Select blobs by row IDs.</p>
<p>Instead of loading large binary blob data into memory before processing it,
this API allows you to open binary blob data as a regular Python file-like
object. For more details, see <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />class:<code>lance.BlobFile</code>.</p>
<p>Exactly one of ids, addresses, or indices must be specified.
Parameters</p>
<hr />
<p>blob_column : str
    The name of the blob column to select.
ids : Integer Array or array-like
    row IDs to select in the dataset.
addresses: Integer Array or array-like
    The (unstable) row addresses to select in the dataset.
indices : Integer Array or array-like
    The offset / indices of the row in the dataset.</p>
<h6 id="lance.dataset.LanceDataset.take_blobs--returns">Returns<a class="headerlink" href="#lance.dataset.LanceDataset.take_blobs--returns" title="Permanent link">&para;</a></h6>
<p>blob_files : List[BlobFile]</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.to_batches" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">to_batches</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">filter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nearest</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_readahead</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fragment_readahead</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scan_in_order</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">prefilter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_row_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_row_address</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_stats</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">full_text_query</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">io_buffer_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">late_materialization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_scalar_index</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">strict_batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.to_batches" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Read the dataset as materialized record batches.</p>
<h6 id="lance.dataset.LanceDataset.to_batches--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.to_batches--parameters" title="Permanent link">&para;</a></h6>
<p>**kwargs : dict, optional
    Arguments for <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />meth:<code>~LanceDataset.scanner</code>.</p>
<h6 id="lance.dataset.LanceDataset.to_batches--returns">Returns<a class="headerlink" href="#lance.dataset.LanceDataset.to_batches--returns" title="Permanent link">&para;</a></h6>
<p>record_batches : Iterator of <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />class:<code>~pyarrow.RecordBatch</code></p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.to_table" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">to_table</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">filter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nearest</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_readahead</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fragment_readahead</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scan_in_order</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">prefilter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_row_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_row_address</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_stats</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fast_search</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">full_text_query</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">io_buffer_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">late_materialization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_scalar_index</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">include_deleted_rows</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.to_table" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Read the data into memory as a <img alt="🇵🇾" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f1f5-1f1fe.svg" title=":py:" />class:<code>pyarrow.Table</code></p>
<h6 id="lance.dataset.LanceDataset.to_table--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.to_table--parameters" title="Permanent link">&para;</a></h6>
<p>columns: list of str, or dict of str to str default None
    List of column names to be fetched.
    Or a dictionary of column names to SQL expressions.
    All columns are fetched if None or unspecified.
filter : pa.compute.Expression or str
    Expression or str that is a valid SQL where clause. See
    <code>Lance filter pushdown &lt;https://lancedb.github.io/lance/introduction/read_and_write.html#filter-push-down&gt;</code>_
    for valid SQL expressions.
limit: int, default None
    Fetch up to this many rows. All rows if None or unspecified.
offset: int, default None
    Fetch starting with this row. 0 if None or unspecified.
nearest: dict, default None
    Get the rows corresponding to the K most similar vectors. Example:</p>
<div class="codehilite"><pre><span></span><code>.. code-block:: python

    {
        &quot;column&quot;: &lt;embedding col name&gt;,
        &quot;q&quot;: &lt;query vector as pa.Float32Array&gt;,
        &quot;k&quot;: 10,
        &quot;metric&quot;: &quot;cosine&quot;,
        &quot;nprobes&quot;: 1,
        &quot;refine_factor&quot;: 1
    }
</code></pre></div>


<details class="batch_size" open>
  <summary>int, optional</summary>
  <p>The number of rows to read at a time.</p>
</details>        <p>io_buffer_size: int, default None
    The size of the IO buffer.  See <code>ScannerBuilder.io_buffer_size</code>
    for more information.
batch_readahead: int, optional
    The number of batches to read ahead.
fragment_readahead: int, optional
    The number of fragments to read ahead.
scan_in_order: bool, optional, default True
    Whether to read the fragments and batches in order. If false,
    throughput may be higher, but batches will be returned out of order
    and memory use might increase.
prefilter: bool, optional, default False
    Run filter before the vector search.
late_materialization: bool or List[str], default None
    Allows custom control over late materialization.  See
    <code>ScannerBuilder.late_materialization</code> for more information.
use_scalar_index: bool, default True
    Allows custom control over scalar index usage.  See
    <code>ScannerBuilder.use_scalar_index</code> for more information.
with_row_id: bool, optional, default False
    Return row ID.
with_row_address: bool, optional, default False
    Return row address
use_stats: bool, optional, default True
    Use stats pushdown during filters.
fast_search: bool, optional, default False
full_text_query: str or dict, optional
    query string to search for, the results will be ranked by BM25.
    e.g. "hello world", would match documents contains "hello" or "world".
    or a dictionary with the following keys:</p>
<div class="codehilite"><pre><span></span><code>- columns: list[str]
    The columns to search,
    currently only supports a single column in the columns list.
- query: str
    The query string to search for.
</code></pre></div>

<p>include_deleted_rows: bool, optional, default False
    If True, then rows that have been deleted, but are still present in the
    fragment, will be returned.  These rows will have the _rowid column set
    to null.  All other columns will reflect the value stored on disk and may
    not be null.</p>
<div class="codehilite"><pre><span></span><code>Note: if this is a search operation, or a take operation (including scalar
indexed scans) then deleted rows cannot be returned.
</code></pre></div>

<h6 id="lance.dataset.LanceDataset.to_table--notes">Notes<a class="headerlink" href="#lance.dataset.LanceDataset.to_table--notes" title="Permanent link">&para;</a></h6>
<p>If BOTH filter and nearest is specified, then:</p>
<ol>
<li>nearest is executed first.</li>
<li>The results are filtered afterward, unless pre-filter sets to True.</li>
</ol>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.update" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">update</span><span class="p">(</span><span class="n">updates</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceDataset.update" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Update column values for rows matching where.</p>
<h6 id="lance.dataset.LanceDataset.update--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceDataset.update--parameters" title="Permanent link">&para;</a></h6>
<p>updates : dict of str to str
    A mapping of column names to a SQL expression.
where : str, optional
    A SQL predicate indicating which rows should be updated.</p>
<h6 id="lance.dataset.LanceDataset.update--returns">Returns<a class="headerlink" href="#lance.dataset.LanceDataset.update--returns" title="Permanent link">&para;</a></h6>
<p>updates : dict
    A dictionary containing the number of rows updated.</p>
<h6 id="lance.dataset.LanceDataset.update--examples">Examples<a class="headerlink" href="#lance.dataset.LanceDataset.update--examples" title="Permanent link">&para;</a></h6>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
table = pa.table({"a": [1, 2, 3], "b": ["a", "b", "c"]})
dataset = lance.write_dataset(table, "example")
update_stats = dataset.update(dict(a = 'a + 2'), where="b != 'a'")
update_stats["num_updated_rows"] = 2
dataset.to_table().to_pandas()
   a  b
0  1  a
1  4  b
2  5  c</p>
</blockquote>
</blockquote>
</blockquote>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.validate" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">validate</span><span class="p">()</span></code>

<a href="#lance.dataset.LanceDataset.validate" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Validate the dataset.</p>
<p>This checks the integrity of the dataset and will raise an exception if
the dataset is corrupted.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceDataset.versions" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">versions</span><span class="p">()</span></code>

<a href="#lance.dataset.LanceDataset.versions" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Return all versions in this dataset.</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="lance.dataset.LanceOperation" class="doc doc-heading">
            <code>LanceOperation</code>


<a href="#lance.dataset.LanceOperation" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">











  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="lance.dataset.LanceOperation.Append" class="doc doc-heading">
            <code>Append</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.dataset.LanceOperation.Append" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="BaseOperation (lance.dataset.LanceOperation.BaseOperation)" href="#lance.dataset.LanceOperation.BaseOperation">BaseOperation</a></code></p>


        <p>Append new rows to the dataset.</p>
<h6 id="lance.dataset.LanceOperation.Append--attributes">Attributes<a class="headerlink" href="#lance.dataset.LanceOperation.Append--attributes" title="Permanent link">&para;</a></h6>
<p>fragments: list[FragmentMetadata]
    The fragments that contain the new rows.</p>
<h6 id="lance.dataset.LanceOperation.Append--warning">Warning<a class="headerlink" href="#lance.dataset.LanceOperation.Append--warning" title="Permanent link">&para;</a></h6>
<p>This is an advanced API for distributed operations. To append to a
dataset on a single machine, use :func:<code>lance.write_dataset</code>.</p>
<h6 id="lance.dataset.LanceOperation.Append--examples">Examples<a class="headerlink" href="#lance.dataset.LanceOperation.Append--examples" title="Permanent link">&para;</a></h6>
<p>To append new rows to a dataset, first use
:meth:<code>lance.fragment.LanceFragment.create</code> to create fragments. Then
collect the fragment metadata into a list and pass it to this class.
Finally, pass the operation to the :meth:<code>LanceDataset.commit</code>
method to create the new dataset.</p>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]})
dataset = lance.write_dataset(tab1, "example")
tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]})
fragment = lance.fragment.LanceFragment.create("example", tab2)
operation = lance.LanceOperation.Append([fragment])
dataset = lance.LanceDataset.commit("example", operation,
...                                     read_version=dataset.version)
dataset.to_table().to_pandas()
   a  b
0  1  a
1  2  b
2  3  c
3  4  d</p>
</blockquote>
</blockquote>
</blockquote>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.dataset.LanceOperation.BaseOperation" class="doc doc-heading">
            <code>BaseOperation</code>


<a href="#lance.dataset.LanceOperation.BaseOperation" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="abc.ABC">ABC</span></code></p>


        <p>Base class for operations that can be applied to a dataset.</p>
<p>See available operations under :class:<code>LanceOperation</code>.</p>








    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.dataset.LanceOperation.CreateIndex" class="doc doc-heading">
            <code>CreateIndex</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.dataset.LanceOperation.CreateIndex" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="BaseOperation (lance.dataset.LanceOperation.BaseOperation)" href="#lance.dataset.LanceOperation.BaseOperation">BaseOperation</a></code></p>


        <p>Operation that creates an index on the dataset.</p>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.dataset.LanceOperation.DataReplacement" class="doc doc-heading">
            <code>DataReplacement</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.dataset.LanceOperation.DataReplacement" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="BaseOperation (lance.dataset.LanceOperation.BaseOperation)" href="#lance.dataset.LanceOperation.BaseOperation">BaseOperation</a></code></p>


        <p>Operation that replaces existing datafiles in the dataset.</p>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.dataset.LanceOperation.DataReplacementGroup" class="doc doc-heading">
            <code>DataReplacementGroup</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.dataset.LanceOperation.DataReplacementGroup" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">


        <p>Group of data replacements</p>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.dataset.LanceOperation.Delete" class="doc doc-heading">
            <code>Delete</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.dataset.LanceOperation.Delete" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="BaseOperation (lance.dataset.LanceOperation.BaseOperation)" href="#lance.dataset.LanceOperation.BaseOperation">BaseOperation</a></code></p>


        <p>Remove fragments or rows from the dataset.</p>
<h6 id="lance.dataset.LanceOperation.Delete--attributes">Attributes<a class="headerlink" href="#lance.dataset.LanceOperation.Delete--attributes" title="Permanent link">&para;</a></h6>
<p>updated_fragments: list[FragmentMetadata]
    The fragments that have been updated with new deletion vectors.
deleted_fragment_ids: list[int]
    The ids of the fragments that have been deleted entirely. These are
    the fragments where :meth:<code>LanceFragment.delete()</code> returned None.
predicate: str
    The original SQL predicate used to select the rows to delete.</p>
<h6 id="lance.dataset.LanceOperation.Delete--warning">Warning<a class="headerlink" href="#lance.dataset.LanceOperation.Delete--warning" title="Permanent link">&para;</a></h6>
<p>This is an advanced API for distributed operations. To delete rows from
dataset on a single machine, use :meth:<code>lance.LanceDataset.delete</code>.</p>
<h6 id="lance.dataset.LanceOperation.Delete--examples">Examples<a class="headerlink" href="#lance.dataset.LanceOperation.Delete--examples" title="Permanent link">&para;</a></h6>
<p>To delete rows from a dataset, call :meth:<code>lance.fragment.LanceFragment.delete</code>
on each of the fragments. If that returns a new fragment, add that to
the <code>updated_fragments</code> list. If it returns None, that means the whole
fragment was deleted, so add the fragment id to the <code>deleted_fragment_ids</code>.
Finally, pass the operation to the :meth:<code>LanceDataset.commit</code> method to
complete the deletion operation.</p>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
table = pa.table({"a": [1, 2], "b": ["a", "b"]})
dataset = lance.write_dataset(table, "example")
table = pa.table({"a": [3, 4], "b": ["c", "d"]})
dataset = lance.write_dataset(table, "example", mode="append")
dataset.to_table().to_pandas()
   a  b
0  1  a
1  2  b
2  3  c
3  4  d
predicate = "a &gt;= 2"
updated_fragments = []
deleted_fragment_ids = []
for fragment in dataset.get_fragments():
...     new_fragment = fragment.delete(predicate)
...     if new_fragment is not None:
...         updated_fragments.append(new_fragment)
...     else:
...         deleted_fragment_ids.append(fragment.fragment_id)
operation = lance.LanceOperation.Delete(updated_fragments,
...                                         deleted_fragment_ids,
...                                         predicate)
dataset = lance.LanceDataset.commit("example", operation,
...                                     read_version=dataset.version)
dataset.to_table().to_pandas()
   a  b
0  1  a</p>
</blockquote>
</blockquote>
</blockquote>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.dataset.LanceOperation.Merge" class="doc doc-heading">
            <code>Merge</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.dataset.LanceOperation.Merge" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="BaseOperation (lance.dataset.LanceOperation.BaseOperation)" href="#lance.dataset.LanceOperation.BaseOperation">BaseOperation</a></code></p>


        <p>Operation that adds columns. Unlike Overwrite, this should not change
the structure of the fragments, allowing existing indices to be kept.</p>
<h6 id="lance.dataset.LanceOperation.Merge--attributes">Attributes<a class="headerlink" href="#lance.dataset.LanceOperation.Merge--attributes" title="Permanent link">&para;</a></h6>
<p>fragments: iterable of FragmentMetadata
    The fragments that make up the new dataset.
schema: LanceSchema or pyarrow.Schema
    The schema of the new dataset. Passing a LanceSchema is preferred,
    and passing a pyarrow.Schema is deprecated.</p>
<h6 id="lance.dataset.LanceOperation.Merge--warning">Warning<a class="headerlink" href="#lance.dataset.LanceOperation.Merge--warning" title="Permanent link">&para;</a></h6>
<p>This is an advanced API for distributed operations. To overwrite or
create new dataset on a single machine, use :func:<code>lance.write_dataset</code>.</p>
<h6 id="lance.dataset.LanceOperation.Merge--examples">Examples<a class="headerlink" href="#lance.dataset.LanceOperation.Merge--examples" title="Permanent link">&para;</a></h6>
<p>To add new columns to a dataset, first define a method that will create
the new columns based on the existing columns. Then use
:meth:<code>lance.fragment.LanceFragment.add_columns</code></p>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
import pyarrow.compute as pc
table = pa.table({"a": [1, 2, 3, 4], "b": ["a", "b", "c", "d"]})
dataset = lance.write_dataset(table, "example")
dataset.to_table().to_pandas()
   a  b
0  1  a
1  2  b
2  3  c
3  4  d
def double_a(batch: pa.RecordBatch) -&gt; pa.RecordBatch:
...     doubled = pc.multiply(batch["a"], 2)
...     return pa.record_batch([doubled], ["a_doubled"])
fragments = []
for fragment in dataset.get_fragments():
...     new_fragment, new_schema = fragment.merge_columns(double_a,
...                                                       columns=['a'])
...     fragments.append(new_fragment)
operation = lance.LanceOperation.Merge(fragments, new_schema)
dataset = lance.LanceDataset.commit("example", operation,
...                                     read_version=dataset.version)
dataset.to_table().to_pandas()
   a  b  a_doubled
0  1  a          2
1  2  b          4
2  3  c          6
3  4  d          8</p>
</blockquote>
</blockquote>
</blockquote>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.dataset.LanceOperation.Overwrite" class="doc doc-heading">
            <code>Overwrite</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.dataset.LanceOperation.Overwrite" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="BaseOperation (lance.dataset.LanceOperation.BaseOperation)" href="#lance.dataset.LanceOperation.BaseOperation">BaseOperation</a></code></p>


        <p>Overwrite or create a new dataset.</p>
<h6 id="lance.dataset.LanceOperation.Overwrite--attributes">Attributes<a class="headerlink" href="#lance.dataset.LanceOperation.Overwrite--attributes" title="Permanent link">&para;</a></h6>
<p>new_schema: pyarrow.Schema
    The schema of the new dataset.
fragments: list[FragmentMetadata]
    The fragments that make up the new dataset.</p>
<h6 id="lance.dataset.LanceOperation.Overwrite--warning">Warning<a class="headerlink" href="#lance.dataset.LanceOperation.Overwrite--warning" title="Permanent link">&para;</a></h6>
<p>This is an advanced API for distributed operations. To overwrite or
create new dataset on a single machine, use :func:<code>lance.write_dataset</code>.</p>
<h6 id="lance.dataset.LanceOperation.Overwrite--examples">Examples<a class="headerlink" href="#lance.dataset.LanceOperation.Overwrite--examples" title="Permanent link">&para;</a></h6>
<p>To create or overwrite a dataset, first use
:meth:<code>lance.fragment.LanceFragment.create</code> to create fragments. Then
collect the fragment metadata into a list and pass it along with the
schema to this class. Finally, pass the operation to the
:meth:<code>LanceDataset.commit</code> method to create the new dataset.</p>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
tab1 = pa.table({"a": [1, 2], "b": ["a", "b"]})
tab2 = pa.table({"a": [3, 4], "b": ["c", "d"]})
fragment1 = lance.fragment.LanceFragment.create("example", tab1)
fragment2 = lance.fragment.LanceFragment.create("example", tab2)
fragments = [fragment1, fragment2]
operation = lance.LanceOperation.Overwrite(tab1.schema, fragments)
dataset = lance.LanceDataset.commit("example", operation)
dataset.to_table().to_pandas()
   a  b
0  1  a
1  2  b
2  3  c
3  4  d</p>
</blockquote>
</blockquote>
</blockquote>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.dataset.LanceOperation.Project" class="doc doc-heading">
            <code>Project</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.dataset.LanceOperation.Project" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="BaseOperation (lance.dataset.LanceOperation.BaseOperation)" href="#lance.dataset.LanceOperation.BaseOperation">BaseOperation</a></code></p>


        <p>Operation that project columns.
Use this operator for drop column or rename/swap column.</p>
<h6 id="lance.dataset.LanceOperation.Project--attributes">Attributes<a class="headerlink" href="#lance.dataset.LanceOperation.Project--attributes" title="Permanent link">&para;</a></h6>
<p>schema: LanceSchema
    The lance schema of the new dataset.</p>
<h6 id="lance.dataset.LanceOperation.Project--examples">Examples<a class="headerlink" href="#lance.dataset.LanceOperation.Project--examples" title="Permanent link">&para;</a></h6>
<p>Use the projece operator to swap column:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import lance
import pyarrow as pa
import pyarrow.compute as pc
from lance.schema import LanceSchema
table = pa.table({"a": [1, 2], "b": ["a", "b"], "b1": ["c", "d"]})
dataset = lance.write_dataset(table, "example")
dataset.to_table().to_pandas()
   a  b b1
0  1  a  c
1  2  b  d</p>
<h6 id="lance.dataset.LanceOperation.Project--rename-column-b-into-b0-and-rename-b1-into-b">rename column <code>b</code> into <code>b0</code> and rename b1 into <code>b</code><a class="headerlink" href="#lance.dataset.LanceOperation.Project--rename-column-b-into-b0-and-rename-b1-into-b" title="Permanent link">&para;</a></h6>
<p>table = pa.table({"a": [3, 4], "b0": ["a", "b"], "b": ["c", "d"]})
lance_schema = LanceSchema.from_pyarrow(table.schema)
operation = lance.LanceOperation.Project(lance_schema)
dataset = lance.LanceDataset.commit("example", operation, read_version=1)
dataset.to_table().to_pandas()
   a b0  b
0  1  a  c
1  2  b  d</p>
</blockquote>
</blockquote>
</blockquote>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.dataset.LanceOperation.Restore" class="doc doc-heading">
            <code>Restore</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.dataset.LanceOperation.Restore" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="BaseOperation (lance.dataset.LanceOperation.BaseOperation)" href="#lance.dataset.LanceOperation.BaseOperation">BaseOperation</a></code></p>


        <p>Operation that restores a previous version of the dataset.</p>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.dataset.LanceOperation.Rewrite" class="doc doc-heading">
            <code>Rewrite</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.dataset.LanceOperation.Rewrite" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="BaseOperation (lance.dataset.LanceOperation.BaseOperation)" href="#lance.dataset.LanceOperation.BaseOperation">BaseOperation</a></code></p>


        <p>Operation that rewrites one or more files and indices into one
or more files and indices.</p>
<h6 id="lance.dataset.LanceOperation.Rewrite--attributes">Attributes<a class="headerlink" href="#lance.dataset.LanceOperation.Rewrite--attributes" title="Permanent link">&para;</a></h6>
<p>groups: list[RewriteGroup]
    Groups of files that have been rewritten.
rewritten_indices: list[RewrittenIndex]
    Indices that have been rewritten.</p>
<h6 id="lance.dataset.LanceOperation.Rewrite--warning">Warning<a class="headerlink" href="#lance.dataset.LanceOperation.Rewrite--warning" title="Permanent link">&para;</a></h6>
<p>This is an advanced API not intended for general use.</p>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.dataset.LanceOperation.RewriteGroup" class="doc doc-heading">
            <code>RewriteGroup</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.dataset.LanceOperation.RewriteGroup" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">


        <p>Collection of rewritten files</p>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.dataset.LanceOperation.RewrittenIndex" class="doc doc-heading">
            <code>RewrittenIndex</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.dataset.LanceOperation.RewrittenIndex" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">


        <p>An index that has been rewritten</p>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="lance.dataset.LanceOperation.Update" class="doc doc-heading">
            <code>Update</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#lance.dataset.LanceOperation.Update" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="BaseOperation (lance.dataset.LanceOperation.BaseOperation)" href="#lance.dataset.LanceOperation.BaseOperation">BaseOperation</a></code></p>


        <p>Operation that updates rows in the dataset.</p>
<h6 id="lance.dataset.LanceOperation.Update--attributes">Attributes<a class="headerlink" href="#lance.dataset.LanceOperation.Update--attributes" title="Permanent link">&para;</a></h6>
<p>removed_fragment_ids: list[int]
    The ids of the fragments that have been removed entirely.
updated_fragments: list[FragmentMetadata]
    The fragments that have been updated with new deletion vectors.
new_fragments: list[FragmentMetadata]
    The fragments that contain the new rows.</p>










  <div class="doc doc-children">











  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="lance.dataset.LanceScanner" class="doc doc-heading">
            <code>LanceScanner</code>


<a href="#lance.dataset.LanceScanner" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pyarrow.dataset.Scanner">Scanner</span></code></p>











  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="lance.dataset.LanceScanner.dataset_schema" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">dataset_schema</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#lance.dataset.LanceScanner.dataset_schema" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>The schema with which batches will be read from fragments.</p>

    </div>

</div>



<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceScanner.analyze_plan" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">analyze_plan</span><span class="p">()</span></code>

<a href="#lance.dataset.LanceScanner.analyze_plan" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Execute the plan for this scanner and display with runtime metrics.</p>
<h6 id="lance.dataset.LanceScanner.analyze_plan--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceScanner.analyze_plan--parameters" title="Permanent link">&para;</a></h6>
<p>verbose : bool, default False
    Use a verbose output format.</p>
<h6 id="lance.dataset.LanceScanner.analyze_plan--returns">Returns<a class="headerlink" href="#lance.dataset.LanceScanner.analyze_plan--returns" title="Permanent link">&para;</a></h6>
<p>plan : str</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceScanner.count_rows" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">count_rows</span><span class="p">()</span></code>

<a href="#lance.dataset.LanceScanner.count_rows" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Count rows matching the scanner filter.</p>
<h6 id="lance.dataset.LanceScanner.count_rows--returns">Returns<a class="headerlink" href="#lance.dataset.LanceScanner.count_rows--returns" title="Permanent link">&para;</a></h6>
<p>count : int</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceScanner.explain_plan" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">explain_plan</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceScanner.explain_plan" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Return the execution plan for this scanner.</p>
<h6 id="lance.dataset.LanceScanner.explain_plan--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceScanner.explain_plan--parameters" title="Permanent link">&para;</a></h6>
<p>verbose : bool, default False
    Use a verbose output format.</p>
<h6 id="lance.dataset.LanceScanner.explain_plan--returns">Returns<a class="headerlink" href="#lance.dataset.LanceScanner.explain_plan--returns" title="Permanent link">&para;</a></h6>
<p>plan : str</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceScanner.from_batches" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">from_batches</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#lance.dataset.LanceScanner.from_batches" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Not implemented</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceScanner.from_dataset" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">from_dataset</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#lance.dataset.LanceScanner.from_dataset" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Not implemented</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceScanner.from_fragment" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">from_fragment</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#lance.dataset.LanceScanner.from_fragment" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Not implemented</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceScanner.head" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">head</span><span class="p">(</span><span class="n">num_rows</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceScanner.head" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Load the first N rows of the dataset.</p>
<h6 id="lance.dataset.LanceScanner.head--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceScanner.head--parameters" title="Permanent link">&para;</a></h6>
<p>num_rows : int
    The number of rows to load.</p>
<h6 id="lance.dataset.LanceScanner.head--returns">Returns<a class="headerlink" href="#lance.dataset.LanceScanner.head--returns" title="Permanent link">&para;</a></h6>
<p>Table</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceScanner.scan_batches" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">scan_batches</span><span class="p">()</span></code>

<a href="#lance.dataset.LanceScanner.scan_batches" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Consume a Scanner in record batches with corresponding fragments.</p>
<h6 id="lance.dataset.LanceScanner.scan_batches--returns">Returns<a class="headerlink" href="#lance.dataset.LanceScanner.scan_batches--returns" title="Permanent link">&para;</a></h6>
<p>record_batches : iterator of TaggedRecordBatch</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceScanner.take" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">take</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceScanner.take" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Not implemented</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceScanner.to_table" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">to_table</span><span class="p">()</span></code>

<a href="#lance.dataset.LanceScanner.to_table" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Read the data into memory and return a pyarrow Table.</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="lance.dataset.LanceStats" class="doc doc-heading">
            <code>LanceStats</code>


<a href="#lance.dataset.LanceStats" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">


        <p>Statistics about a LanceDataset.</p>










  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceStats.data_stats" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">data_stats</span><span class="p">()</span></code>

<a href="#lance.dataset.LanceStats.data_stats" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Statistics about the data in the dataset.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceStats.dataset_stats" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">dataset_stats</span><span class="p">(</span><span class="n">max_rows_per_group</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceStats.dataset_stats" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Statistics about the dataset.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.LanceStats.index_stats" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">index_stats</span><span class="p">(</span><span class="n">index_name</span><span class="p">)</span></code>

<a href="#lance.dataset.LanceStats.index_stats" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Statistics about an index.</p>
<h6 id="lance.dataset.LanceStats.index_stats--parameters">Parameters<a class="headerlink" href="#lance.dataset.LanceStats.index_stats--parameters" title="Permanent link">&para;</a></h6>
<p>index_name: str
    The name of the index to get statistics for.</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="lance.dataset.MergeInsertBuilder" class="doc doc-heading">
            <code>MergeInsertBuilder</code>


<a href="#lance.dataset.MergeInsertBuilder" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="lance.lance._MergeInsertBuilder">_MergeInsertBuilder</span></code></p>











  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="lance.dataset.MergeInsertBuilder.conflict_retries" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">conflict_retries</span><span class="p">(</span><span class="n">max_retries</span><span class="p">)</span></code>

<a href="#lance.dataset.MergeInsertBuilder.conflict_retries" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Set number of times to retry the operation if there is contention.</p>
<p>If this is set &gt; 0, then the operation will keep a copy of the input data
either in memory or on disk (depending on the size of the data) and will
retry the operation if there is contention.</p>
<p>Default is 10.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.MergeInsertBuilder.execute" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">execute</span><span class="p">(</span><span class="n">data_obj</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.dataset.MergeInsertBuilder.execute" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Executes the merge insert operation</p>
<p>This function updates the original dataset and returns a dictionary with
information about merge statistics - i.e. the number of inserted, updated,
and deleted rows.</p>
<h6 id="lance.dataset.MergeInsertBuilder.execute--parameters">Parameters<a class="headerlink" href="#lance.dataset.MergeInsertBuilder.execute--parameters" title="Permanent link">&para;</a></h6>


<details class="data_obj" open>
  <summary>ReaderLike</summary>
  <p>The new data to use as the source table for the operation.  This parameter
can be any source of data (e.g. table / dataset) that
:func:<code>~lance.write_dataset</code> accepts.</p>
</details>        <p>schema: Optional[pa.Schema]
    The schema of the data.  This only needs to be supplied whenever the data
    source is some kind of generator.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.MergeInsertBuilder.execute_uncommitted" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">execute_uncommitted</span><span class="p">(</span><span class="n">data_obj</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.dataset.MergeInsertBuilder.execute_uncommitted" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Executes the merge insert operation without committing</p>
<p>This function updates the original dataset and returns a dictionary with
information about merge statistics - i.e. the number of inserted, updated,
and deleted rows.</p>
<h6 id="lance.dataset.MergeInsertBuilder.execute_uncommitted--parameters">Parameters<a class="headerlink" href="#lance.dataset.MergeInsertBuilder.execute_uncommitted--parameters" title="Permanent link">&para;</a></h6>


<details class="data_obj" open>
  <summary>ReaderLike</summary>
  <p>The new data to use as the source table for the operation.  This parameter
can be any source of data (e.g. table / dataset) that
:func:<code>~lance.write_dataset</code> accepts.</p>
</details>        <p>schema: Optional[pa.Schema]
    The schema of the data.  This only needs to be supplied whenever the data
    source is some kind of generator.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.MergeInsertBuilder.retry_timeout" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">retry_timeout</span><span class="p">(</span><span class="n">timeout</span><span class="p">)</span></code>

<a href="#lance.dataset.MergeInsertBuilder.retry_timeout" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Set the timeout used to limit retries.</p>
<p>This is the maximum time to spend on the operation before giving up. At
least one attempt will be made, regardless of how long it takes to complete.
Subsequent attempts will be cancelled once this timeout is reached. If
the timeout has been reached during the first attempt, the operation
will be cancelled immediately before making a second attempt.</p>
<p>The default is 30 seconds.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.MergeInsertBuilder.when_matched_update_all" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">when_matched_update_all</span><span class="p">(</span><span class="n">condition</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.dataset.MergeInsertBuilder.when_matched_update_all" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Configure the operation to update matched rows</p>
<p>After this method is called, when the merge insert operation executes,
any rows that match both the source table and the target table will be
updated.  The rows from the target table will be removed and the rows
from the source table will be added.</p>
<p>An optional condition may be specified.  This should be an SQL filter
and, if present, then only matched rows that also satisfy this filter will
be updated.  The SQL filter should use the prefix <code>target.</code> to refer to
columns in the target table and the prefix <code>source.</code> to refer to columns
in the source table.  For example, <code>source.last_update &lt; target.last_update</code>.</p>
<p>If a condition is specified and rows do not satisfy the condition then these
rows will not be updated.  Failure to satisfy the filter does not cause
a "matched" row to become a "not matched" row.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.MergeInsertBuilder.when_not_matched_by_source_delete" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">when_not_matched_by_source_delete</span><span class="p">(</span><span class="n">expr</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.dataset.MergeInsertBuilder.when_not_matched_by_source_delete" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Configure the operation to delete source rows that do not match</p>
<p>After this method is called, when the merge insert operation executes,
any rows that exist only in the target table will be deleted.  An
optional filter can be specified to limit the scope of the delete
operation.  If given (as an SQL filter) then only rows which match
the filter will be deleted.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.MergeInsertBuilder.when_not_matched_insert_all" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">when_not_matched_insert_all</span><span class="p">()</span></code>

<a href="#lance.dataset.MergeInsertBuilder.when_not_matched_insert_all" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Configure the operation to insert not matched rows</p>
<p>After this method is called, when the merge insert operation executes,
any rows that exist only in the source table will be inserted into
the target table.</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="lance.dataset.ScannerBuilder" class="doc doc-heading">
            <code>ScannerBuilder</code>


<a href="#lance.dataset.ScannerBuilder" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">











  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="lance.dataset.ScannerBuilder.batch_readahead" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">batch_readahead</span><span class="p">(</span><span class="n">nbatches</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.dataset.ScannerBuilder.batch_readahead" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This parameter is ignored when reading v2 files</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.ScannerBuilder.batch_size" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">batch_size</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span></code>

<a href="#lance.dataset.ScannerBuilder.batch_size" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Set batch size for Scanner</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.ScannerBuilder.fast_search" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">fast_search</span><span class="p">(</span><span class="n">flag</span><span class="p">)</span></code>

<a href="#lance.dataset.ScannerBuilder.fast_search" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Enable fast search, which only perform search on the indexed data.</p>
<p>Users can use <code>Table::optimize()</code> or <code>create_index()</code> to include the new data
into index, thus make new data searchable.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.ScannerBuilder.full_text_search" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">full_text_search</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#lance.dataset.ScannerBuilder.full_text_search" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Filter rows by full text searching. <em>Experimental API</em>,
may remove it after we support to do this within <code>filter</code> SQL-like expression</p>
<p>Must create inverted index on the given column before searching,</p>
<h6 id="lance.dataset.ScannerBuilder.full_text_search--parameters">Parameters<a class="headerlink" href="#lance.dataset.ScannerBuilder.full_text_search--parameters" title="Permanent link">&para;</a></h6>
<p>query : str | Query
    If str, the query string to search for, a match query would be performed.
    If Query, the query object to search for,
    and the <code>columns</code> parameter will be ignored.
columns : list of str, optional
    The columns to search in. If None, search in all indexed columns.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.ScannerBuilder.include_deleted_rows" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">include_deleted_rows</span><span class="p">(</span><span class="n">flag</span><span class="p">)</span></code>

<a href="#lance.dataset.ScannerBuilder.include_deleted_rows" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Include deleted rows</p>
<p>Rows which have been deleted, but are still present in the fragment, will be
returned.  These rows will have all columns (except _rowaddr) set to null</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.ScannerBuilder.io_buffer_size" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">io_buffer_size</span><span class="p">(</span><span class="n">io_buffer_size</span><span class="p">)</span></code>

<a href="#lance.dataset.ScannerBuilder.io_buffer_size" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Set the I/O buffer size for the Scanner</p>
<p>This is the amount of RAM that will be reserved for holding I/O received from
storage before it is processed.  This is used to control the amount of memory
used by the scanner.  If the buffer is full then the scanner will block until
the buffer is processed.</p>
<p>Generally this should scale with the number of concurrent I/O threads.  The
default is 2GiB which comfortably provides enough space for somewhere between
32 and 256 concurrent I/O threads.</p>
<p>This value is not a hard cap on the amount of RAM the scanner will use.  Some
space is used for the compute (which can be controlled by the batch size) and
Lance does not keep track of memory after it is returned to the user.</p>
<p>Currently, if there is a single batch of data which is larger than the io buffer
size then the scanner will deadlock.  This is a known issue and will be fixed in
a future release.</p>
<p>This parameter is only used when reading v2 files</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.ScannerBuilder.scan_in_order" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">scan_in_order</span><span class="p">(</span><span class="n">scan_in_order</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#lance.dataset.ScannerBuilder.scan_in_order" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Whether to scan the dataset in order of fragments and batches.</p>
<p>If set to False, the scanner may read fragments concurrently and yield
batches out of order. This may improve performance since it allows more
concurrency in the scan, but can also use more memory.</p>
<p>This parameter is ignored when using v2 files.  In the v2 file format
there is no penalty to scanning in order and so all scans will scan in
order.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.ScannerBuilder.scan_stats_callback" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">scan_stats_callback</span><span class="p">(</span><span class="n">callback</span><span class="p">)</span></code>

<a href="#lance.dataset.ScannerBuilder.scan_stats_callback" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Set a callback function that will be called with the scan statistics after the
scan is complete.  Errors raised by the callback will be logged but not
re-raised.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.ScannerBuilder.strict_batch_size" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">strict_batch_size</span><span class="p">(</span><span class="n">strict_batch_size</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#lance.dataset.ScannerBuilder.strict_batch_size" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>If True, then all batches except the last batch will have exactly
<code>batch_size</code> rows.
By default, it is false.
If this is true then small batches will need to be merged together
which will require a data copy and incur a (typically very small)
performance penalty.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.ScannerBuilder.use_scalar_index" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">use_scalar_index</span><span class="p">(</span><span class="n">use_scalar_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#lance.dataset.ScannerBuilder.use_scalar_index" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Set whether scalar indices should be used in a query</p>
<p>Scans will use scalar indices, when available, to optimize queries with filters.
However, in some corner cases, scalar indices may make performance worse.  This
parameter allows users to disable scalar indices in these cases.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.ScannerBuilder.use_stats" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">use_stats</span><span class="p">(</span><span class="n">use_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#lance.dataset.ScannerBuilder.use_stats" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Enable use of statistics for query planning.</p>
<p>Disabling statistics is used for debugging and benchmarking purposes.
This should be left on for normal use.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.ScannerBuilder.with_row_address" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">with_row_address</span><span class="p">(</span><span class="n">with_row_address</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#lance.dataset.ScannerBuilder.with_row_address" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Enables returns with row addresses.</p>
<p>Row addresses are a unique but unstable identifier for each row in the
dataset that consists of the fragment id (upper 32 bits) and the row
offset in the fragment (lower 32 bits).  Row IDs are generally preferred
since they do not change when a row is modified or compacted.  However,
row addresses may be useful in some advanced use cases.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.ScannerBuilder.with_row_id" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">with_row_id</span><span class="p">(</span><span class="n">with_row_id</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#lance.dataset.ScannerBuilder.with_row_id" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Enable returns with row IDs.</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="lance.dataset.Tags" class="doc doc-heading">
            <code>Tags</code>


<a href="#lance.dataset.Tags" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">


        <p>Dataset tag manager.</p>










  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="lance.dataset.Tags.create" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">create</span><span class="p">(</span><span class="n">tag</span><span class="p">,</span> <span class="n">version</span><span class="p">)</span></code>

<a href="#lance.dataset.Tags.create" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Create a tag for a given dataset version.</p>
<h6 id="lance.dataset.Tags.create--parameters">Parameters<a class="headerlink" href="#lance.dataset.Tags.create--parameters" title="Permanent link">&para;</a></h6>
<p>tag: str,
    The name of the tag to create. This name must be unique among all tag
    names for the dataset.
version: int,
    The dataset version to tag.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.Tags.delete" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">delete</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span></code>

<a href="#lance.dataset.Tags.delete" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Delete tag from the dataset.</p>
<h6 id="lance.dataset.Tags.delete--parameters">Parameters<a class="headerlink" href="#lance.dataset.Tags.delete--parameters" title="Permanent link">&para;</a></h6>
<p>tag: str,
    The name of the tag to delete.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.Tags.list" class="doc doc-heading">
            <code class="highlight language-python"><span class="nb">list</span><span class="p">()</span></code>

<a href="#lance.dataset.Tags.list" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>List all dataset tags.</p>
<h6 id="lance.dataset.Tags.list--returns">Returns<a class="headerlink" href="#lance.dataset.Tags.list--returns" title="Permanent link">&para;</a></h6>
<p>dict[str, Tag]
    A dictionary mapping tag names to version numbers.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.Tags.update" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">update</span><span class="p">(</span><span class="n">tag</span><span class="p">,</span> <span class="n">version</span><span class="p">)</span></code>

<a href="#lance.dataset.Tags.update" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Update tag to a new version.</p>
<h6 id="lance.dataset.Tags.update--parameters">Parameters<a class="headerlink" href="#lance.dataset.Tags.update--parameters" title="Permanent link">&para;</a></h6>
<p>tag: str,
    The name of the tag to update.
version: int,
    The new dataset version to tag.</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="lance.dataset.VectorIndexReader" class="doc doc-heading">
            <code>VectorIndexReader</code>


<a href="#lance.dataset.VectorIndexReader" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">


        <p>This class allows you to initialize a reader for a specific vector index,
retrieve the number of partitions,
access the centroids of the index,
and read specific partitions of the index.</p>
<h5 id="lance.dataset.VectorIndexReader--parameters">Parameters<a class="headerlink" href="#lance.dataset.VectorIndexReader--parameters" title="Permanent link">&para;</a></h5>
<p>dataset: LanceDataset
    The dataset containing the index.
index_name: str
    The name of the vector index to read.</p>
<h5 id="lance.dataset.VectorIndexReader--examples">Examples<a class="headerlink" href="#lance.dataset.VectorIndexReader--examples" title="Permanent link">&para;</a></h5>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>import lance
from lance.dataset import VectorIndexReader
import numpy as np
import pyarrow as pa
vectors = np.random.rand(256, 2)
data = pa.table({&quot;vector&quot;: pa.array(vectors.tolist(),
    type=pa.list_(pa.float32(), 2))})
dataset = lance.write_dataset(data, &quot;/tmp/index_reader_demo&quot;)
dataset.create_index(&quot;vector&quot;, index_type=&quot;IVF_PQ&quot;,
    num_partitions=4, num_sub_vectors=2)
reader = VectorIndexReader(dataset, &quot;vector_idx&quot;)
assert reader.num_partitions() == 4
partition = reader.read_partition(0)
assert &quot;_rowid&quot; in partition.column_names
</code></pre></div>

<h5 id="lance.dataset.VectorIndexReader--exceptions">Exceptions<a class="headerlink" href="#lance.dataset.VectorIndexReader--exceptions" title="Permanent link">&para;</a></h5>
<p>ValueError
    If the specified index is not a vector index.</p>










  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="lance.dataset.VectorIndexReader.centroids" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">centroids</span><span class="p">()</span></code>

<a href="#lance.dataset.VectorIndexReader.centroids" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Returns the centroids of the index</p>
<h6 id="lance.dataset.VectorIndexReader.centroids--returns">Returns<a class="headerlink" href="#lance.dataset.VectorIndexReader.centroids--returns" title="Permanent link">&para;</a></h6>
<p>np.ndarray
    The centroids of IVF
    with shape (num_partitions, dim)</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.VectorIndexReader.num_partitions" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">num_partitions</span><span class="p">()</span></code>

<a href="#lance.dataset.VectorIndexReader.num_partitions" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Returns the number of partitions in the dataset.</p>
<h6 id="lance.dataset.VectorIndexReader.num_partitions--returns">Returns<a class="headerlink" href="#lance.dataset.VectorIndexReader.num_partitions--returns" title="Permanent link">&para;</a></h6>
<p>int
    The number of partitions.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lance.dataset.VectorIndexReader.read_partition" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">read_partition</span><span class="p">(</span><span class="n">partition_id</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">with_vector</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#lance.dataset.VectorIndexReader.read_partition" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Returns a pyarrow table for the given IVF partition</p>
<h6 id="lance.dataset.VectorIndexReader.read_partition--parameters">Parameters<a class="headerlink" href="#lance.dataset.VectorIndexReader.read_partition--parameters" title="Permanent link">&para;</a></h6>
<p>partition_id: int
    The id of the partition to read
with_vector: bool, default False
    Whether to include the vector column in the reader,
    for IVF_PQ, the vector column is PQ codes</p>
<h6 id="lance.dataset.VectorIndexReader.read_partition--returns">Returns<a class="headerlink" href="#lance.dataset.VectorIndexReader.read_partition--returns" title="Permanent link">&para;</a></h6>
<p>pa.Table
    A pyarrow table for the given partition,
    containing the row IDs, and quantized vectors (if with_vector is True).</p>


    </div>

</div>



  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h3 id="lance.dataset.write_dataset" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">write_dataset</span><span class="p">(</span><span class="n">data_obj</span><span class="p">,</span> <span class="n">uri</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;create&#39;</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">max_rows_per_file</span><span class="o">=</span><span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">max_rows_per_group</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">max_bytes_per_file</span><span class="o">=</span><span class="mi">90</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">commit_lock</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">progress</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">storage_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">data_storage_version</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_legacy_format</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">enable_v2_manifest_paths</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">enable_move_stable_row_ids</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#lance.dataset.write_dataset" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Write a given data_obj to the given uri</p>
<h5 id="lance.dataset.write_dataset--parameters">Parameters<a class="headerlink" href="#lance.dataset.write_dataset--parameters" title="Permanent link">&para;</a></h5>
<p>data_obj: Reader-like
    The data to be written. Acceptable types are:
    - Pandas DataFrame, Pyarrow Table, Dataset, Scanner, or RecordBatchReader
    - Huggingface dataset
uri: str, Path, or LanceDataset
    Where to write the dataset to (directory). If a LanceDataset is passed,
    the session will be reused.
schema: Schema, optional
    If specified and the input is a pandas DataFrame, use this schema
    instead of the default pandas to arrow table conversion.
mode: str
    <strong>create</strong> - create a new dataset (raises if uri already exists).
    <strong>overwrite</strong> - create a new snapshot version
    <strong>append</strong> - create a new version that is the concat of the input the
    latest version (raises if uri does not exist)
max_rows_per_file: int, default 1024 * 1024
    The max number of rows to write before starting a new file
max_rows_per_group: int, default 1024
    The max number of rows before starting a new group (in the same file)
max_bytes_per_file: int, default 90 * 1024 * 1024 * 1024
    The max number of bytes to write before starting a new file. This is a
    soft limit. This limit is checked after each group is written, which
    means larger groups may cause this to be overshot meaningfully. This
    defaults to 90 GB, since we have a hard limit of 100 GB per file on
    object stores.
commit_lock : CommitLock, optional
    A custom commit lock.  Only needed if your object store does not support
    atomic commits.  See the user guide for more details.
progress: FragmentWriteProgress, optional
    <em>Experimental API</em>. Progress tracking for writing the fragment. Pass
    a custom class that defines hooks to be called when each fragment is
    starting to write and finishing writing.
storage_options : optional, dict
    Extra options that make sense for a particular storage connection. This is
    used to store connection parameters like credentials, endpoint, etc.
data_storage_version: optional, str, default None
    The version of the data storage format to use. Newer versions are more
    efficient but require newer versions of lance to read.  The default (None)
    will use the latest stable version.  See the user guide for more details.
use_legacy_format : optional, bool, default None
    Deprecated method for setting the data storage version. Use the
    <code>data_storage_version</code> parameter instead.
enable_v2_manifest_paths : bool, optional
    If True, and this is a new dataset, uses the new V2 manifest paths.
    These paths provide more efficient opening of datasets with many
    versions on object stores. This parameter has no effect if the dataset
    already exists. To migrate an existing dataset, instead use the
    :meth:<code>LanceDataset.migrate_manifest_paths_v2</code> method. Default is False.
enable_move_stable_row_ids : bool, optional
    Experimental parameter: if set to true, the writer will use move-stable row ids.
    These row ids are stable after compaction operations, but not after updates.
    This makes compaction more efficient, since with stable row ids no
    secondary indices need to be updated to point to new row ids.</p>


    </div>

</div>



  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="https://github.com/lancedb/lance" target="_blank" rel="noopener" title="Source on GitHub" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
    <a href="https://pypi.org/project/pylance/" target="_blank" rel="noopener" title="PyPI Package" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6M286.2 404c11.1 0 20.1 9.1 20.1 20.3 0 11.3-9 20.4-20.1 20.4-11 0-20.1-9.2-20.1-20.4.1-11.3 9.1-20.3 20.1-20.3M167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4m-6.7-142.6c-11.1 0-20.1-9.1-20.1-20.3.1-11.3 9-20.4 20.1-20.4 11 0 20.1 9.2 20.1 20.4s-9 20.3-20.1 20.3"/></svg>
    </a>
  
    
    
    
    
    <a href="https://discord.gg/zMM32dvNtd" target="_blank" rel="noopener" title="Discord Community" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M524.531 69.836a1.5 1.5 0 0 0-.764-.7A485 485 0 0 0 404.081 32.03a1.82 1.82 0 0 0-1.923.91 338 338 0 0 0-14.9 30.6 447.9 447.9 0 0 0-134.426 0 310 310 0 0 0-15.135-30.6 1.89 1.89 0 0 0-1.924-.91 483.7 483.7 0 0 0-119.688 37.107 1.7 1.7 0 0 0-.788.676C39.068 183.651 18.186 294.69 28.43 404.354a2.02 2.02 0 0 0 .765 1.375 487.7 487.7 0 0 0 146.825 74.189 1.9 1.9 0 0 0 2.063-.676A348 348 0 0 0 208.12 430.4a1.86 1.86 0 0 0-1.019-2.588 321 321 0 0 1-45.868-21.853 1.885 1.885 0 0 1-.185-3.126 251 251 0 0 0 9.109-7.137 1.82 1.82 0 0 1 1.9-.256c96.229 43.917 200.41 43.917 295.5 0a1.81 1.81 0 0 1 1.924.233 235 235 0 0 0 9.132 7.16 1.884 1.884 0 0 1-.162 3.126 301.4 301.4 0 0 1-45.89 21.83 1.875 1.875 0 0 0-1 2.611 391 391 0 0 0 30.014 48.815 1.86 1.86 0 0 0 2.063.7A486 486 0 0 0 610.7 405.729a1.88 1.88 0 0 0 .765-1.352c12.264-126.783-20.532-236.912-86.934-334.541M222.491 337.58c-28.972 0-52.844-26.587-52.844-59.239s23.409-59.241 52.844-59.241c29.665 0 53.306 26.82 52.843 59.239 0 32.654-23.41 59.241-52.843 59.241m195.38 0c-28.971 0-52.843-26.587-52.843-59.239s23.409-59.241 52.843-59.241c29.667 0 53.307 26.82 52.844 59.239 0 32.654-23.177 59.241-52.844 59.241"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.expand", "navigation.sections", "navigation.top", "navigation.tracking", "content.tabs.link", "content.code.copy", "content.code.annotate", "search.highlight", "search.share", "search.suggest"], "search": "../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="../../assets/extra.js"></script>
      
    
  </body>
</html>